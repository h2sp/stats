<?xml version='1.0'?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<book>
    <title>Think Stats</title>
  
  
  

  
  

  
  

  
  

  
  

  
  

  
  

  
  <bookinfo>
  <title>Think Stats</title>
  <authorgroup>
    <author>
      <surname>Allen B. Downey</surname>
    </author>
  </authorgroup>
  
  
</bookinfo>

  
  
<preface id="preface">
  <title>Preface</title>
  
  
  
<sect1 id="a0000000003" remap="section">
  <title>Why I wrote this book</title>
    
  
  <para><emphasis>Think Stats: Probability and Statistics for Programmers</emphasis> is a textbook for a new kind of introductory prob-stat class. It emphasizes the use of statistics to explore large datasets. It takes a computational approach, which has several advantages: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>Students write programs as a way of developing and testing their understanding. For example, they write functions to compute a least squares fit, residuals, and the coefficient of determination. Writing and testing this code requires them to understand the concepts and implicitly corrects misunderstandings. </para>
</listitem>
  
    <listitem>
  
  <para>Students run experiments to test statistical behavior. For example, they explore the Central Limit Theorem (CLT) by generating samples from several distributions. When they see that the sum of values from a Pareto distribution doesn’t converge to normal, they remember the assumptions the CLT is based on. </para>
</listitem>
  
    <listitem>
  
  <para>Some ideas that are hard to grasp mathematically are easy to understand by simulation. For example, we approximate p-values by running Monte Carlo simulations, which reinforces the meaning of the p-value. </para>
</listitem>
  
    <listitem>
  
  <para>Using discrete distributions and computation makes it possible to present topics like Bayesian estimation that are not usually covered in an introductory class. For example, one exercise asks students to compute the posterior distribution for the “German tank problem,” which is difficult analytically but surprisingly easy computationally. </para>
</listitem>
  
    <listitem>
  
  <para>Because students work in a general-purpose programming language (Python), they are able to import data from almost any source. They are not limited to data that has been cleaned and formatted for a particular statistics tool. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>The book lends itself to a project-based approach. In my class, students work on a semester-long project that requires them to pose a statistical question, find a dataset that can address it, and apply each of the techniques they learn to their own data. </para>

  
  <para>To demonstrate the kind of analysis I want students to do, the book presents a case study that runs through all of the chapters. It uses data from two sources: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>The National Survey of Family Growth (NSFG), conducted by the U.S. Centers for Disease Control and Prevention (CDC) to gather “information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health.” (See <ulink url="http://cdc.gov/nchs/nsfg.htm">http://cdc.gov/nchs/nsfg.htm</ulink>.) </para>
</listitem>
  
    <listitem>
  
  <para>The Behavioral Risk Factor Surveillance System (BRFSS), conducted by the National Center for Chronic Disease Prevention and Health Promotion to “track health conditions and risk behaviors in the United States.” (See <ulink url="http://cdc.gov/BRFSS/">http://cdc.gov/BRFSS/</ulink>.) </para>
</listitem>
  
</itemizedlist></para>

  
  <para>Other examples use data from the IRS, the U.S. Census, and the Boston Marathon. </para>

</sect1><sect1 id="a0000000004" remap="section">
  <title>How I wrote this book</title>
    
  
  <para>When people write a new textbook, they usually start by reading a stack of old textbooks. As a result, most books contain the same material in pretty much the same order. Often there are phrases, and errors, that propagate from one book to the next; Stephen Jay Gould pointed out an example in his essay, “The Case of the Creeping Fox Terrier<footnote><para>A breed of dog that is about half the size of a Hyracotherium (see <ulink url="http://wikipedia.org/wiki/Hyracotherium">http://wikipedia.org/wiki/Hyracotherium</ulink>).</para></footnote>.” </para>

  
  <para>I did not do that. In fact, I used almost no printed material while I was writing this book, for several reasons: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>My goal was to explore a new approach to this material, so I didn’t want much exposure to existing approaches. </para>
</listitem>
  
    <listitem>
  
  <para>Since I am making this book available under a free license, I wanted to make sure that no part of it was encumbered by copyright restrictions. </para>
</listitem>
  
    <listitem>
  
  <para>Many readers of my books don’t have access to libraries of printed material, so I tried to make references to resources that are freely available on the Internet. </para>
</listitem>
  
    <listitem>
  
  <para>Proponents of old media think that the exclusive use of electronic resources is lazy and unreliable. They might be right about the first part, but I think they are wrong about the second, so I wanted to test my theory. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>The resource I used more than any other is Wikipedia, the bugbear of librarians everywhere. In general, the articles I read on statistical topics were very good (although I made a few small changes along the way). I include references to Wikipedia pages throughout the book and I encourage you to follow those links; in many cases, the Wikipedia page picks up where my description leaves off. The vocabulary and notation in this book are generally consistent with Wikipedia, unless I had a good reason to deviate. </para>

  
  <para>Other resources I found useful were Wolfram MathWorld and (of course) Google. I also used two books, David MacKay’s <emphasis>Information Theory, Inference, and Learning Algorithms</emphasis>, which is the book that got me hooked on Bayesian statistics, and Press et al.’s <emphasis>Numerical Recipes in C</emphasis>. But both books are available online, so I don’t feel too bad. </para>

  
  <para>Allen B. Downey Needham MA </para>

  
  <para>Allen B. Downey is a Professor of Computer Science at the Franklin W. Olin College of Engineering. </para>

</sect1><sect1 id="a0000000005" remap="section">
  <title>Contributor List</title>
    
  
  <para>If you have a suggestion or correction, please send email to <literal>downey@allendowney.com</literal>. If I make a change based on your feedback, I will add you to the contributor list (unless you ask to be omitted). <indexterm>
  <primary>contributors</primary>

</indexterm> </para>

  
  <para>If you include at least part of the sentence the error appears in, that makes it easy for me to search. Page and section numbers are fine, too, but not quite as easy to work with. Thanks! </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>Lisa Downey and June Downey read an early draft and made many corrections and suggestions. </para>
</listitem>
  
    <listitem>
  
  <para>Steven Zhang found several errors. </para>
</listitem>
  
    <listitem>
  
  <para>Andy Pethan and Molly Farison helped debug some of the solutions, and Molly spotted several typos. </para>
</listitem>
  
    <listitem>
  
  <para>Andrew Heine found an error in my error function. </para>
</listitem>
  
    <listitem>
  
  <para>Dr. Nikolas Akerblom knows how big a Hyracotherium is. </para>
</listitem>
  
    <listitem>
  
  <para>Alex Morrow clarified one of the code examples. </para>
</listitem>
  
    <listitem>
  
  <para>Jonathan Street caught an error in the nick of time. </para>
</listitem>
  
    <listitem>
  
  <para>Gábor Lipták found a typo in the book and the relay race solution. </para>
</listitem>
  
    <listitem>
  
  <para>Many thanks to Kevin Smith and Tim Arnold for their work on plasTeX, which I used to convert this book to DocBook. </para>
</listitem>
  
    <listitem>
  
  <para>George Caplan sent several suggestions for improving clarity. </para>
</listitem>
  
</itemizedlist></para>

  
  

  
  

  
  

  
  

</sect1>
</preface><chapter id="intro">
  <title>Statistical thinking for programmers</title>
  
  
  

  
  <para>This book is about turning data into knowledge. Data is cheap (at least relatively); knowledge is harder to come by. </para>

  
  <para>I will present three related pieces: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>Probability</term>
      <listitem>
  
  <para>is the study of random events. Most people have an intuitive understanding of degrees of probability, which is why you can use words like “probably” and “unlikely” without special training, but we will talk about how to make quantitative claims about those degrees. <indexterm>
  <primary>probability</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Statistics</term>
      <listitem>
  
  <para>is the discipline of using data samples to support claims about populations. Most statistical analysis is based on probability, which is why these pieces are usually presented together. <indexterm>
  <primary>statistics</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Computation</term>
      <listitem>
  
  <para>is a tool that is well-suited to quantitative analysis, and computers are commonly used to process statistics. Also, computational experiments are useful for exploring concepts in probability and statistics. <indexterm>
  <primary>computation</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>The thesis of this book is that if you know how to program, you can use that skill to help you understand probability and statistics. These topics are often presented from a mathematical perspective, and that approach works well for some people. But some important ideas in this area are hard to work with mathematically and relatively easy to approach computationally. </para>

  
  <para>The rest of this chapter presents a case study motivated by a question I heard when my wife and I were expecting our first child: do first babies tend to arrive late? <indexterm>
  <primary>first babies</primary>

</indexterm> </para>
<sect1 id="a0000000007" remap="section">
  <title>Do first babies arrive late?</title>
    
  
  <para>If you Google this question, you will find plenty of discussion. Some people claim it’s true, others say it’s a myth, and some people say it’s the other way around: first babies come early. </para>

  
  <para>In many of these discussions, people provide data to support their claims. I found many examples like these: </para>

  
  <blockquote remap="quote">
  <para>“My two friends that have given birth recently to their first babies, BOTH went almost 2 weeks overdue before going into labour or being induced.” </para>

  
  <para>“My first one came 2 weeks late and now I think the second one is going to come out two weeks early!!” </para>

  
  <para>“I don’t think that can be true because my sister was my mother’s first and she was early, as with many of my cousins.” </para>
</blockquote>

  
  <para>Reports like these are called <emphasis role="bold">anecdotal evidence</emphasis> because they are based on data that is unpublished and usually personal. In casual conversation, there is nothing wrong with anecdotes, so I don’t mean to pick on the people I quoted. <indexterm>
  <primary>anecdotal evidence</primary>

</indexterm> </para>

  
  <para>But we might want evidence that is more persuasive and an answer that is more reliable. By those standards, anecdotal evidence usually fails, because: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>Small number of observations:</term>
      <listitem>
  
  <para>If the gestation period is longer for first babies, the difference is probably small compared to the natural variation. In that case, we might have to compare a large number of pregnancies to be sure that a difference exists. <indexterm>
  <primary>sample size</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Selection bias:</term>
      <listitem>
  
  <para>People who join a discussion of this question might be interested because their first babies were late. In that case the process of selecting data would bias the results. <indexterm>
  <primary>selection bias</primary>

</indexterm> <indexterm>
  <primary>bias</primary>
<secondary>selection</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Confirmation bias:</term>
      <listitem>
  
  <para>People who believe the claim might be more likely to contribute examples that confirm it. People who doubt the claim are more likely to cite counterexamples. <indexterm>
  <primary>confirmation bias</primary>

</indexterm> <indexterm>
  <primary>bias</primary>
<secondary>confirmation</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Inaccuracy:</term>
      <listitem>
  
  <para>Anecdotes are often personal stories, and often misremembered, misrepresented, repeated inaccurately, etc. </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>So how can we do better? </para>

</sect1><sect1 id="a0000000008" remap="section">
  <title>A statistical approach</title>
    
  
  <para>To address the limitations of anecdotes, we will use the tools of statistics, which include: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>Data collection:</term>
      <listitem>
  
  <para>We will use data from a large national survey that was designed explicitly with the goal of generating statistically valid inferences about the U.S. population. <indexterm>
  <primary>data collection</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Descriptive statistics:</term>
      <listitem>
  
  <para>We will generate statistics that summarize the data concisely, and evaluate different ways to visualize data. <indexterm>
  <primary>descriptive statistics</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Exploratory data analysis:</term>
      <listitem>
  
  <para>We will look for patterns, differences, and other features that address the questions we are interested in. At the same time we will check for inconsistencies and identify limitations. <indexterm>
  <primary>exploratory data analysis</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Hypothesis testing:</term>
      <listitem>
  
  <para>Where we see apparent effects, like a difference between two groups, we will evaluate whether the effect is real, or whether it might have happened by chance. <indexterm>
  <primary>hypothesis testing</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Estimation:</term>
      <listitem>
  
  <para>We will use data from a sample to estimate characteristics of the general population. <indexterm>
  <primary>estimation</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>By performing these steps with care to avoid pitfalls, we can reach conclusions that are more justifiable and more likely to be correct. </para>

</sect1><sect1 id="nsfg" remap="section">
  <title>The National Survey of Family Growth</title>
    
  
  

  
  <para>Since 1973 the U.S. Centers for Disease Control and Prevention (CDC) have conducted the National Survey of Family Growth (NSFG), which is intended to gather “information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health. The survey results are used ... to plan health services and health education programs, and to do statistical studies of families, fertility, and health.”<footnote><para>See <ulink url="http://cdc.gov/nchs/nsfg.htm">http://cdc.gov/nchs/nsfg.htm</ulink>.</para></footnote> <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>We will use data collected by this survey to investigate whether first babies tend to come late, and other questions. In order to use this data effectively, we have to understand the design of the study. </para>

  
  <para>The NSFG is a <emphasis role="bold">cross-sectional</emphasis> study, which means that it captures a snapshot of a group at a point in time. The most common alternative is a <emphasis role="bold">longitudinal</emphasis> study, which observes a group repeatedly over a period of time. <indexterm>
  <primary>cross-sectional study</primary>

</indexterm> <indexterm>
  <primary>study</primary>
<secondary>cross-sectional</secondary>
</indexterm> <indexterm>
  <primary>longitudinal study</primary>

</indexterm> <indexterm>
  <primary>study</primary>
<secondary>longitudinal</secondary>
</indexterm> </para>

  
  <para>The NSFG has been conducted seven times; each deployment is called a <emphasis role="bold">cycle</emphasis>. We will be using data from Cycle 6, which was conducted from January 2002 to March 2003. <indexterm>
  <primary>cycle</primary>

</indexterm> </para>

  
  <para>The goal of the survey is to draw conclusions about a <emphasis role="bold">population</emphasis>; the target population of the NSFG is people in the United States aged 15-44. <indexterm>
  <primary>population</primary>

</indexterm> </para>

  
  <para>The people who participate in a survey are called <emphasis role="bold">respondents</emphasis>; a group of respondents is called a <emphasis role="bold">cohort</emphasis>. In general, cross-sectional studies are meant to be <emphasis role="bold">representative</emphasis>, which means that every member of the target population has an equal chance of participating. Of course that ideal is hard to achieve in practice, but people who conduct surveys come as close as they can. <indexterm>
  <primary>respondent</primary>

</indexterm> <indexterm>
  <primary>representative</primary>

</indexterm> </para>

  
  <para>The NSFG is not representative; instead it is deliberately <emphasis role="bold">oversampled</emphasis>. The designers of the study recruited three groups—Hispanics, African-Americans and teenagers—at rates higher than their representation in the U.S. population. The reason for oversampling is to make sure that the number of respondents in each of these groups is large enough to draw valid statistical inferences. <indexterm>
  <primary>oversampling</primary>

</indexterm> </para>

  
  <para>Of course, the drawback of oversampling is that it is not as easy to draw conclusions about the general population based on statistics from the survey. We will come back to this point later. </para>

  
  <example id="a0000000010">
  <title></title>
  
  
  <para> Although the NSFG has been conducted seven times, it is not a longitudinal study. Read the Wikipedia pages <ulink url="http://wikipedia.org/wiki/Cross-sectional_study">http://wikipedia.org/wiki/Cross-sectional_study</ulink> and <ulink url="http://wikipedia.org/wiki/Longitudinal_study">http://wikipedia.org/wiki/Longitudinal_study</ulink> to make sure you understand why not. </para>

</example>

  
  <example id="a0000000011">
  <title></title>
  
  
  <para> In this exercise, you will download data from the NSFG; we will use this data throughout the book. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Go to <ulink url="http://thinkstats.com/nsfg.html">http://thinkstats.com/nsfg.html</ulink>. Read the terms of use for this data and click “I accept these terms” (assuming that you do). </para>
</listitem>
  
  <listitem>
  
  <para>Download the files named <literal>2002FemResp.dat.gz</literal> and <literal>2002FemPreg.dat.gz</literal>. The first is the respondent file, which contains one line for each of the 7,643 female respondents. The second file contains one line for each pregnancy reported by a respondent. </para>
</listitem>
  
  <listitem>
  
  <para>Online documentation of the survey is at <ulink url="http://nsfg.icpsr.umich.edu/cocoon/WebDocs/NSFG/public/index.htm">http://nsfg.icpsr.umich.edu/cocoon/WebDocs/NSFG/public/index.htm</ulink>. Browse the sections in the left navigation bar to get a sense of what data are included. You can also read the questionnaires at <ulink url="http://cdc.gov/nchs/data/nsfg/nsfg_2002_questionnaires.htm">http://cdc.gov/nchs/data/nsfg/nsfg_2002_questionnaires.htm</ulink>. </para>
</listitem>
  
  <listitem>
  
  <para>The web page for this book provides code to process the data files from the NSFG. Download <ulink url="http://thinkstats.com/survey.py">http://thinkstats.com/survey.py</ulink> and run it in the same directory you put the data files in. It should read the data files and print the number of lines in each: <indexterm>
  <primary>survey.py</primary>

</indexterm> </para>

  
  <programlisting>Number of respondents 7643
Number of pregnancies 13593</programlisting>
</listitem>
  
  <listitem>
  
  <para>Browse the code to get a sense of what it does. The next section explains how it works. </para>
</listitem>
  
</orderedlist></para>

</example>

</sect1><sect1 id="a0000000012" remap="section">
  <title>Tables and records</title>
    
  
  <para>The poet-philosopher Steve Martin once said: <indexterm>
  <primary>Martin, Steve</primary>

</indexterm> <indexterm>
  <primary>oeuf</primary>

</indexterm> <indexterm>
  <primary>chapeau</primary>

</indexterm> <indexterm>
  <primary>French</primary>

</indexterm> </para>

  
  <blockquote remap="quote">
  <para> “Oeuf” means egg, “chapeau” means hat. It’s like those French have a different word for everything. </para>
</blockquote>

  
  <para> Like the French, database programmers speak a slightly different language, and since we’re working with a database we need to learn some vocabulary. <indexterm>
  <primary>database</primary>

</indexterm> <indexterm>
  <primary>record (database)</primary>

</indexterm> <indexterm>
  <primary>field (database)</primary>

</indexterm> <indexterm>
  <primary>table (database)</primary>

</indexterm> </para>

  
  <para>Each line in the respondents file contains information about one respondent. This information is called a <emphasis role="bold">record</emphasis>. The variables that make up a record are called <emphasis role="bold">fields</emphasis>. A collection of records is called a <emphasis role="bold">table</emphasis>. <indexterm>
  <primary>survey.py</primary>

</indexterm> </para>

  
  <para>If you read <literal>survey.py</literal> you will see class definitions for <literal>Record</literal>, which is an object that represents a record, and <literal>Table</literal>, which represents a table. </para>

  
  <para>There are two subclasses of <literal>Record</literal>—<literal>Respondent</literal> and <literal>Pregnancy</literal>—which contain records from the respondent and pregnancy tables. For the time being, these classes are empty; in particular, there is no init method to initialize their attributes. Instead we will use <literal>Table.MakeRecord</literal> to convert a line of text into a <literal>Record</literal> object. <indexterm>
  <primary>init method</primary>

</indexterm> <indexterm>
  <primary>method</primary>
<secondary>init</secondary>
</indexterm> </para>

  
  <para>There are also two subclasses of <literal>Table</literal>: <literal>Respondents</literal> and <literal>Pregnancies</literal>. The init method in each class specifies the default name of the data file and the type of record to create. Each <literal>Table</literal> object has an attribute named <literal>records</literal>, which is a list of <literal>Record</literal> objects. </para>

  
  <para>For each <literal>Table</literal>, the <literal>GetFields</literal> method returns a list of tuples that specify the fields from the record that will be stored as attributes in each <literal>Record</literal> object. (You might want to read that last sentence twice.) </para>

  
  <para>For example, here is <literal>Pregnancies.GetFields</literal>: </para>

  
  <programlisting>def GetFields(self):
        return [
            ('caseid', 1, 12, int),
            ('prglength', 275, 276, int),
            ('outcome', 277, 277, int),
            ('birthord', 278, 279, int),
            ('finalwgt', 423, 440, float),
            ]</programlisting>

  
  <para>The first tuple says that the field <literal>caseid</literal> is in columns 1 through 12 and it’s an integer. Each tuple contains the following information: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>field:</term>
      <listitem>
  
  <para>The name of the attribute where the field will be stored. Most of the time I use the name from the NSFG codebook, converted to all lower case. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>start:</term>
      <listitem>
  
  <para>The index of the starting column for this field. For example, the start index for <literal>caseid</literal> is 1. You can look up these indices in the NSFG codebook at <ulink url="http://nsfg.icpsr.umich.edu/cocoon/WebDocs/NSFG/public/index.htm">http://nsfg.icpsr.umich.edu/cocoon/WebDocs/NSFG/public/index.htm</ulink>. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>end:</term>
      <listitem>
  
  <para>The index of the ending column for this field; for example, the end index for <literal>caseid</literal> is 12. Unlike in Python, the end index is <emphasis>inclusive</emphasis>. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>conversion function:</term>
      <listitem>
  
  <para>A function that takes a string and converts it to an appropriate type. You can use built-in functions, like <literal>int</literal> and <literal>float</literal>, or user-defined functions. If the conversion fails, the attribute gets the string value <literal>’NA’</literal>. If you don’t want to convert a field, you can provide an identity function or use <literal>str</literal>. </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>For pregnancy records, we extract the following variables: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>caseid</term>
      <listitem>
  
  <para>is the integer ID of the respondent. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>prglength</term>
      <listitem>
  
  <para>is the integer duration of the pregnancy in weeks. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>outcome</term>
      <listitem>
  
  <para>is an integer code for the outcome of the pregnancy. The code 1 indicates a live birth. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>birthord</term>
      <listitem>
  
  <para>is the integer birth order of each live birth; for example, the code for a first child is 1. For outcomes other than live birth, this field is blank. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>finalwgt</term>
      <listitem>
  
  <para>is the statistical weight associated with the respondent. It is a floating-point value that indicates the number of people in the U.S. population this respondent represents. Members of oversampled groups have lower weights.<indexterm>
  <primary>weight</primary>
<secondary>sample</secondary>
</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>If you read the casebook carefully, you will see that most of these variables are <emphasis role="bold">recodes</emphasis>, which means that they are not part of the <emphasis role="bold">raw data</emphasis> collected by the survey, but they are calculated using the raw data. <indexterm>
  <primary>recode</primary>

</indexterm> <indexterm>
  <primary>raw data</primary>

</indexterm> </para>

  
  <para>For example, <literal>prglength</literal> for live births is equal to the raw variable <literal>wksgest</literal> (weeks of gestation) if it is available; otherwise it is estimated using <literal>mosgest * 4.33</literal> (months of gestation times the average number of weeks in a month). </para>

  
  <para>Recodes are often based on logic that checks the consistency and accuracy of the data. In general it is a good idea to use recodes unless there is a compelling reason to process the raw data yourself. </para>

  
  <para>You might also notice that <literal>Pregnancies</literal> has a method called <literal>Recode</literal> that does some additional checking and recoding. </para>

  
  <example id="a0000000013">
  <title></title>
  
  
  <para> In this exercise you will write a program to explore the data in the Pregnancies table. </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>In the directory where you put <literal>survey.py</literal> and the data files, create a file named <literal remap="verb">first.py</literal> and type or paste in the following code: <indexterm>
  <primary>survey.py</primary>

</indexterm> <indexterm>
  <primary>first.py</primary>

</indexterm> </para>

  
  <programlisting>import survey
table = survey.Pregnancies()
table.ReadRecords()
print 'Number of pregnancies', len(table.records)</programlisting>

  
  <para>The result should be 13593 pregnancies. </para>
</listitem>
  
  <listitem>
  
  <para>Write a loop that iterates <literal remap="verb">table</literal> and counts the number of live births. Find the documentation of <literal>outcome</literal> and confirm that your result is consistent with the summary in the documentation. </para>
</listitem>
  
  <listitem>
  
  <para>Modify the loop to partition the live birth records into two groups, one for first babies and one for the others. Again, read the documentation of <literal>birthord</literal> to see if your results are consistent. </para>

  
  <para>When you are working with a new dataset, these kinds of checks are useful for finding errors and inconsistencies in the data, detecting bugs in your program, and checking your understanding of the way the fields are encoded. </para>
</listitem>
  
  <listitem>
  
  <para>Compute the average pregnancy length (in weeks) for first babies and others. Is there a difference between the groups? How big is it? <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>
</listitem>
  
</orderedlist></para>

  
  <para>You can download a solution to this exercise from <ulink url="http://thinkstats.com/first.py">http://thinkstats.com/first.py</ulink>.<indexterm>
  <primary>first.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000014" remap="section">
  <title>Significance</title>
    
  
  <para>In the previous exercise, you compared the gestation period for first babies and others; if things worked out, you found that first babies are born about 13 hours later, on average. <indexterm>
  <primary>apparent effect</primary>

</indexterm> </para>

  
  <para>A difference like that is called an <emphasis role="bold">apparent effect</emphasis>; that is, there might be something going on, but we are not yet sure. There are several questions we still want to ask: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>If the two groups have different means, what about other <emphasis role="bold">summary statistics</emphasis>, like median and variance? Can we be more precise about how the groups differ? <indexterm>
  <primary>summary statistic</primary>

</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>Is it possible that the difference we saw could occur by chance, even if the groups we compared were actually the same? If so, we would conclude that the effect was not <emphasis role="bold">statistically significant</emphasis>. <indexterm>
  <primary>statistically significant</primary>

</indexterm> <indexterm>
  <primary>significance</primary>

</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>Is it possible that the apparent effect is due to selection bias or some other error in the experimental setup? If so, then we might conclude that the effect is an <emphasis role="bold">artifact</emphasis>; that is, something we created (by accident) rather than found. <indexterm>
  <primary>artifact</primary>

</indexterm> </para>
</listitem>
  
</itemizedlist></para>

  
  <para>Answering these questions will take most of the rest of this book. </para>

  
  <example id="a0000000015">
  <title></title>
  
  
  <para> The best way to learn about statistics is to work on a project you are interested in. Is there a question like, “Do first babies arrive late,” that you would like to investigate? </para>

  
  <para>Think about questions you find personally interesting, or items of conventional wisdom, or controversial topics, or questions that have political consequences, and see if you can formulate a question that lends itself to statistical inquiry. </para>

  
  <para>Look for data to help you address the question. Governments are good sources because data from public research is often freely available<footnote><para>On the day I wrote this paragraph, a court in the UK ruled that the Freedom of Information Act applies to scientific research data.</para></footnote>. </para>

  
  <para>Another way to find data is Wolfram Alpha, which is a curated collection of good-quality datasets at <ulink url="http://wolframalpha.com">http://wolframalpha.com</ulink>. Results from Wolfram Alpha are subject to copyright restrictions; you might want to check the terms before you commit yourself. </para>

  
  <para>Google and other search engines can also help you find data, but it can be harder to evaluate the quality of resources on the web. </para>

  
  <para>If it seems like someone has answered your question, look closely to see whether the answer is justified. There might be flaws in the data or the analysis that make the conclusion unreliable. In that case you could perform a different analysis of the same data, or look for a better source of data. </para>

  
  <para>If you find a published paper that addresses your question, you should be able to get the raw data. Many authors make their data available on the web, but for sensitive data you might have to write to the authors, provide information about how you plan to use the data, or agree to certain terms of use. Be persistent! </para>

</example>

</sect1><sect1 id="a0000000016" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>anecdotal evidence:</term>
      <listitem>
  
  <para>Evidence, often personal, that is collected casually rather than by a well-designed study. <indexterm>
  <primary>anecdotal evidence</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>population:</term>
      <listitem>
  
  <para>A group we are interested in studying, often a group of people, but the term is also used for animals, vegetables and minerals<footnote><para>If you don’t recognize this phrase, see <ulink url="http://wikipedia.org/wiki/Twenty_Questions">http://wikipedia.org/wiki/Twenty_Questions</ulink>.</para></footnote>. <indexterm>
  <primary>population</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>cross-sectional study:</term>
      <listitem>
  
  <para>A study that collects data about a population at a particular point in time. <indexterm>
  <primary>cross-sectional study</primary>

</indexterm> <indexterm>
  <primary>study</primary>
<secondary>cross-sectional</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>longitudinal study:</term>
      <listitem>
  
  <para>A study that follows a population over time, collecting data from the same group repeatedly. <indexterm>
  <primary>longitudinal study</primary>

</indexterm> <indexterm>
  <primary>study</primary>
<secondary>longitudinal</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>respondent:</term>
      <listitem>
  
  <para>A person who responds to a survey. <indexterm>
  <primary>respondent</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>cohort:</term>
      <listitem>
  
  <para>A group of respondents <indexterm>
  <primary>cohort</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>sample:</term>
      <listitem>
  
  <para>The subset of a population used to collect data. <indexterm>
  <primary>sample</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>representative:</term>
      <listitem>
  
  <para>A sample is representative if every member of the population has the same chance of being in the sample. <indexterm>
  <primary>representative</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>oversampling:</term>
      <listitem>
  
  <para>The technique of increasing the representation of a sub-population in order to avoid errors due to small sample sizes. <indexterm>
  <primary>oversampling</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>record:</term>
      <listitem>
  
  <para>In a database, a collection of information about a single person or other object of study. <indexterm>
  <primary>record</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>field:</term>
      <listitem>
  
  <para>In a database, one of the named variables that makes up a record. <indexterm>
  <primary>field</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>table:</term>
      <listitem>
  
  <para>In a database, a collection of records. <indexterm>
  <primary>table</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>raw data:</term>
      <listitem>
  
  <para>Values collected and recorded with little or no checking, calculation or interpretation. <indexterm>
  <primary>raw data</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>recode:</term>
      <listitem>
  
  <para>A value that is generated by calculation and other logic applied to raw data. <indexterm>
  <primary>recode</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>summary statistic:</term>
      <listitem>
  
  <para>The result of a computation that reduces a dataset to a single number (or at least a smaller set of numbers) that captures some characteristic of the data. <indexterm>
  <primary>summary statistic</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>apparent effect:</term>
      <listitem>
  
  <para>A measurement or summary statistic that suggests that something interesting is happening. <indexterm>
  <primary>apparent effect</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>statistically significant:</term>
      <listitem>
  
  <para>An apparent effect is statistically significant if it is unlikely to occur by chance. <indexterm>
  <primary>statistically significant</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>artifact:</term>
      <listitem>
  
  <para>An apparent effect that is caused by bias, measurement error, or some other kind of error. <indexterm>
  <primary>artifact</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="descriptive">
  <title>Descriptive statistics</title>
  
  
  
<sect1 id="mean" remap="section">
  <title>Means and averages</title>
    
  
  

  
  <para>In the previous chapter, I mentioned three summary statistics—mean, variance and median—without explaining what they are. So before we go any farther, let’s take care of that. <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>average</primary>

</indexterm> <indexterm>
  <primary>descriptive statistics</primary>

</indexterm> <indexterm>
  <primary>summary statistics</primary>

</indexterm> </para>

  
  <para>If you have a sample of <emphasis>n</emphasis> values, <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>, the mean, <emphasis>μ</emphasis>, is the sum of the values divided by the number of values; in other words </para>

  
  <para><informalequation id="a0000000019" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0001.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mu = \frac{1}{n} \sum _ i x_ i  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The words “mean” and “average” are sometimes used interchangeably, but I will maintain this distinction: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>The “mean” of a sample is the summary statistic computed with the previous formula. </para>
</listitem>
  
    <listitem>
  
  <para>An “average” is one of many summary statistics you might choose to describe the typical value or the <emphasis role="bold">central tendency</emphasis> of a sample. <indexterm>
  <primary>central tendency</primary>

</indexterm> </para>
</listitem>
  
</itemizedlist></para>

  
  <para>Sometimes the mean is a good description of a set of values. For example, apples are all pretty much the same size (at least the ones sold in supermarkets). So if I buy 6 apples and the total weight is 3 pounds, it would be a reasonable summary to say they are about a half pound each. <indexterm>
  <primary>weight</primary>
<secondary>pumpkin</secondary>
</indexterm> </para>

  
  <para>But pumpkins are more diverse. Suppose I grow several varieties in my garden, and one day I harvest three decorative pumpkins that are 1 pound each, two pie pumpkins that are 3 pounds each, and one Atlantic Giant pumpkin that weighs 591 pounds. The mean of this sample is 100 pounds, but if I told you “The average pumpkin in my garden is 100 pounds,” that would be wrong, or at least misleading. <indexterm>
  <primary>pumpkin</primary>

</indexterm> </para>

  
  <para>In this example, there is no meaningful average because there is no typical pumpkin. </para>

</sect1><sect1 id="a0000000020" remap="section">
  <title>Variance</title>
    
  
  <para> <indexterm>
  <primary>variance</primary>

</indexterm> </para>

  
  <para>If there is no single number that summarizes pumpkin weights, we can do a little better with two numbers: mean and <emphasis role="bold">variance</emphasis>. </para>

  
  <para>In the same way that the mean is intended to describe the central tendency, variance is intended to describe the <emphasis role="bold">spread</emphasis>. The variance of a set of values is </para>

  
  <para><informalequation id="a0000000021" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0002.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \sigma ^2 = \frac{1}{n} \sum _ i (x_ i - \mu )^2  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The term <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>-<emphasis>μ</emphasis> is called the “deviation from the mean,” so variance is the mean squared deviation, which is why it is denoted <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>. The square root of variance, <emphasis>σ</emphasis>, is called the <emphasis role="bold">standard deviation</emphasis>. <indexterm>
  <primary>deviation</primary>

</indexterm> <indexterm>
  <primary>standard deviation</primary>

</indexterm> </para>

  
  <para>By itself, variance is hard to interpret. One problem is that the units are strange; in this case the measurements are in pounds, so the variance is in pounds squared. Standard deviation is more meaningful; in this case the units are pounds. </para>

  
  <example id="a0000000022">
  <title></title>
  
  
  <para> For the exercises in this chapter you should download <ulink url="http://thinkstats.com/thinkstats.py">http://thinkstats.com/thinkstats.py</ulink>, which contains general-purpose functions we will use throughout the book. You can read documentation of these functions in <ulink url="http://thinkstats.com/thinkstats.html">http://thinkstats.com/thinkstats.html</ulink>. <indexterm>
  <primary>thinkstats.py</primary>

</indexterm> </para>

  
  <para>Write a function called <literal>Pumpkin</literal> that uses functions from <literal>thinkstats.py</literal> to compute the mean, variance and standard deviation of the pumpkins weights in the previous section. </para>

</example>

  
  <example id="a0000000023">
  <title></title>
  
  
  <para> Reusing code from <literal>survey.py</literal> and <literal>first.py</literal>, compute the standard deviation of gestation time for first babies and others. Does it look like the spread is the same for the two groups? <indexterm>
  <primary>survey.py</primary>

</indexterm> <indexterm>
  <primary>first.py</primary>

</indexterm> </para>

  
  <para>How big is the difference in the means compared to these standard deviations? What does this comparison suggest about the statistical significance of the difference? </para>

</example>

  
  <para>If you have prior experience, you might have seen a formula for variance with <emphasis>n</emphasis> − 1 in the denominator, rather than <emphasis>n</emphasis>. This statistic is called the “sample variance,” and it is used to estimate the variance in a population using a sample. We will come back to this in <xref linkend="estimation" />. <indexterm>
  <primary>sample variance</primary>

</indexterm> </para>

</sect1><sect1 id="distributions" remap="section">
  <title>Distributions</title>
    
  
  <para> <anchor id="a0000000024" /> <indexterm>
  <primary>distribution</primary>

</indexterm> </para>

  
  <para>Summary statistics are concise, but dangerous, because they obscure the data. An alternative is to look at the <emphasis role="bold">distribution</emphasis> of the data, which describes how often each value appears. </para>

  
  <para>The most common representation of a distribution is a <emphasis role="bold">histogram</emphasis>, which is a graph that shows the frequency or probability of each value.<indexterm>
  <primary>histogram</primary>

</indexterm> </para>

  
  <para>In this context, <emphasis role="bold">frequency</emphasis> means the number of times a value appears in a dataset—it has nothing to do with the pitch of a sound or tuning of a radio signal. A <emphasis role="bold">probability</emphasis> is a frequency expressed as a fraction of the sample size, <emphasis>n</emphasis>. <indexterm>
  <primary>frequency</primary>

</indexterm> <indexterm>
  <primary>probability</primary>

</indexterm> <indexterm>
  <primary>dictionary</primary>

</indexterm> </para>

  
  <para>In Python, an efficient way to compute frequencies is with a dictionary. Given a sequence of values, <literal>t</literal>: </para>

  
  <programlisting>hist = {}
for x in t:
    hist[x] = hist.get(x, 0) + 1</programlisting>

  
  <para>The result is a dictionary that maps from values to frequencies. To get from frequencies to probabilities, we divide through by <emphasis>n</emphasis>, which is called <emphasis role="bold">normalization</emphasis>:<indexterm>
  <primary>normalization</primary>

</indexterm> </para>

  
  <programlisting>n = float(len(t))
pmf = {}
for x, freq in hist.items():
    pmf[x] = freq / n</programlisting>

  
  <para>The normalized histogram is called a <emphasis role="bold">PMF</emphasis>, which stands for “probability mass function”; that is, it’s a function that maps from values to probabilities (I’ll explain “mass” in <xref linkend="expo_pdf" />). <indexterm>
  <primary>PMF</primary>

</indexterm> <indexterm>
  <primary>probability mass function</primary>

</indexterm> </para>

  
  <para>It might be confusing to call a Python dictionary a function. In mathematics, a function is a map from one set of values to another. In Python, we <emphasis>usually</emphasis> represent mathematical functions with function objects, but in this case we are using a dictionary (dictionaries are also called “maps,” if that helps).<indexterm>
  <primary>map</primary>

</indexterm> </para>

</sect1><sect1 id="a0000000025" remap="section">
  <title>Representing histograms</title>
    
  
  <para> <indexterm>
  <primary>histogram</primary>

</indexterm> <indexterm>
  <primary>Hist object</primary>

</indexterm> <indexterm>
  <primary sortas="pmf.py">Pmf.py</primary>

</indexterm> </para>

  
  <para>I wrote a Python module called <literal>Pmf.py</literal> that contains class definitions for Hist objects, which represent histograms, and Pmf objects, which represent PMFs. You can read the documentation at <literal>thinkstats.com/Pmf.html</literal> and download the code from <literal>thinkstats.com/Pmf.py</literal>. </para>

  
  <para>The function <literal>MakeHistFromList</literal> takes a list of values and returns a new Hist object. You can test it in Python’s interactive mode: </para>

  
  <programlisting>&gt;&gt;&gt; import Pmf
&gt;&gt;&gt; hist = Pmf.MakeHistFromList([1, 2, 2, 3, 5])
&gt;&gt;&gt; print hist
&lt;Pmf.Hist object at 0xb76cf68c&gt;</programlisting>

  
  <para><literal>Pmf.Hist</literal> means that this object is a member of the Hist class, which is defined in the Pmf module. In general, I use upper case letters for the names of classes and functions, and lower case letters for variables. </para>

  
  <para>Hist objects provide methods to look up values and their probabilities. <literal>Freq</literal> takes a value and returns its frequency: </para>

  
  <programlisting>&gt;&gt;&gt; hist.Freq(2)
2</programlisting>

  
  <para>If you look up a value that has never appeared, the frequency is 0. </para>

  
  <programlisting>&gt;&gt;&gt; hist.Freq(4)
0</programlisting>

  
  <para><literal>Values</literal> returns an unsorted list of the values in the Hist: </para>

  
  <programlisting>&gt;&gt;&gt; hist.Values()
[1, 5, 3, 2]</programlisting>

  
  <para>To loop through the values in order, you can use the built-in function <literal>sorted</literal>: </para>

  
  <programlisting>for val in sorted(hist.Values()):
    print val, hist.Freq(val)</programlisting>

  
  <para>If you are planning to look up all of the frequencies, it is more efficient to use <literal>Items</literal>, which returns an unsorted list of value-frequency pairs: </para>

  
  <programlisting>for val, freq in hist.Items():
     print val, freq</programlisting>

  
  <example id="a0000000026">
  <title></title>
  
  
  <para> The mode of a distribution is the most frequent value (see <ulink url="http://wikipedia.org/wiki/Mode_(statistics)">http://wikipedia.org/wiki/Mode_(statistics)</ulink>). Write a function called <literal>Mode</literal> that takes a Hist object and returns the most frequent value.<indexterm>
  <primary>mode</primary>

</indexterm> </para>

  
  <para>As a more challenging version, write a function called <literal>AllModes</literal> that takes a Hist object and returns a list of value-frequency pairs in descending order of frequency. Hint: the <literal>operator</literal> module provides a function called <literal>itemgetter</literal> which you can pass as a key to <literal>sorted</literal>. </para>

</example>

</sect1><sect1 id="a0000000027" remap="section">
  <title>Plotting histograms</title>
    
  
  <para> <indexterm>
  <primary>plotting</primary>

</indexterm> <indexterm>
  <primary>pyplot</primary>

</indexterm> </para>

  
  <para>There are a number of Python packages for making figures and graphs. The one I will demonstrate is <literal>pyplot</literal>, which is part of the <literal>matplotlib</literal> package at <ulink url="http://matplotlib.sourceforge.net">http://matplotlib.sourceforge.net</ulink>. </para>

  
  <para>This package is included in many Python installations. To see whether you have it, launch the Python interpreter and run: </para>

  
  <programlisting>import matplotlib.pyplot as pyplot
pyplot.pie([1,2,3])
pyplot.show()</programlisting>

  
  <para>If you have <literal>matplotlib</literal> you should see a simple pie chart; otherwise you will have to install it. <indexterm>
  <primary>matplotlib</primary>

</indexterm> <indexterm>
  <primary>bar plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>bar</secondary>
</indexterm> </para>

  
  <para>Histograms and PMFs are most often plotted as bar charts. The <literal>pyplot</literal> function to draw a bar chart is <literal>bar</literal>. Hist objects provide a method called <literal>Render</literal> that returns a sorted list of values and a list of the corresponding frequencies, which is the format <literal>bar</literal> expects: </para>

  
  <programlisting>&gt;&gt;&gt; vals, freqs = hist.Render()
&gt;&gt;&gt; rectangles = pyplot.bar(vals, freqs)
&gt;&gt;&gt; pyplot.show()</programlisting>

  
  <para>I wrote a module called <literal>myplot.py</literal> that provides functions for plotting histograms, PMFs and other objects we will see soon. You can read the documentation at <literal>thinkstats.com/myplot.html</literal> and download the code from <literal>thinkstats.com/myplot.py</literal>. Or you can use <literal>pyplot</literal> directly, if you prefer. Either way, you can find the documentation for <literal>pyplot</literal> on the web. <indexterm>
  <primary>myplot.py</primary>

</indexterm> </para>

  
  <para><xref linkend="nsfg_hist" /> shows histograms of pregnancy lengths for first babies and others. <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <figure id="nsfg_hist">
  
  <title>Histogram of pregnancy lengths.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/nsfg_hist.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>Histograms are useful because they make the following features immediately apparent: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>Mode:</term>
      <listitem>
  
  <para>The most common value in a distribution is called the <emphasis role="bold">mode</emphasis>. In <xref linkend="nsfg_hist" /> there is a clear mode at 39 weeks. In this case, the mode is the summary statistic that does the best job of describing the typical value. <indexterm>
  <primary>mode</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Shape:</term>
      <listitem>
  
  <para>Around the mode, the distribution is asymmetric; it drops off quickly to the right and more slowly to the left. From a medical point of view, this makes sense. Babies are often born early, but seldom later than 42 weeks. Also, the right side of the distribution is truncated because doctors often intervene after 42 weeks. <indexterm>
  <primary>shape</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Outliers:</term>
      <listitem>
  
  <para>Values far from the mode are called <emphasis role="bold">outliers</emphasis>. Some of these are just unusual cases, like babies born at 30 weeks. But many of them are probably due to errors, either in the reporting or recording of data. <indexterm>
  <primary>outlier</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>Although histograms make some features apparent, they are usually not useful for comparing two distributions. In this example, there are fewer “first babies” than “others,” so some of the apparent differences in the histograms are due to sample sizes. We can address this problem using PMFs. </para>

</sect1><sect1 id="a0000000029" remap="section">
  <title>Representing PMFs</title>
    
  
  <para> <indexterm>
  <primary>Pmf object</primary>

</indexterm> <indexterm>
  <primary sortas="pmf.py">Pmf.py</primary>

</indexterm> </para>

  
  <para><literal>Pmf.py</literal> provides a class called <literal>Pmf</literal> that represents PMFs. The notation can be confusing, but here it is: <literal>Pmf</literal> is the name of the module and also the class, so the full name of the class is <literal>Pmf.Pmf</literal>. I often use <literal>pmf</literal> as a variable name. Finally, in the text, I use PMF to refer to the general concept of a probability mass function, independent of my implementation. </para>

  
  <para>To create a Pmf object, use <literal>MakePmfFromList</literal>, which takes a list of values: </para>

  
  <programlisting>&gt;&gt;&gt; import Pmf
&gt;&gt;&gt; pmf = Pmf.MakePmfFromList([1, 2, 2, 3, 5])
&gt;&gt;&gt; print pmf
&lt;Pmf.Pmf object at 0xb76cf68c&gt;</programlisting>

  
  <para>Pmf and Hist objects are similar in many ways. The methods <literal>Values</literal> and <literal>Items</literal> work the same way for both types. The biggest difference is that a Hist maps from values to integer counters; a Pmf maps from values to floating-point probabilities. </para>

  
  <para>To look up the probability associated with a value, use <literal>Prob</literal>: </para>

  
  <programlisting>&gt;&gt;&gt; pmf.Prob(2)
0.4</programlisting>

  
  <para>You can modify an existing Pmf by incrementing the probability associated with a value: </para>

  
  <programlisting>&gt;&gt;&gt; pmf.Incr(2, 0.2)
&gt;&gt;&gt; pmf.Prob(2)
0.6</programlisting>

  
  <para>Or you can multiple a probability by a factor: </para>

  
  <programlisting>&gt;&gt;&gt; pmf.Mult(2, 0.5)
&gt;&gt;&gt; pmf.Prob(2)
0.3</programlisting>

  
  <para>If you modify a Pmf, the result may not be normalized; that is, the probabilities may no longer add up to 1. To check, you can call <literal>Total</literal>, which returns the sum of the probabilities: </para>

  
  <programlisting>&gt;&gt;&gt; pmf.Total()
0.9</programlisting>

  
  <para>To renormalize, call <literal>Normalize</literal>: </para>

  
  <programlisting>&gt;&gt;&gt; pmf.Normalize()
&gt;&gt;&gt; pmf.Total()
1.0</programlisting>

  
  <para>Pmf objects provide a <literal>Copy</literal> method so you can make and and modify a copy without affecting the original. </para>

  
  <example id="a0000000030">
  <title></title>
  
  
  <para> According to Wikipedia, “Survival analysis is a branch of statistics which deals with death in biological organisms and failure in mechanical systems;” see <ulink url="http://wikipedia.org/wiki/Survival_analysis">http://wikipedia.org/wiki/Survival_analysis</ulink>. <indexterm>
  <primary>survival analysis</primary>

</indexterm> </para>

  
  <para>As part of survival analysis, it is often useful to compute the remaining lifetime of, for example, a mechanical component. If we know the distribution of lifetimes and the age of the component, we can compute the distribution of remaining lifetimes. </para>

  
  <para>Write a function called <literal>RemainingLifetime</literal> that takes a Pmf of lifetimes and an age, and returns a new Pmf that represents the distribution of remaining lifetimes. </para>

</example>

  
  <example id="a0000000031">
  <title></title>
  
  
  <para> <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>variance</primary>

</indexterm> <indexterm>
  <primary>PMF</primary>

</indexterm> In <xref linkend="mean" /> we computed the mean of a sample by adding up the elements and dividing by <emphasis>n</emphasis>. If you are given a PMF, you can still compute the mean, but the process is slightly different: </para>

  
  <para><informalequation id="a0000000032" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0003.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mu = \sum _ i p_ i x_ i  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> where the <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis> are the unique values in the PMF and <emphasis>p</emphasis><emphasis><subscript>i</subscript></emphasis>=PMF(<emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>). Similarly, you can compute variance like this: </para>

  
  <para><informalequation id="a0000000033" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0004.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \sigma ^2 = \sum _ i p_ i (x_ i - \mu )^2 \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Write functions called <literal>PmfMean</literal> and <literal>PmfVar</literal> that take a Pmf object and compute the mean and variance. To test these methods, check that they are consistent with the methods <literal>Mean</literal> and <literal>Var</literal> in <literal>Pmf.py</literal>. <indexterm>
  <primary sortas="pmf.py">Pmf.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000034" remap="section">
  <title>Plotting PMFs</title>
    
  
  <para> <indexterm>
  <primary>plotting</primary>

</indexterm> <indexterm>
  <primary>PMF</primary>

</indexterm> </para>

  
  <para>There are two common ways to plot Pmfs: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>To plot a Pmf as a bar graph, you can use <literal>pyplot.bar</literal> or <literal>myplot.Hist</literal>. Bar graphs are most useful if the number of values in the Pmf is small. <indexterm>
  <primary>bar plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>bar</secondary>
</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>To plot a Pmf as a line, you can use <literal>pyplot.plot</literal> or <literal>myplot.Pmf</literal>. Line plots are most useful if there are a large number of values and the Pmf is smooth. <indexterm>
  <primary>line plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>line</secondary>
</indexterm> </para>
</listitem>
  
</itemizedlist></para>

  
  <figure id="nsfg_pmf">
  
  <title>PMF of pregnancy lengths.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/nsfg_pmf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para> <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para><xref linkend="nsfg_pmf" /> shows the PMF of pregnancy lengths as a bar graph. Using the PMF, we can see more clearly where the distributions differ. First babies seem to be less likely to arrive on time (week 39) and more likely to be a late (weeks 41 and 42). </para>

  
  <para>The code that generates the figures in this chapters is available from <ulink url="http://thinkstats.com/descriptive.py">http://thinkstats.com/descriptive.py</ulink>. To run it, you will need the modules it imports and the data from the NSFG (see <xref linkend="nsfg" />). <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> <indexterm>
  <primary>descriptive.py</primary>

</indexterm> </para>

  
  <para>Note: <literal>pyplot</literal> provides a function called <literal>hist</literal> that takes a sequence of values, computes the histogram and plots it. Since I use <literal>Hist</literal> objects, I usually don’t use <literal>pyplot.hist</literal>. </para>

</sect1><sect1 id="a0000000036" remap="section">
  <title>Outliers</title>
    
  
  <para> <indexterm>
  <primary>outlier</primary>

</indexterm> </para>

  
  <para>Outliers are values that are far from the central tendency. Outliers might be caused by errors in collecting or processing the data, or they might be correct but unusual measurements. It is always a good idea to check for outliers, and sometimes it is useful and appropriate to discard them. </para>

  
  <para>In the list of pregnancy lengths for live births, the 10 lowest values are {0, 4, 9, 13, 17, 17, 18, 19, 20, 21}. Values below 20 weeks are certainly errors, and values higher than 30 weeks are probably legitimate. But values in between are hard to interpret. <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>On the other end, the highest values are: </para>

  
  <programlisting>weeks  count
43     148
44     46
45     10
46     1
47     1
48     7
50     2</programlisting>

  
  <para>Again, some values are almost certainly errors, but it is hard to know for sure. One option is to <emphasis role="bold">trim</emphasis> the data by discarding some fraction of the highest and lowest values (see <ulink url="http://wikipedia.org/wiki/Truncated_mean">http://wikipedia.org/wiki/Truncated_mean</ulink>). <indexterm>
  <primary>trimmed mean</primary>

</indexterm> <indexterm>
  <primary>mean</primary>
<secondary>trimmed</secondary>
</indexterm> <indexterm>
  <primary>truncated mean</primary>

</indexterm> <indexterm>
  <primary>mean</primary>
<secondary>truncated</secondary>
</indexterm> </para>

</sect1><sect1 id="a0000000037" remap="section">
  <title>Other visualizations</title>
    
  
  <para> <indexterm>
  <primary>visualization</primary>

</indexterm> <indexterm>
  <primary>exploratory data analysis</primary>

</indexterm> </para>

  
  <para>Histograms and PMFs are useful for exploratory data analysis; once you have an idea what is going on, it is often useful to design a visualization that focuses on the apparent effect. <indexterm>
  <primary>apparent effect</primary>

</indexterm> </para>

  
  <para>In the NSFG data, the biggest differences in the distributions are near the mode. So it makes sense to zoom in on that part of the graph, and to transform the data to emphasize differences. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para><xref linkend="nsfg_diffs" /> shows the difference between the PMFs for weeks 35–45. I multiplied by 100 to express the differences in percentage points. </para>

  
  <figure id="nsfg_diffs">
  
  <title>Difference in percentage, by week.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/nsfg_diffs.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>This figure makes the pattern clearer: first babies are less likely to be born in week 39, and somewhat more likely to be born in weeks 41 and 42. </para>

</sect1><sect1 id="relative.risk" remap="section">
  <title>Relative risk</title>
    
  
  <para> <anchor id="a0000000039" /> <indexterm>
  <primary>relative risk</primary>

</indexterm> </para>

  
  <para>We started with the question, “Do first babies arrive late?” To make that more precise, let’s say that a baby is early if it is born during Week 37 or earlier, on time if it is born during Week 38, 39 or 40, and late if it is born during Week 41 or later. Ranges like these that are used to group data are called <emphasis role="bold">bins</emphasis>. <indexterm>
  <primary>bin</primary>

</indexterm> <indexterm>
  <primary>risk.py</primary>

</indexterm> </para>

  
  <example id="a0000000040">
  <title></title>
  
  
  <para> Create a file named <literal>risk.py</literal>. Write functions named <literal>ProbEarly</literal>, <literal>ProbOnTime</literal> and <literal>ProbLate</literal> that take a PMF and compute the fraction of births that fall into each bin. Hint: write a generalized function that these functions call. </para>

  
  <para>Make three PMFs, one for first babies, one for others, and one for all live births. For each PMF, compute the probability of being born early, on time, or late. </para>

  
  <para>One way to summarize data like this is with <emphasis role="bold">relative risk</emphasis>, which is a ratio of two probabilities. For example, the probability that a first baby is born early is 18.2%. For other babies it is 16.8%, so the relative risk is 1.08. That means that first babies are about 8% more likely to be early. </para>

  
  <para>Write code to confirm that result, then compute the relative risks of being born on time and being late. You can download a solution from <ulink url="http://thinkstats.com/risk.py">http://thinkstats.com/risk.py</ulink>. </para>

</example>

</sect1><sect1 id="a0000000041" remap="section">
  <title>Conditional probability</title>
    
  
  <para> <indexterm>
  <primary>conditional probability</primary>

</indexterm> <indexterm>
  <primary>probability</primary>
<secondary>conditional</secondary>
</indexterm> </para>

  
  <para>Imagine that someone you know is pregnant, and it is the beginning of Week 39. What is the chance that the baby will be born in the next week? How much does the answer change if it’s a first baby? </para>

  
  <para>We can answer these questions by computing a <emphasis role="bold">conditional probability</emphasis>, which is (ahem!) a probability that depends on a condition. In this case, the condition is that we know the baby didn’t arrive during Weeks 0–38. </para>

  
  <para>Here’s one way to do it: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Given a PMF, generate a fake cohort of 1000 pregnancies. For each number of weeks, <emphasis>x</emphasis>, the number of pregnancies with duration <emphasis>x</emphasis> is 1000 PMF(<emphasis>x</emphasis>). <indexterm>
  <primary>cohort</primary>

</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>Remove from the cohort all pregnancies with length less than 39. <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>Compute the PMF of the remaining durations; the result is the conditional PMF. </para>
</listitem>
  
  <listitem>
  
  <para>Evaluate the conditional PMF at <emphasis>x</emphasis> = 39 weeks. </para>
</listitem>
  
</orderedlist></para>

  
  <para>This algorithm is conceptually clear, but not very efficient. A simple alternative is to remove from the distribution the values less than 39 and then renormalize. </para>

  
  <example id="a0000000042">
  <title></title>
  
  
  <para> Write a function that implements either of these algorithms and computes the probability that a baby will be born during Week 39, given that it was not born prior to Week 39. </para>

  
  <para>Generalize the function to compute the probability that a baby will be born during Week <emphasis>x</emphasis>, given that it was not born prior to Week <emphasis>x</emphasis>, for all <emphasis>x</emphasis>. Plot this value as a function of <emphasis>x</emphasis> for first babies and others. </para>

  
  <para>You can download a solution to this problem from <ulink url="http://thinkstats.com/conditional.py">http://thinkstats.com/conditional.py</ulink>. <indexterm>
  <primary>conditional.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000043" remap="section">
  <title>Reporting results</title>
    
  
  <para>At this point we have explored the data and seen several apparent effects. For now, let’s assume that these effects are real (but let’s remember that it’s an assumption). How should we report these results? </para>

  
  <para>The answer might depend on who is asking the question. For example, a scientist might be interested in any (real) effect, no matter how small. A doctor might only care about effects that are <emphasis role="bold">clinically significant</emphasis>; that is, differences that affect treatment decisions. A pregnant woman might be interested in results that are relevant to her, like the conditional probabilities in the previous section. <indexterm>
  <primary>clinically significant</primary>

</indexterm> <indexterm>
  <primary>significance</primary>

</indexterm> </para>

  
  <para>How you report results also depends on your goals. If you are trying to demonstrate the significance of an effect, you might choose summary statistics, like relative risk, that emphasize differences. If you are trying to reassure a patient, you might choose statistics that put the differences in context. </para>

  
  <example id="a0000000044">
  <title></title>
  
  
  <para> Based on the results from the previous exercises, suppose you were asked to summarize what you learned about whether first babies arrive late. </para>

  
  <para>Which summary statistics would you use if you wanted to get a story on the evening news? Which ones would you use if you wanted to reassure an anxious patient? <indexterm>
  <primary>Adams, Cecil</primary>

</indexterm> <indexterm>
  <primary>Straight Dope, The</primary>

</indexterm> </para>

  
  <para>Finally, imagine that you are Cecil Adams, author of <emphasis>The Straight Dope</emphasis> (<ulink url="http://straightdope.com">http://straightdope.com</ulink>), and your job is to answer the question, “Do first babies arrive late?” Write a paragraph that uses the results in this chapter to answer the question clearly, precisely, and accurately. </para>

</example>

</sect1><sect1 id="a0000000045" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>central tendency:</term>
      <listitem>
  
  <para>A characteristic of a sample or population; intuitively, it is the most average value. <indexterm>
  <primary>central tendency</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>spread:</term>
      <listitem>
  
  <para>A characteristic of a sample or population; intuitively, it describes how much variability there is. <indexterm>
  <primary>spread</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>variance:</term>
      <listitem>
  
  <para>A summary statistic often used to quantify spread. <indexterm>
  <primary>variance</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>standard deviation:</term>
      <listitem>
  
  <para>The square root of variance, also used as a measure of spread. <indexterm>
  <primary>standard deviation</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>frequency:</term>
      <listitem>
  
  <para>The number of times a value appears in a sample. <indexterm>
  <primary>frequency</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>histogram:</term>
      <listitem>
  
  <para>A mapping from values to frequencies, or a graph that shows this mapping. <indexterm>
  <primary>histogram</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>probability:</term>
      <listitem>
  
  <para>A frequency expressed as a fraction of the sample size. <indexterm>
  <primary>probability</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>normalization:</term>
      <listitem>
  
  <para>The process of dividing a frequency by a sample size to get a probability. <indexterm>
  <primary>normalization</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>distribution:</term>
      <listitem>
  
  <para>A summary of the values that appear in a sample and the frequency, or probability, of each. <indexterm>
  <primary>distribution</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>PMF:</term>
      <listitem>
  
  <para>Probability mass function: a representation of a distribution as a function that maps from values to probabilities. <indexterm>
  <primary>PMF</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>mode:</term>
      <listitem>
  
  <para>The most frequent value in a sample. <indexterm>
  <primary>mode</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>outlier:</term>
      <listitem>
  
  <para>A value far from the central tendency. <indexterm>
  <primary>outlier</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>trim:</term>
      <listitem>
  
  <para>To remove outliers from a dataset. <indexterm>
  <primary>trim</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>bin:</term>
      <listitem>
  
  <para>A range used to group nearby values. <indexterm>
  <primary>bin</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>relative risk:</term>
      <listitem>
  
  <para>A ratio of two probabilities, often used to measure a difference between distributions. <indexterm>
  <primary>relative risk</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>conditional probability:</term>
      <listitem>
  
  <para>A probability computed under the assumption that some condition holds. <indexterm>
  <primary>conditional probability</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>clinically significant:</term>
      <listitem>
  
  <para>A result, like a difference between groups, that is relevant in practice. <indexterm>
  <primary>clinically significant</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="cumulative">
  <title>Cumulative distribution functions</title>
  
  
  
<sect1 id="a0000000047" remap="section">
  <title>The class size paradox</title>
    
  
  <para> <indexterm>
  <primary>class size</primary>

</indexterm> </para>

  
  <para>At many American colleges and universities, the student-to-faculty ratio is about 10:1. But students are often surprised to discover that their average class size is bigger than 10. There are two reasons for the discrepancy: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>Students typically take 4–5 classes per semester, but professors often teach 1 or 2. </para>
</listitem>
  
    <listitem>
  
  <para>The number of students who enjoy a small class is small, but the number of students in a large class is (ahem!) large. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>The first effect is obvious (at least once it is pointed out); the second is more subtle. So let’s look at an example. Suppose that a college offers 65 classes in a given semester, with the following distribution of sizes: </para>

  
  <programlisting>size      count
 5- 9          8
10-14          8
15-19         14
20-24          4
25-29          6
30-34         12
35-39          8
40-44          3
45-49          2</programlisting>

  
  <para>If you ask the Dean for the average class size, he would construct a PMF, compute the mean, and report that the average class size is 24. </para>

  
  <para>But if you survey a group of students, ask them how many students are in their classes, and compute the mean, you would think that the average class size was higher. </para>

  
  <example id="a0000000048">
  <title></title>
  
  
  <para> Build a PMF of these data and compute the mean as perceived by the Dean. Since the data have been grouped in bins, you can use the mid-point of each bin. <indexterm>
  <primary>PMF</primary>

</indexterm> </para>

  
  <para>Now find the distribution of class sizes as perceived by students and compute its mean. </para>

  
  <para>Suppose you want to find the distribution of class sizes at a college, but you can’t get reliable data from the Dean. An alternative is to choose a random sample of students and ask them the number of students in each of their classes. Then you could compute the PMF of their responses. <indexterm>
  <primary>bias</primary>
<secondary>oversampling</secondary>
</indexterm> <indexterm>
  <primary>oversampling</primary>

</indexterm> </para>

  
  <para>The result would be biased because large classes would be oversampled, but you could estimate the actual distribution of class sizes by applying an appropriate transformation to the observed distribution. </para>

  
  <para>Write a function called <literal remap="verb">UnbiasPmf</literal> that takes the PMF of the observed values and returns a new Pmf object that estimates the distribution of class sizes. </para>

  
  <para>You can download a solution to this problem from <ulink url="http://thinkstats.com/class_size.py">http://thinkstats.com/class_size.py</ulink>. <indexterm>
  <primary>class_size.py</primary>

</indexterm> </para>

</example>

  
  <example id="relay">
  <title></title>
  
  
  

  
  <para>In most foot races, everyone starts at the same time. If you are a fast runner, you usually pass a lot of people at the beginning of the race, but after a few miles everyone around you is going at the same speed. <indexterm>
  <primary>relay race</primary>

</indexterm> <indexterm>
  <primary>bias</primary>
<secondary>oversampling</secondary>
</indexterm> <indexterm>
  <primary>oversampling</primary>

</indexterm> </para>

  
  <para>When I ran a long-distance (209 miles) relay race for the first time, I noticed an odd phenomenon: when I overtook another runner, I was usually much faster, and when another runner overtook me, he was usually much faster. </para>

  
  <para>At first I thought that the distribution of speeds might be bimodal; that is, there were many slow runners and many fast runners, but few at my speed. </para>

  
  <para>Then I realized that I was the victim of selection bias. The race was unusual in two ways: it used a staggered start, so teams started at different times; also, many teams included runners at different levels of ability. <indexterm>
  <primary>bias</primary>
<secondary>selection</secondary>
</indexterm> <indexterm>
  <primary>selection bias</primary>

</indexterm> </para>

  
  <para>As a result, runners were spread out along the course with little relationship between speed and location. When I started running my leg, the runners near me were (pretty much) a random sample of the runners in the race. </para>

  
  <para>So where does the bias come from? During my time on the course, the chance of overtaking a runner, or being overtaken, is proportional to the difference in our speeds. To see why, think about the extremes. If another runner is going at the same speed as me, neither of us will overtake the other. If someone is going so fast that they cover the entire course while I am running, they are certain to overtake me. </para>

  
  <para>Write a function called <literal>BiasPmf</literal> that takes a Pmf representing the actual distribution of runners’ speeds, and the speed of a running observer, and returns a new Pmf representing the distribution of runners’ speeds as seen by the observer. </para>

  
  <para>To test your function, get the distribution of speeds from a normal road race (not a relay). I wrote a program that reads the results from the James Joyce Ramble 10K in Dedham MA and converts the pace of each runner to MPH. Download it from <ulink url="http://thinkstats.com/relay.py">http://thinkstats.com/relay.py</ulink>. Run it and look at the PMF of speeds. <indexterm>
  <primary>relay.py</primary>

</indexterm> <indexterm>
  <primary>relay_soln.py</primary>

</indexterm> </para>

  
  <para>Now compute the distribution of speeds you would observe if you ran a relay race at 7.5 MPH with this group of runners. You can download a solution from <ulink url="http://thinkstats.com/relay_soln.py">http://thinkstats.com/relay_soln.py</ulink> </para>

</example>

</sect1><sect1 id="a0000000050" remap="section">
  <title>The limits of PMFs</title>
    
  
  <para> <indexterm>
  <primary>PMF</primary>

</indexterm> </para>

  
  <para>PMFs work well if the number of values is small. But as the number of values increases, the probability associated with each value gets smaller and the effect of random noise increases. </para>

  
  <para>For example, we might be interested in the distribution of birth weights. In the NSFG data, the variable <literal remap="verb">totalwgt_oz</literal> records weight at birth in ounces. <xref linkend="nsfg_birthwgt_pmf" /> shows the PMF of these values for first babies and others. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> </para>

  
  <figure id="nsfg_birthwgt_pmf">
  
  <title>PMF of birth weights. This figure shows a limitation of PMFs: they are hard to compare.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/nsfg_birthwgt_pmf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>Overall, these distributions resemble the familiar “bell curve,” with many values near the mean and a few values much higher and lower. </para>

  
  <para>But parts of this figure are hard to interpret. There are many spikes and valleys, and some apparent differences between the distributions. It is hard to tell which of these features are significant. Also, it is hard to see overall patterns; for example, which distribution do you think has the higher mean? <indexterm>
  <primary>binning</primary>

</indexterm> </para>

  
  <para>These problems can be mitigated by binning the data; that is, dividing the domain into non-overlapping intervals and counting the number of values in each bin. Binning can be useful, but it is tricky to get the size of the bins right. If they are big enough to smooth out noise, they might also smooth out useful information. </para>

  
  <para>An alternative that avoids these problems is the <emphasis role="bold">cumulative distribution function</emphasis>, or <emphasis role="bold">CDF</emphasis>. But before we can get to that, we have to talk about percentiles. <indexterm>
  <primary>cumulative distribution function</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>

</indexterm> </para>

</sect1><sect1 id="a0000000052" remap="section">
  <title>Percentiles</title>
    
  
  <para> <indexterm>
  <primary>percentile rank</primary>

</indexterm> </para>

  
  <para>If you have taken a standardized test, you probably got your results in the form of a raw score and a <emphasis role="bold">percentile rank</emphasis>. In this context, the percentile rank is the fraction of people who scored lower than you (or the same). So if you are “in the 90th percentile,” you did as well as or better than 90% of the people who took the exam. </para>

  
  <para>Here’s how you could compute the percentile rank of a value, <literal remap="verb">your_score</literal>, relative to the scores in the sequence <literal>scores</literal>: </para>

  
  <programlisting>def PercentileRank(scores, your_score):
    count = 0
    for score in scores:
        if score &lt;= your_score:
            count += 1

    percentile_rank = 100.0 * count / len(scores)
    return percentile_rank</programlisting>

  
  <para> For example, if the scores in the sequence were 55, 66, 77, 88 and 99, and you got the 88, then your percentile rank would be <literal>100 * 4 / 5</literal> which is 80. </para>

  
  <para>If you are given a value, it is easy to find its percentile rank; going the other way is slightly harder. If you are given a percentile rank and you want to find the corresponding value, one option is to sort the values and search for the one you want: </para>

  
  <programlisting>def Percentile(scores, percentile_rank):
    scores.sort()
    for score in scores:
        if PercentileRank(scores, score) &gt;= percentile_rank:
            return score</programlisting>

  
  <para>The result of this calculation is a <emphasis role="bold">percentile</emphasis>. For example, the 50th percentile is the value with percentile rank 50. In the distribution of exam scores, the 50th percentile is 77. <indexterm>
  <primary>percentile</primary>

</indexterm> </para>

  
  <example id="a0000000053">
  <title></title>
  
  
  <para> This implementation of <literal>Percentile</literal> is not very efficient. A better approach is to use the percentile rank to compute the index of the corresponding percentile. Write a version of <literal>Percentile</literal> that uses this algorithm. </para>

  
  <para>You can download a solution from <ulink url="http://thinkstats.com/score_example.py">http://thinkstats.com/score_example.py</ulink>. <indexterm>
  <primary>score_example.py</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000054">
  <title></title>
  
  
  <para> Optional: If you only want to compute one percentile, it is not efficient to sort the scores. A better option is the selection algorithm, which you can read about at <ulink url="http://wikipedia.org/wiki/Selection_algorithm">http://wikipedia.org/wiki/Selection_algorithm</ulink>. <indexterm>
  <primary>selection algorithm</primary>

</indexterm> </para>

  
  <para>Write (or find) an implementation of the selection algorithm and use it to write an efficient version of <literal>Percentile</literal>. </para>

</example>

</sect1><sect1 id="a0000000055" remap="section">
  <title>Cumulative distribution functions</title>
    
  
  <para> <indexterm>
  <primary>cumulative distribution function</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>

</indexterm> </para>

  
  <para>Now that we understand percentiles, we are ready to tackle the cumulative distribution function (CDF). The CDF is the function that maps values to their percentile rank in a distribution. </para>

  
  <para>The CDF is a function of <emphasis>x</emphasis>, where <emphasis>x</emphasis> is any value that might appear in the distribution. To evaluate CDF(<emphasis>x</emphasis>) for a particular value of <emphasis>x</emphasis>, we compute the fraction of the values in the sample less than (or equal to) <emphasis>x</emphasis>. </para>

  
  <para>Here’s what that looks like as a function that takes a sample, <literal>t</literal>, and a value, <literal>x</literal>: </para>

  
  <programlisting>def Cdf(t, x):
    count = 0.0
    for value in t:
        if value &lt;= x:
            count += 1.0

    prob = count / len(t)
    return prob</programlisting>

  
  <para>This function should look familiar; it is almost identical to <literal>PercentileRank</literal>, except that the result is in a probability in the range 0–1 rather than a percentile rank in the range 0–100. </para>

  
  <para>As an example, suppose a sample has the values {1, 2, 2, 3, 5}. Here are some values from its CDF: </para>

  
  <para><simplelist>
  <member> CDF(0) = 0 </member>
</simplelist> </para>

  
  <para><simplelist>
  <member> CDF(1) = 0.2 </member>
</simplelist> </para>

  
  <para><simplelist>
  <member> CDF(2) = 0.6 </member>
</simplelist> </para>

  
  <para><simplelist>
  <member> CDF(3) = 0.8 </member>
</simplelist> </para>

  
  <para><simplelist>
  <member> CDF(4) = 0.8 </member>
</simplelist> </para>

  
  <para><simplelist>
  <member> CDF(5) = 1 </member>
</simplelist> </para>

  
  <para>We can evaluate the CDF for any value of <emphasis>x</emphasis>, not just values that appear in the sample. If <emphasis>x</emphasis> is less than the smallest value in the sample, CDF(<emphasis>x</emphasis>) is 0. If <emphasis>x</emphasis> is greater than the largest value, CDF(<emphasis>x</emphasis>) is 1. </para>

  
  <figure id="example_cdf">
  
  <title>Example of a CDF.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/example_cdf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para><xref linkend="example_cdf" /> is a graphical representation of this CDF. The CDF of a sample is a step function. In the next chapter we will see distributions whose CDFs are continuous functions. </para>

</sect1><sect1 id="a0000000057" remap="section">
  <title>Representing CDFs</title>
    
  
  <para> <indexterm>
  <primary sortas="cdf.py">Cdf.py</primary>

</indexterm> <indexterm>
  <primary>Cdf object</primary>

</indexterm> </para>

  
  <para>I have written a module called <literal>Cdf</literal> that provides a class named <literal>Cdf</literal> that represents CDFs. You can read the documentation of this module at <ulink url="http://thinkstats.com/Cdf.html">http://thinkstats.com/Cdf.html</ulink> and you can download it from <ulink url="http://thinkstats.com/Cdf.py">http://thinkstats.com/Cdf.py</ulink>. </para>

  
  <para>Cdfs are implemented with two sorted lists: <literal>xs</literal>, which contains the values, and <literal>ps</literal>, which contains the probabilities. The most important methods Cdfs provide are: </para>

  
  <para><variablelist>
  <varlistentry>
    <term><literal>Prob(x)</literal>:</term>
      <listitem>
  
  <para>Given a value <emphasis>x</emphasis>, computes the probability <emphasis>p</emphasis> = CDF(<emphasis>x</emphasis>). </para>
</listitem>
  </varlistentry><varlistentry>
    <term><literal>Value(p)</literal>:</term>
      <listitem>
  
  <para>Given a probability <emphasis>p</emphasis>, computes the corresponding value, <emphasis>x</emphasis>; that is, the inverse CDF of <emphasis>p</emphasis>. </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>Because <literal>xs</literal> and <literal>ps</literal> are sorted, these operations can use the bisection algorithm, which is efficient. The run time is proportional to the logarithm of the number of values; see <ulink url="http://wikipedia.org/wiki/Time_complexity">http://wikipedia.org/wiki/Time_complexity</ulink>. <indexterm>
  <primary>bisection algorithm</primary>

</indexterm> </para>

  
  <para>Cdfs also provide <literal>Render</literal>, which returns two lists, <literal>xs</literal> and <literal>ps</literal>, suitable for plotting the CDF. Because the CDF is a step function, these lists have two elements for each unique value in the distribution. </para>

  
  <para>The Cdf module provides several functions for making Cdfs, including <literal>MakeCdfFromList</literal>, which takes a sequence of values and returns their Cdf. </para>

  
  <para>Finally, <literal>myplot.py</literal> provides functions named <literal>Cdf</literal> and <literal>Cdfs</literal> that plot Cdfs as lines. <indexterm>
  <primary>myplot.py</primary>

</indexterm> </para>

  
  <example id="a0000000058">
  <title></title>
  
  
  <para> Download <literal>Cdf.py</literal> and <literal remap="verb">relay.py</literal> (see <xref linkend="relay" />) and generate a plot that shows the CDF of running speeds. Which gives you a better sense of the shape of the distribution, the PMF or the CDF? You can download a solution from <ulink url="http://thinkstats.com/relay_cdf.py">http://thinkstats.com/relay_cdf.py</ulink>. <indexterm>
  <primary sortas="cdf.py">Cdf.py</primary>

</indexterm> <indexterm>
  <primary>relay_cdf.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="birth_weights" remap="section">
  <title>Back to the survey data</title>
    
  
  <para> <anchor id="a0000000059" /> <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> </para>

  
  <para><xref linkend="nsfg_birthwgt_cdf" /> shows the CDFs of birth weight for first babies and others in the NSFG dataset. </para>

  
  <figure id="nsfg_birthwgt_cdf">
  
  <title>CDF of birth weights.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/nsfg_birthwgt_cdf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>This figure makes the shape of the distributions, and the differences between them, much clearer. We can see that first babies are slightly lighter throughout the distribution, with a larger discrepancy above the mean. <indexterm>
  <primary>shape</primary>

</indexterm> </para>

  
  <example id="a0000000061">
  <title></title>
  
  
  <para> How much did you weigh at birth? If you don’t know, call your mother or someone else who knows. Using the pooled data (all live births), compute the distribution of birth weights and use it to find your percentile rank. If you were a first baby, find your percentile rank in the distribution for first babies. Otherwise use the distribution for others. How big is the difference between your percentile ranks in the two distributions? </para>

</example>

  
  <example id="a0000000062">
  <title></title>
  
  
  <para> Suppose you and your classmates compute the percentile rank of your birth weights and then compute the CDF of the percentile ranks. What do you expect it to look like? Hint: what fraction of the class do you expect to be above the median? <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> </para>

</example>

</sect1><sect1 id="a0000000063" remap="section">
  <title>Conditional distributions</title>
    
  
  <para> <indexterm>
  <primary>conditional distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>conditional</secondary>
</indexterm> </para>

  
  <para>A <emphasis role="bold">conditional distribution</emphasis> is the distribution of a subset of the data which is selected according to a condition. </para>

  
  <para>For example, if you are above average in weight, but way above average in height, then you might be relatively light for your height. Here’s how you could make that claim more precise. </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Select a cohort of people who are the same height as you (within some range). <indexterm>
  <primary>cohort</primary>

</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>Find the CDF of weight for those people. </para>
</listitem>
  
  <listitem>
  
  <para>Find the percentile rank of your weight in that distribution. </para>
</listitem>
  
</orderedlist></para>

  
  <para>Percentile ranks are useful for comparing measurements from different tests, or tests applied to different groups. <indexterm>
  <primary>percentile rank</primary>

</indexterm> <indexterm>
  <primary>rank</primary>
<secondary>percentile</secondary>
</indexterm> </para>

  
  <para>For example, people who compete in foot races are usually grouped by age and gender. To compare people in different groups, you can convert race times to percentile ranks. </para>

  
  <example id="a0000000064">
  <title></title>
  
  
  <para> I recently ran the James Joyce Ramble 10K in Dedham MA. The results are available from <ulink url="http://coolrunning.com/results/10/ma/Apr25_27thAn_set1.shtml">http://coolrunning.com/results/10/ma/Apr25_27thAn_set1.shtml</ulink>. Go to that page and find my results. I came in 97th in a field of 1633, so what is my percentile rank in the field? <indexterm>
  <primary>James Joyce Ramble</primary>

</indexterm> <indexterm>
  <primary>race time</primary>

</indexterm> </para>

  
  <para>In my division (M4049 means “male between 40 and 49 years of age”) I came in 26th out of 256. What is my percentile rank in my division? </para>

  
  <para>If I am still running in 10 years (and I hope I am), I will be in the M5059 division. Assuming that my percentile rank in my division is the same, how much slower should I expect to be? </para>

  
  <para>I maintain a friendly rivalry with a student of mine who is in the F2029 division. How fast does she have to run her next 10K to “beat” me in terms of percentile ranks? </para>

</example>

</sect1><sect1 id="random" remap="section">
  <title>Random numbers</title>
    
  
  <para> <anchor id="a0000000065" /> <indexterm>
  <primary>random number</primary>

</indexterm> </para>

  
  <para>CDFs are useful for generating random numbers with a given distribution. Here’s how: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>Choose a random probability in the range 0–1. </para>
</listitem>
  
    <listitem>
  
  <para>Use <literal>Cdf.Value</literal> to find the value in the distribution that corresponds to the probability you chose. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>It might not be obvious why this works, but since it is easier to implement than to explain, let’s try it out. </para>

  
  <example id="a0000000066">
  <title></title>
  
  
  <para> Write a function called <literal>Sample</literal>, that takes a Cdf and an integer, <emphasis>n</emphasis>, and returns a list of <emphasis>n</emphasis> values chosen at random from the Cdf. Hint: use <literal>random.random</literal>. You will find a solution to this exercise in <literal>Cdf.py</literal>. <indexterm>
  <primary>inverse CDF algorithm</primary>

</indexterm> <indexterm>
  <primary sortas="cdf.py">Cdf.py</primary>

</indexterm> </para>

  
  <para>Using the distribution of birth weights from the NSFG, generate a random sample with 1000 elements. Compute the CDF of the sample. Make a plot that shows the original CDF and the CDF of the random sample. For large values of <emphasis>n</emphasis>, the distributions should be the same. <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

</example>

  
  <para>This process, generating a random sample based on a measured sample, is called <emphasis role="bold">resampling</emphasis>. <indexterm>
  <primary>resampling</primary>

</indexterm> <indexterm>
  <primary>sampling</primary>

</indexterm> <indexterm>
  <primary>replacement, sampling</primary>

</indexterm> </para>

  
  <para>There are two ways to draw a sample from a population: with and without replacement. If you imagine drawing marbles from an urn<footnote><para>The marbles-in-an-urn scenario is a standard model for random sampling processes (see <ulink url="http://wikipedia.org/wiki/Urn_problem">http://wikipedia.org/wiki/Urn_problem</ulink>).</para></footnote>, “replacement” means putting the marbles back as you go (and stirring), so the population is the same for every draw. “Without replacement,” means that each marble can only be drawn once, so the remaining population is different after each draw. </para>

  
  <para>In Python, sampling with replacement can be implemented with <literal>random.random</literal> to choose a percentile rank, or <literal>random.choice</literal> to choose an element from a sequence. Sampling without replacement is provided by <literal>random.sample</literal>. <indexterm>
  <primary>random module</primary>

</indexterm> </para>

  
  <example id="a0000000067">
  <title></title>
  
  
  <para> The numbers generated by <literal>random.random</literal> are supposed to be uniform between 0 and 1; that is, every value in the range should have the same probability. </para>

  
  <para>Generate 1000 numbers from <literal>random.random</literal> and plot their PMF and CDF. Can you tell whether they are uniform? </para>

  
  <para>You can read about the uniform distribution at <ulink url="http://wikipedia.org/wiki/Uniform_distribution_(discrete)">http://wikipedia.org/wiki/Uniform_distribution_(discrete)</ulink>. <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> </para>

</example>

</sect1><sect1 id="a0000000068" remap="section">
  <title>Summary statistics revisited</title>
    
  
  <para> <indexterm>
  <primary>summary statistic</primary>

</indexterm> <indexterm>
  <primary>interquartile range</primary>

</indexterm> <indexterm>
  <primary>quartile</primary>

</indexterm> <indexterm>
  <primary>percentile</primary>

</indexterm> <indexterm>
  <primary>median</primary>

</indexterm> <indexterm>
  <primary>central tendency</primary>

</indexterm> <indexterm>
  <primary>spread</primary>

</indexterm> </para>

  
  <para>Once you have computed a CDF, it is easy to compute other summary statistics. The median is just the 50th percentile<footnote><para>You might see other definitions of the median. In particular, some sources suggest that if you have an even number of elements in a sample, the median is the average of the middle two elements. This is an unnecessary special case, and it has the odd effect of generating a value that is not in the sample. As far as I’m concerned, the median is the 50th percentile. Period.</para></footnote>. The 25th and 75th percentiles are often used to check whether a distribution is symmetric, and their difference, which is called the <emphasis role="bold">interquartile range</emphasis>, measures the spread. </para>

  
  <example id="a0000000069">
  <title></title>
  
  
  <para> Write a function called <literal>Median</literal> that takes a Cdf and computes the median, and one called <literal>Interquartile</literal> that computes the interquartile range. </para>

  
  <para>Compute the 25th, 50th, and 75th percentiles of the birth weight CDF. Do these values suggest that the distribution is symmetric? <indexterm>
  <primary>symmetric</primary>

</indexterm> <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> </para>

</example>

</sect1><sect1 id="a0000000070" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>percentile rank:</term>
      <listitem>
  
  <para>The percentage of values in a distribution that are less than or equal to a given value. <indexterm>
  <primary>percentile rank</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>CDF:</term>
      <listitem>
  
  <para>Cumulative distribution function, a function that maps from values to their percentile ranks. <indexterm>
  <primary>CDF</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>percentile:</term>
      <listitem>
  
  <para>The value associated with a given percentile rank. <indexterm>
  <primary>percentile</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>conditional distribution:</term>
      <listitem>
  
  <para>A distribution computed under the assumption that some condition holds. <indexterm>
  <primary>conditional distribution</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>resampling:</term>
      <listitem>
  
  <para>The process of generating a random sample from a distribution that was computed from a sample. <indexterm>
  <primary>resampling</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>replacement:</term>
      <listitem>
  
  <para>During a sampling process, “replacement” indicates that the population is the same for every sample. “Without replacement” indicates that each element can be selected only once. <indexterm>
  <primary>replacement</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>interquartile range:</term>
      <listitem>
  
  <para>A measure of spread, the difference between the 75th and 25th percentiles. <indexterm>
  <primary>interquartile range</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="continuous">
  <title>Continuous distributions</title>
  
  
  <para> <anchor id="a0000000071" /> <indexterm>
  <primary>continuous distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>continuous</secondary>
</indexterm> <indexterm>
  <primary>empirical distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>empirical</secondary>
</indexterm> </para>

  
  <para>The distributions we have used so far are called <emphasis role="bold">empirical distributions</emphasis> because they are based on empirical observations, which are necessarily finite samples. </para>

  
  <para>The alternative is a <emphasis role="bold">continuous distribution</emphasis>, which is characterized by a CDF that is a continuous function (as opposed to a step function). Many real world phenomena can be approximated by continuous distributions. </para>
<sect1 id="a0000000072" remap="section">
  <title>The exponential distribution</title>
    
  
  <para> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>

  
  <para>I’ll start with the exponential distribution because it is easy to work with. In the real world, exponential distributions come up when we look at a series of events and measure the times between events, which are called <emphasis role="bold">interarrival times</emphasis>. If the events are equally likely to occur at any time, the distribution of interarrival times tends to look like an exponential distribution. <indexterm>
  <primary>interarrival time</primary>

</indexterm> </para>

  
  <para>The CDF of the exponential distribution is: </para>

  
  <para><informalequation id="a0000000073" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0005.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  CDF(x) = 1 - e^{-\lambda x}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The parameter, <emphasis>λ</emphasis>, determines the shape of the distribution. <xref linkend="expo_cdf" /> shows what this CDF looks like with <emphasis>λ</emphasis> = 2. <indexterm>
  <primary>parameter</primary>

</indexterm> </para>

  
  <figure id="expo_cdf">
  
  <title>CDF of exponential distribution.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/expo_cdf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>In general, the mean of an exponential distribution is 1/<emphasis>λ</emphasis>, so the mean of this distribution is 0.5. The median is log(2)/<emphasis>λ</emphasis>, which is roughly 0.35. <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>median</primary>

</indexterm> <indexterm>
  <primary>Australia</primary>

</indexterm> <indexterm>
  <primary>Brisbane</primary>

</indexterm> </para>

  
  <para>To see an example of a distribution that is approximately exponential, we will look at the interarrival time of babies. On December 18, 1997, 44 babies were born in a hospital in Brisbane, Australia<footnote><para>This example is based on information and data from Dunn, “A Simple Dataset for Demonstrating Common Distributions,” Journal of Statistics Education v.7, n.3 (1999).</para></footnote>. The times of birth for all 44 babies were reported in the local paper; you can download the data from <ulink url="http://thinkstats.com/babyboom.dat">http://thinkstats.com/babyboom.dat</ulink>. <indexterm>
  <primary>birth time</primary>

</indexterm> </para>

  
  <para><xref linkend="interarrival_cdf" /> shows the CDF of the interarrival times in minutes. It seems to have the general shape of an exponential distribution, but how can we tell? <indexterm>
  <primary>complementary CDF</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>
<secondary>complementary</secondary>
</indexterm> <indexterm>
  <primary>CCDF</primary>

</indexterm> </para>

  
  <para>One way is to plot the complementary CDF, 1 − CDF(<emphasis>x</emphasis>), on a log-<emphasis>y</emphasis> scale. For data from an exponential distribution, the result is a straight line. Let’s see why that works. </para>

  
  <para>If you plot the complementary CDF (CCDF) of a dataset that you think is exponential, you expect to see a function like: </para>

  
  <para><informalequation id="a0000000075" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0006.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  y \approx e^{-\lambda x}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Taking the log of both sides yields: </para>

  
  <para><simplelist>
  <member> log <emphasis>y</emphasis> ≈ -<emphasis>λ</emphasis><emphasis>x</emphasis></member>
</simplelist> </para>

  
  <para>So on a log-<emphasis>y</emphasis> scale the CCDF is a straight line with slope −<emphasis>λ</emphasis>. <indexterm>
  <primary>logarithmic scale</primary>

</indexterm> <indexterm>
  <primary>complementary CDF</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>
<secondary>complementary</secondary>
</indexterm> <indexterm>
  <primary>CCDF</primary>

</indexterm> </para>

  
  <figure id="interarrival_cdf">
  
  <title>CDF of interarrival times</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/interarrivals.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <figure id="interarrival_ccdf">
  
  <title>CCDF of interarrival times.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/interarrivals_logy.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para><xref linkend="interarrival_ccdf" /> shows the CCDF of the interarrivals on a log-<emphasis>y</emphasis> scale. It is not exactly straight, which suggests that the exponential distribution is only an approximation. Most likely the underlying assumption—that a birth is equally likely at any time of day—is not exactly true. </para>

  
  <example id="a0000000078">
  <title></title>
  
  
  <para> For small values of <emphasis>n</emphasis>, we don’t expect an empirical distribution to fit a continuous distribution exactly. One way to evaluate the quality of fit is to generate a sample from a continuous distribution and see how well it matches the data. <indexterm>
  <primary>empirical distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>empirical</secondary>
</indexterm> <indexterm>
  <primary>random module</primary>

</indexterm> </para>

  
  <para>The function <literal>expovariate</literal> in the <literal>random</literal> module generates random values from an exponential distribution with a given value of <emphasis>λ</emphasis>. Use it to generate 44 values from an exponential distribution with mean 32.6. Plot the CCDF on a log-<emphasis>y</emphasis> scale and compare it to <xref linkend="interarrival_ccdf" />. <indexterm>
  <primary>pyplot</primary>

</indexterm> </para>

  
  <para>Hint: You can use the function <literal>pyplot.yscale</literal> to plot the <emphasis>y</emphasis> axis on a log scale. </para>

  
  <para>Or, if you use <literal>myplot</literal>, the <literal>Cdf</literal> function takes a boolean option, <literal>complement</literal>, that determines whether to plot the CDF or CCDF, and string options, <literal>xscale</literal> and <literal>yscale</literal>, that transform the axes; to plot a CCDF on a log-<emphasis>y</emphasis> scale: </para>

  
  <programlisting>myplot.Cdf(cdf, complement=True, xscale='linear', yscale='log')</programlisting>

</example>

  
  <example id="a0000000079">
  <title></title>
  
  
  <para> Collect the birthdays of the students in your class, sort them, and compute the interarrival times in days. Plot the CDF of the interarrival times and the CCDF on a log-<emphasis>y</emphasis> scale. Does it look like an exponential distribution? <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>birthday</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000080" remap="section">
  <title>The Pareto distribution</title>
    
  
  <para> <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> <indexterm>
  <primary>Pareto, Vilfredo</primary>

</indexterm> </para>

  
  <para>The Pareto distribution is named after the economist Vilfredo Pareto, who used it to describe the distribution of wealth (see <ulink url="http://wikipedia.org/wiki/Pareto_distribution">http://wikipedia.org/wiki/Pareto_distribution</ulink>). Since then, it has been used to describe phenomena in the natural and social sciences including sizes of cities and towns, sand particles and meteorites, forest fires and earthquakes. <indexterm>
  <primary>CDF</primary>

</indexterm> </para>

  
  <para>The CDF of the Pareto distribution is: </para>

  
  <para><informalequation id="a0000000081" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0007.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  CDF(x) = 1 - \left( \frac{x}{x_ m} \right) ^{-\alpha }  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The parameters <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis> and <emphasis>α</emphasis> determine the location and shape of the distribution. <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis> is the minimum possible value. <xref linkend="weibull" /> shows the CDF of a Pareto distribution with parameters <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis> = 0.5 and <emphasis>α</emphasis> = 1. <indexterm>
  <primary>parameter</primary>

</indexterm> </para>

  
  <figure id="pareto_cdf">
  
  <title>CDF of a Pareto distribution.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/pareto_cdf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>The median of this distribution is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0008.png" depth="11.500px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$x_ m 2^{1/\alpha }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>, which is 1, but the 95th percentile is 10. By contrast, the exponential distribution with median 1 has 95th percentile of only 1.5. <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>median</primary>

</indexterm> <indexterm>
  <primary>logarithmic scale</primary>

</indexterm> </para>

  
  <para>There is a simple visual test that indicates whether an empirical distribution fits a Pareto distribution: on a log-log scale, the CCDF looks like a straight line. If you plot a the CCDF of a sample from a Pareto distribution on a linear scale, you expect to see a function like: </para>

  
  <para><informalequation id="a0000000083" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0009.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  y \approx \left( \frac{x}{x_ m} \right) ^{-\alpha }  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Taking the log of both sides yields: </para>

  
  <para><simplelist>
  <member> log y ≈−<emphasis>α</emphasis> (log <emphasis>x</emphasis> − log x<emphasis><subscript>m</subscript></emphasis>) </member>
</simplelist> </para>

  
  <para>So if you plot log <emphasis>y</emphasis> versus log <emphasis>x</emphasis>, it should look like a straight line with slope −<emphasis>α</emphasis> and intercept −<emphasis>α</emphasis> log <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis>. </para>

  
  <example id="a0000000084">
  <title></title>
  
  
  <para> The <literal>random</literal> module provides <literal>paretovariate</literal>, which generates random values from a Pareto distribution. It takes a parameter for <emphasis>α</emphasis>, but not <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis>. The default value for <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis> is 1; you can generate a distribution with a different parameter by multiplying by <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis>. <indexterm>
  <primary>random module</primary>

</indexterm> <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> </para>

  
  <para>Write a wrapper function named <literal>paretovariate</literal> that takes <emphasis>α</emphasis> and <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis> as parameters and uses <literal>random.paretovariate</literal> to generate values from a two-parameter Pareto distribution. <indexterm>
  <primary>parameter</primary>

</indexterm> </para>

  
  <para>Use your function to generate a sample from a Pareto distribution. Compute the CCDF and plot it on a log-log scale. Is it a straight line? What is the slope? <indexterm>
  <primary>complementary CDF</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>
<secondary>complementary</secondary>
</indexterm> <indexterm>
  <primary>CCDF</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000085">
  <title></title>
  
  
  <para> To get a feel for the Pareto distribution, imagine what the world would be like if the distribution of human height were Pareto. Choosing the parameters <emphasis>x</emphasis><emphasis><subscript>m</subscript></emphasis> = 100 cm and <emphasis>α</emphasis> = 1.7, we get a distribution with a reasonable minimum, 100 cm, and median, 150 cm. <indexterm>
  <primary>height</primary>

</indexterm> </para>

  
  <para>Generate 6 billion random values from this distribution. What is the mean of this sample? What fraction of the population is shorter than the mean? How tall is the tallest person in Pareto World? <indexterm>
  <primary>Pareto World</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000086">
  <title></title>
  
  
  <para> Zipf’s law is an observation about how often different words are used. The most common words have very high frequencies, but there are many unusual words, like “hapaxlegomenon,” that appear only a few times. Zipf’s law predicts that in a body of text, called a “corpus,” the distribution of word frequencies is roughly Pareto. <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> <indexterm>
  <primary>Zipf’s law</primary>

</indexterm> <indexterm>
  <primary>hapaxlegomenon</primary>

</indexterm> <indexterm>
  <primary>corpus</primary>

</indexterm> <indexterm>
  <primary>frequency</primary>

</indexterm> <indexterm>
  <primary>word frequency</primary>

</indexterm> </para>

  
  <para>Find a large corpus, in any language, in electronic format. Count how many times each word appears. Find the CCDF of the word counts and plot it on a log-log scale. Does Zipf’s law hold? What is the value of <emphasis>α</emphasis>, approximately? <indexterm>
  <primary>complementary CDF</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>
<secondary>complementary</secondary>
</indexterm> <indexterm>
  <primary>CCDF</primary>

</indexterm> </para>

</example>

  
  <example id="weibull">
  <title></title>
  
  
  

  
  <para>The Weibull distribution is a generalization of the exponential distribution that comes up in failure analysis (see <ulink url="http://wikipedia.org/wiki/Weibull_distribution">http://wikipedia.org/wiki/Weibull_distribution</ulink>). Its CDF is </para>

  
  <para><informalequation id="a0000000088" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0010.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  CDF(x) = 1 - e^{-(x / \lambda )^ k}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Can you find a transformation that makes a Weibull distribution look like a straight line? What do the slope and intercept of the line indicate? <indexterm>
  <primary>Weibull distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Weibull</secondary>
</indexterm> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>random module</primary>

</indexterm> </para>

  
  <para>Use <literal>random.weibullvariate</literal> to generate a sample from a Weibull distribution and use it to test your transformation. </para>

</example>

</sect1><sect1 id="normal" remap="section">
  <title>The normal distribution</title>
    
  
  <para> <anchor id="a0000000089" /> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  

  
  <para>The normal distribution, also called Gaussian, is the most commonly used because it describes so many phenomena, at least approximately. It turns out that there is a good reason for its ubiquity, which we will get to in <xref linkend="CLT" />. <indexterm>
  <primary>error function</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>

</indexterm> </para>

  
  <para>The normal distribution has many properties that make it amenable for analysis, but the CDF is not one of them. Unlike the other distributions we have looked at, there is no closed-form expression for the normal CDF; the most common alternative is to write it in terms of the <emphasis role="bold">error function</emphasis>, which is a special function written erf(<emphasis>x</emphasis>): </para>

  
  <para><informalequation id="a0000000090" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0011.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  CDF(x) = \frac{1}{2} \left[ 1 + \mathrm{erf}\left( \frac{x - \mu }{\sigma \sqrt {2}} \right) \right]  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para><informalequation id="a0000000091" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0012.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{erf}(x) = \frac{2}{\sqrt {\pi }} \int _{0}^ x e^{-t^2} dt  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The parameters <emphasis>μ</emphasis> and <emphasis>σ</emphasis> determine the mean and standard deviation of the distribution. <indexterm>
  <primary>parameter</primary>

</indexterm> <indexterm>
  <primary>erf.py</primary>

</indexterm> </para>

  
  <para>If these formulas make your eyes hurt, don’t worry; they are easy to implement in Python<footnote><para>As of Python 3.2, it is even easier; <literal>erf</literal> is in the <literal>math</literal> module.</para></footnote>. There are many fast and accurate ways to approximate erf(<emphasis>x</emphasis>). You can download one of them from <ulink url="http://thinkstats.com/erf.py">http://thinkstats.com/erf.py</ulink>, which provides functions named <literal>erf</literal> and <literal>NormalCdf</literal>. </para>

  
  <para><xref linkend="normal_cdf" /> shows the CDF of the normal distribution with parameters <emphasis>μ</emphasis> = 2.0 and <emphasis>σ</emphasis> = 0.5. The sigmoid shape of this curve is a recognizable characteristic of a normal distribution. </para>

  
  <figure id="normal_cdf">
  
  <title>CDF of a normal distribution.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/normal_cdf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>In the previous chapter we looked at the distribution of birth weights in the NSFG. <xref linkend="nsfg_birthwgt_model" /> shows the empirical CDF of weights for all live births and the CDF of a normal distribution with the same mean and variance. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> </para>

  
  <figure id="nsfg_birthwgt_model">
  
  <title>CDF of birth weights with a normal model.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/nsfg_birthwgt_model.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>The normal distribution is a good model for this dataset. A <emphasis role="bold">model</emphasis> is a useful simplification. In this case it is useful because we can summarize the entire distribution with just two numbers, <emphasis>μ</emphasis> = 116.5 and <emphasis>σ</emphasis> = 19.9, and the resulting error (difference between the model and the data) is small. <indexterm>
  <primary>model</primary>

</indexterm> <indexterm>
  <primary>percentile</primary>

</indexterm> </para>

  
  <para>Below the 10th percentile there is a discrepancy between the data and the model; there are more light babies than we would expect in a normal distribution. If we are interested in studying preterm babies, it would be important to get this part of the distribution right, so it might not be appropriate to use the normal model. </para>

  
  <example id="a0000000094">
  <title></title>
  
  
  <para> The Wechsler Adult Intelligence Scale is a test that is intended to measure intelligence<footnote><para>Whether it does or not is a fascinating controversy that I invite you to investigate at your leisure.</para></footnote>. Results are transformed so that the distribution of scores in the general population is normal with <emphasis>μ</emphasis> = 100 and <emphasis>σ</emphasis> = 15. <indexterm>
  <primary>Wechsler Adult Intelligence Scale</primary>

</indexterm> <indexterm>
  <primary>Adult Intelligence Scale</primary>

</indexterm> <indexterm>
  <primary>WAIS</primary>

</indexterm> <indexterm>
  <primary>IQ</primary>

</indexterm> <indexterm>
  <primary>intelligence</primary>

</indexterm> </para>

  
  <para>Use <literal>erf.NormalCdf</literal> to investigate the frequency of rare events in a normal distribution. What fraction of the population has an IQ greater than the mean? What fraction is over 115? 130? 145? </para>

  
  <para>A “six-sigma” event is a value that exceeds the mean by 6 standard deviations, so a six-sigma IQ is 190. In a world of 6 billion people, how many do we expect to have an IQ of 190 or more<footnote><para>On this topic, you might be interested to read <ulink url="http://wikipedia.org/wiki/Christopher_Langan">http://wikipedia.org/wiki/Christopher_Langan</ulink>.</para></footnote>? <indexterm>
  <primary>Langan, Christopher</primary>

</indexterm> <indexterm>
  <primary>six-sigma event</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000095">
  <title></title>
  
  
  <para> Plot the CDF of pregnancy lengths for all live births. Does it look like a normal distribution? <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>Compute the mean and variance of the sample and plot the normal distribution with the same parameters. Is the normal distribution a good model for this data? If you had to summarize this distribution with two statistics, what statistics would you choose? </para>

</example>

</sect1><sect1 id="a0000000096" remap="section">
  <title>Normal probability plot</title>
    
  
  <para> <indexterm>
  <primary>normal probability plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>normal probability</secondary>
</indexterm> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>Weibull distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Weibull</secondary>
</indexterm> <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> </para>

  
  <para>For the exponential, Pareto and Weibull distributions, there are simple transformations we can use to test whether a continuous distribution is a good model of a dataset. <indexterm>
  <primary>model</primary>

</indexterm> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>For the normal distribution there is no such transformation, but there is an alternative called a <emphasis role="bold">normal probability plot</emphasis>. It is based on <emphasis role="bold">rankits</emphasis>: if you generate <emphasis>n</emphasis> values from a normal distribution and sort them, the <emphasis>k</emphasis>th rankit is the mean of the distribution for the <emphasis>k</emphasis>th value. <indexterm>
  <primary>rankit</primary>

</indexterm> </para>

  
  <example id="a0000000097">
  <title></title>
  
  
  <para> Write a function called <literal>Sample</literal> that generates 6 samples from a normal distribution with <emphasis>μ</emphasis> = 0 and <emphasis>σ</emphasis> = 1. Sort and return the values. </para>

  
  <para>Write a function called <literal>Samples</literal> that calls <literal>Sample</literal> 1000 times and returns a list of 1000 lists. </para>

  
  <para>If you apply <literal>zip</literal> to this list of lists, the result is 6 lists with 1000 values in each. Compute the mean of each of these lists and print the results. I predict that you will get something like this: </para>

  
  <para>{−1.2672, −0.6418, −0.2016, 0.2016, 0.6418, 1.2672} </para>

  
  <para>If you increase the number of times you call <literal>Sample</literal>, the results should converge on these values. </para>

</example>

  
  <para>Computing rankits exactly is moderately difficult, but there are numerical methods for approximating them. And there is a quick-and-dirty method that is even easier to implement: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>From a normal distribution with <emphasis>μ</emphasis> = 0 and <emphasis>σ</emphasis> = 1, generate a sample with the same size as your dataset and sort it. </para>
</listitem>
  
  <listitem>
  
  <para>Sort the values in the dataset. </para>
</listitem>
  
  <listitem>
  
  <para>Plot the sorted values from your dataset versus the random values. </para>
</listitem>
  
</orderedlist></para>

  
  <para>For large datasets, this method works well. For smaller datasets, you can improve it by generating <emphasis>m</emphasis>(<emphasis>n</emphasis>+1) − 1 values from a normal distribution, where <emphasis>n</emphasis> is the size of the dataset and <emphasis>m</emphasis> is a multiplier. Then select every <emphasis>m</emphasis>th element, starting with the <emphasis>m</emphasis>th. </para>

  
  <para>This method works with other distributions as well, as long as you know how to generate a random sample. </para>

  
  <para><xref linkend="nsfg_birthwgt_normal" /> is a quick-and-dirty normal probability plot for the birth weight data. <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> </para>

  
  <figure id="nsfg_birthwgt_normal">
  
  <title>Normal probability plot of birth weights.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/nsfg_birthwgt_normal.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>The curvature in this plot suggests that there are deviations from a normal distribution; nevertheless, it is a good (enough) model for many purposes. <indexterm>
  <primary>model</primary>

</indexterm> </para>

  
  <example id="a0000000099">
  <title></title>
  
  
  <para> Write a function called <literal>NormalPlot</literal> that takes a sequence of values and generates a normal probability plot. You can download a solution from <ulink url="http://thinkstats.com/rankit.py">http://thinkstats.com/rankit.py</ulink>. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> <indexterm>
  <primary>rankit.py</primary>

</indexterm> <indexterm>
  <primary>relay.py</primary>

</indexterm> <indexterm>
  <primary>relay_normal.py</primary>

</indexterm> <indexterm>
  <primary>relay race</primary>

</indexterm> <indexterm>
  <primary>race</primary>
<secondary>relay</secondary>
</indexterm> </para>

  
  <para>Use the running speeds from <literal>relay.py</literal> to generate a normal probability plot. Is the normal distribution a good model for this data? You can download a solution from <ulink url="http://thinkstats.com/relay_normal.py">http://thinkstats.com/relay_normal.py</ulink>. </para>

</example>

</sect1><sect1 id="lognormal" remap="section">
  <title>The lognormal distribution</title>
    
  
  <para> <anchor id="a0000000100" /> <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> <indexterm>
  <primary>CDF</primary>

</indexterm> </para>

  
  <para>If the logarithms of a set of values have a normal distribution, the values have a <emphasis role="bold">lognormal</emphasis> distribution. The CDF of the lognormal distribution is the same as the CDF of the normal distribution, with log <emphasis>x</emphasis>substituted for <emphasis>x</emphasis>. </para>

  
  <para><simplelist>
  <member> CDF<emphasis><subscript>lognormal</subscript></emphasis>(<emphasis>x</emphasis>) = CDF<emphasis><subscript>normal</subscript></emphasis>(log <emphasis>x</emphasis>) </member>
</simplelist> </para>

  
  <para>The parameters of the lognormal distribution are usually denoted <emphasis>μ</emphasis> and <emphasis>σ</emphasis>. But remember that these parameters are <emphasis>not</emphasis> the mean and standard deviation; the mean of a lognormal distribution is exp(<emphasis>μ</emphasis> + <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>/2) and the standard deviation is ugly<footnote><para>See <ulink url="http://wikipedia.org/wiki/Log-normal_distribution">http://wikipedia.org/wiki/Log-normal_distribution</ulink>.</para></footnote>. <indexterm>
  <primary>parameter</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>adult</secondary>
</indexterm> <indexterm>
  <primary>adult weight</primary>

</indexterm> </para>

  
  <para>It turns out that the distribution of weights for adults is approximately lognormal<footnote><para>I was tipped off to this possibility by a comment (without citation) at <ulink url="http://mathworld.wolfram.com/LogNormalDistribution.html">http://mathworld.wolfram.com/LogNormalDistribution.html</ulink>. Subsequently I found a paper that proposes the log transform and suggests a cause: Penman and Johnson, “The Changing Shape of the Body Mass Index Distribution Curve in the Population,” Preventing Chronic Disease, 2006 July; 3(3): A74. Online at <ulink url="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1636707">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1636707</ulink>.</para></footnote>. </para>

  
  <para>The National Center for Chronic Disease Prevention and Health Promotion conducts an annual survey as part of the Behavioral Risk Factor Surveillance System (BRFSS)<footnote><para>Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System Survey Data. Atlanta, Georgia: U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, 2008.</para></footnote>. In 2008, they interviewed 414,509 respondents and asked about their demographics, health and health risks. <indexterm>
  <primary>Behavioral Risk Factor Surveillance System</primary>

</indexterm> <indexterm>
  <primary>BRFSS</primary>

</indexterm> </para>

  
  <figure id="brfss_weight_log">
  
   <title>CDF of adult weights (log transform).</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/brfss_weight_log.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>Among the data they collected are the weights in kilograms of 398,484 respondents. <xref linkend="brfss_weight_log" /> shows the distribution of log <emphasis>w</emphasis>, where <emphasis>w</emphasis>is weight in kilograms, along with a normal model. <indexterm>
  <primary>respondent</primary>

</indexterm> <indexterm>
  <primary>model</primary>

</indexterm> </para>

  
  <para>The normal model is a good fit for the data, although the highest weights exceed what we expect from the normal model even after the log transform. Since the distribution of log <emphasis>w</emphasis>fits a normal distribution, we conclude that <emphasis>w</emphasis>fits a lognormal distribution. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> </para>

  
  <example id="a0000000102">
  <title></title>
  
  
  <para> Download the BRFSS data from <ulink url="http://thinkstats.com/CDBRFS08.ASC.gz">http://thinkstats.com/CDBRFS08.ASC.gz</ulink>, and my code for reading it from <ulink url="http://thinkstats.com/brfss.py">http://thinkstats.com/brfss.py</ulink>. Run <literal>brfss.py</literal> and confirm that it prints summary statistics for a few of the variables. <indexterm>
  <primary>Behavioral Risk Factor Surveillance System</primary>

</indexterm> <indexterm>
  <primary>BRFSS</primary>

</indexterm> <indexterm>
  <primary>brfss.py</primary>

</indexterm> <indexterm>
  <primary>brfss_figs.py</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>adult</secondary>
</indexterm> <indexterm>
  <primary>adult weight</primary>

</indexterm> </para>

  
  <para>Write a program that reads adult weights from the BRFSS and generates normal probability plots for <emphasis>w</emphasis>and log <emphasis>w</emphasis>. You can download a solution from <ulink url="http://thinkstats.com/brfss_figs.py">http://thinkstats.com/brfss_figs.py</ulink>. </para>

</example>

  
  <example id="a0000000103">
  <title></title>
  
  
  <para> The distribution of populations for cities and towns has been proposed as an example of a real-world phenomenon that can be described with a Pareto distribution. <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> <indexterm>
  <primary>U.S.Census Bureau</primary>

</indexterm> <indexterm>
  <primary>population</primary>

</indexterm> <indexterm>
  <primary>city size</primary>

</indexterm> </para>

  
  <para>The U.S. Census Bureau publishes data on the population of every incorporated city and town in the United States. I have written a small program that downloads this data and stores it in a file. You can download it from <ulink url="http://thinkstats.com/populations.py">http://thinkstats.com/populations.py</ulink>. <indexterm>
  <primary>populations.py</primary>

</indexterm> </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Read over the program to make sure you know what it does; then run it to download and process the data. </para>
</listitem>
  
  <listitem>
  
  <para>Write a program that computes and plots the distribution of populations for the 14,593 cities and towns in the dataset. </para>
</listitem>
  
  <listitem>
  
  <para>Plot the CDF on linear and log-<emphasis>x</emphasis> scales so you can get a sense of the shape of the distribution. Then plot the CCDF on a log-log scale to see if it has the characteristic shape of a Pareto distribution. <indexterm>
  <primary>complementary CDF</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>
<secondary>complementary</secondary>
</indexterm> <indexterm>
  <primary>CCDF</primary>

</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>Try out the other transformations and plots in this chapter to see if there is a better model for this data. </para>
</listitem>
  
</orderedlist></para>

  
  <para>What conclusion do you draw about the distribution of sizes for cities and towns? You can download a solution from <ulink url="http://populations_cdf.py">http://populations_cdf.py</ulink>. <indexterm>
  <primary>populations_cdf.py</primary>

</indexterm> </para>

</example>

  
  <example id="irs">
  <title></title>
  
  
  

  
  <para>The Internal Revenue Service of the United States (IRS) provides data about income taxes at <ulink url="http://irs.gov/taxstats">http://irs.gov/taxstats</ulink>. <indexterm>
  <primary>Internal Revenue Service</primary>

</indexterm> <indexterm>
  <primary>IRS</primary>

</indexterm> <indexterm>
  <primary>income</primary>

</indexterm> <indexterm>
  <primary>taxes</primary>

</indexterm> </para>

  
  <para>One of their files, containing information about individual incomes for 2008, is available from <ulink url="http://thinkstats.com/08in11si.csv">http://thinkstats.com/08in11si.csv</ulink>. I converted it to a text format called CSV, which stands for “comma-separated values;” you can read it using the <literal>csv</literal> module. </para>

  
  <para>Extract the distribution of incomes from this dataset. Are any of the continuous distributions in this chapter a good model of the data? You can download a solution from <ulink url="http://thinkstats.com/irs.py">http://thinkstats.com/irs.py</ulink>. <indexterm>
  <primary>irs.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000105" remap="section">
  <title>Why model?</title>
    
  
  <para> <indexterm>
  <primary>model</primary>

</indexterm> </para>

  
  <para>At the beginning of this chapter I said that many real world phenomena can be modeled with continuous distributions. “So,” you might ask, “what?” <indexterm>
  <primary>abstraction</primary>

</indexterm> </para>

  
  <para>Like all models, continuous distributions are abstractions, which means they leave out details that are considered irrelevant. For example, an observed distribution might have measurement errors or quirks that are specific to the sample; continuous models smooth out these idiosyncrasies. <indexterm>
  <primary>smoothing</primary>

</indexterm> </para>

  
  <para>Continuous models are also a form of data compression. When a model fits a dataset well, a small set of parameters can summarize a large amount of data. <indexterm>
  <primary>parameter</primary>

</indexterm> <indexterm>
  <primary>compression</primary>

</indexterm> </para>

  
  <para>It is sometimes surprising when data from a natural phenomenon fit a continuous distribution, but these observations can lead to insight into physical systems. Sometimes we can explain why an observed distribution has a particular form. For example, Pareto distributions are often the result of generative processes with positive feedback (so-called preferential attachment processes: see <ulink url="http://wikipedia.org/wiki/Preferential_attachment">http://wikipedia.org/wiki/Preferential_attachment</ulink>.). <indexterm>
  <primary>preferential attachment</primary>

</indexterm> <indexterm>
  <primary>generative process</primary>

</indexterm> <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> <indexterm>
  <primary>analysis</primary>

</indexterm> </para>

  
  <para>Continuous distributions lend themselves to mathematical analysis, as we will see in <xref linkend="operations" />. </para>

</sect1><sect1 id="a0000000106" remap="section">
  <title>Generating random numbers</title>
    
  
  <para> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>random number</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>

</indexterm> <indexterm>
  <primary>inverse CDF algorithm</primary>

</indexterm> <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> </para>

  
  <para>Continuous CDFs are also useful for generating random numbers. If there is an efficient way to compute the inverse CDF, ICDF(<emphasis>p</emphasis>), we can generate random values with the appropriate distribution by choosing from a uniform distribution from 0 to 1, then choosing </para>

  
  <para><simplelist>
  <member> <emphasis>x</emphasis> = ICDF(<emphasis>p</emphasis>) </member>
</simplelist> </para>

  
  <para>For example, the CDF of the exponential distribution is </para>

  
  <para><informalequation id="a0000000107" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0013.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  p = 1 - e^{-\lambda x}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Solving for <emphasis>x</emphasis> yields: </para>

  
  <para><simplelist>
  <member> <emphasis>x</emphasis>= −log (1 − <emphasis>p</emphasis>) / <emphasis>λ</emphasis></member>
</simplelist> </para>

  
  <para>So in Python we can write </para>

  
  <programlisting>def expovariate(lam):
    p = random.random()
    x = -math.log(1-p) / lam
    return x</programlisting>

  
  <para>I called the parameter <literal remap="verb">lam</literal> because <literal remap="verb">lambda</literal> is a Python keyword. Most implementations of <literal>random.random</literal> can return 0 but not 1, so 1 − <emphasis>p</emphasis> can be 1 but not 0, which is good, because log 0 is undefined. <indexterm>
  <primary>random module</primary>

</indexterm> </para>

  
  <example id="a0000000108">
  <title></title>
  
  
  <para> Write a function named <literal remap="verb">weibullvariate</literal> that takes <literal remap="verb">lam</literal> and <literal remap="verb">k</literal> and returns a random value from the Weibull distribution with those parameters. <indexterm>
  <primary>Weibull distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Weibull</secondary>
</indexterm> <indexterm>
  <primary>parameter</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000109" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>empirical distribution:</term>
      <listitem>
  
  <para>The distribution of values in a sample. <indexterm>
  <primary>empirical distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>empirical</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>continuous distribution:</term>
      <listitem>
  
  <para>A distribution described by a continuous function. <indexterm>
  <primary>continuous distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>continuous</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>interarrival time:</term>
      <listitem>
  
  <para>The elapsed time between two events. <indexterm>
  <primary>interarrival time</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>error function:</term>
      <listitem>
  
  <para>A special mathematical function, so-named because it comes up in the study of measurement errors. <indexterm>
  <primary>error function</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>normal probability plot:</term>
      <listitem>
  
  <para>A plot of the sorted values in a sample versus the expected value for each if their distribution is normal. <indexterm>
  <primary>normal probability plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>normal probability</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>rankit:</term>
      <listitem>
  
  <para>The expected value of an element in a sorted list of values from a normal distribution. <indexterm>
  <primary>rankit</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>model:</term>
      <listitem>
  
  <para>A useful simplification. Continuous distributions are often good models of more complex empirical distributions. <indexterm>
  <primary>model</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>corpus:</term>
      <listitem>
  
  <para>A body of text used as a sample of a language. <indexterm>
  <primary>corpus</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>hapaxlegomenon:</term>
      <listitem>
  
  <para>A word that appears only once in a corpus. It appears twice in this book, so far. <indexterm>
  <primary>hapaxlegomenon</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="probability">
  <title>Probability</title>
  
  
  <para> <anchor id="a0000000110" /> <indexterm>
  <primary>probability</primary>

</indexterm> <indexterm>
  <primary>frequency</primary>

</indexterm> <indexterm>
  <primary>sample size</primary>

</indexterm> </para>

  
  <para>In <xref linkend="descriptive" />, I said that a probability is a frequency expressed as a fraction of the sample size. That’s one definition of probability, but it’s not the only one. In fact, the meaning of probability is a topic of some controversy. </para>

  
  <para>We’ll start with the uncontroversial parts and work our way up. There is general agreement that a probability is a real value between 0 and 1 that is intended to be a quantitative measure corresponding to the qualitative notion that some things are more likely than others. <indexterm>
  <primary>event</primary>

</indexterm> <indexterm>
  <primary>trial</primary>

</indexterm> </para>

  
  <para>The “things” we assign probabilities to are called <emphasis role="bold">events</emphasis>. If <emphasis>E</emphasis>represents an event, then <emphasis>P</emphasis>(<emphasis>E</emphasis>) represents the probability that <emphasis>E</emphasis>will occur. A situation where <emphasis>E</emphasis>might or might not happen is called a <emphasis role="bold">trial</emphasis>. <indexterm>
  <primary>success</primary>

</indexterm> <indexterm>
  <primary>failure</primary>

</indexterm> <indexterm>
  <primary>dice</primary>

</indexterm> </para>

  
  <para>As an example, suppose you have a standard six-sided die<footnote><para>“Die” is the singular of “dice”.</para></footnote> and want to know the probability of rolling a 6. Each roll is a trial. Each time a 6 appears is considered a <emphasis role="bold">success</emphasis>; other trials are considered <emphasis role="bold">failures</emphasis>. These terms are used even in scenarios where “success” is bad and “failure” is good. </para>

  
  <para>If we have a finite sample of <emphasis>n</emphasis> trials and we observe <emphasis>s</emphasis> successes, the probability of success is <emphasis>s</emphasis>/<emphasis>n</emphasis>. If the set of trials is infinite, defining probabilities is a little trickier, but most people are willing to accept probabilistic claims about a hypothetical series of identical trials, like tossing a coin or rolling a die. <indexterm>
  <primary>identical trials</primary>

</indexterm> <indexterm>
  <primary>unique events</primary>

</indexterm> </para>

  
  <para>We start to run into trouble when we talk about probabilities of unique events. For example, we might like to know the probability that a candidate will win an election. But every election is unique, so there is no series of identical trials to consider. <indexterm>
  <primary>election</primary>

</indexterm> </para>

  
  <para>In cases like this some people would say that the notion of probability does not apply. This position is sometimes called <emphasis role="bold">frequentism</emphasis> because it defines probability in terms of frequencies. If there is no set of identical trials, there is no probability. <indexterm>
  <primary>frequentism</primary>

</indexterm> <indexterm>
  <primary>Bayesianism</primary>

</indexterm> </para>

  
  <para>Frequentism is philosophically safe, but frustrating because it limits the scope of probability to physical systems that are either random (like atomic decay) or so unpredictable that we model them as random (like a tumbling die). Anything involving people is pretty much off the table. </para>

  
  <para>An alternative is <emphasis role="bold">Bayesianism</emphasis>, which defines probability as a degree of belief that an event will occur. By this definition, the notion of probability can be applied in almost any circumstance. One difficulty with Bayesian probability is that it depends on a person’s state of knowledge; people with different information might have different degrees of belief about the same event. For this reason, many people think that Bayesian probabilities are more subjective than frequency probabilities. <indexterm>
  <primary>subjective belief</primary>

</indexterm> <indexterm>
  <primary>belief</primary>

</indexterm> <indexterm>
  <primary>Thaksin Shinawatra</primary>

</indexterm> <indexterm>
  <primary>Thailand</primary>

</indexterm> <indexterm>
  <primary>Prime Minister</primary>

</indexterm> </para>

  
  <para>As an example, what is the probability that Thaksin Shinawatra is the Prime Minister of Thailand? A frequentist would say that there is no probability for this event because there is no set of trials. Thaksin either is, or is not, the PM; it’s not a question of probability. </para>

  
  <para>In contrast, a Bayesian would be willing to assign a probability to this event based on his or her state of knowledge. For example, if you remember that there was a coup in Thailand in 2006, and you are pretty sure Thaksin was the PM who was ousted, you might assign a probability like 0.1, which acknowledges the possibility that your recollection is incorrect, or that Thaksin has been reinstated. </para>

  
  <para>If you consult Wikipedia, you will learn that Thaksin is not the PM of Thailand (at the time I am writing). Based on this information, you might revise your probability estimate to 0.01, reflecting the possibility that Wikipedia is wrong. </para>
<sect1 id="a0000000111" remap="section">
  <title>Rules of probability</title>
    
  
  <para> <indexterm>
  <primary>probability</primary>
<secondary>rules of</secondary>
</indexterm> </para>

  
  

  
  <para>For frequency probabilities, we can derive rules that relate probabilities of different events. Probably the best known of these rules is </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>A</emphasis>) <emphasis>P</emphasis>(<emphasis>B</emphasis>)  Warning: not always true! </member>
</simplelist> </para>

  
  <para>where <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) is the probability that events <emphasis>A</emphasis> and <emphasis>B</emphasis> both occur. This formula is easy to remember; the only problem is that it is <emphasis>not always true</emphasis>. This formula only applies if <emphasis>A</emphasis> and <emphasis>B</emphasis> are <emphasis role="bold">independent</emphasis>, which means that if I know <emphasis>A</emphasis> occurred, that doesn’t change the probability of <emphasis>B</emphasis>, and vice versa. <indexterm>
  <primary>independent</primary>

</indexterm> <indexterm>
  <primary>event</primary>
<secondary>independent</secondary>
</indexterm> </para>

  
  <para>For example, if <emphasis>A</emphasis> is tossing a coin and getting heads, and <emphasis>B</emphasis> is rolling a die and getting 1, <emphasis>A</emphasis> and <emphasis>B</emphasis> are independent, because the coin toss doesn’t tell me anything about the die roll. <indexterm>
  <primary>dice</primary>

</indexterm> </para>

  
  <para>But if I roll two dice, and <emphasis>A</emphasis> is getting at least one six, and <emphasis>B</emphasis> is getting two sixes, <emphasis>A</emphasis> and <emphasis>B</emphasis> are not independent, because if I know that <emphasis>A</emphasis> occurred, the probability of <emphasis>B</emphasis> is higher, and if I know <emphasis>B</emphasis> occurred, the probability of <emphasis>A</emphasis> is 1. <indexterm>
  <primary>conditional probability</primary>

</indexterm> <indexterm>
  <primary>probability</primary>
<secondary>conditional</secondary>
</indexterm> </para>

  
  <para>When <emphasis>A</emphasis> and <emphasis>B</emphasis> are not independent, it is often useful to compute the conditional probability, <emphasis>P</emphasis>(<emphasis>A</emphasis>|<emphasis>B</emphasis>), which is the probability of <emphasis>A</emphasis> given that we know <emphasis>B</emphasis> occurred: </para>

  
  <para><informalequation id="a0000000112" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0014.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(A|B) = \frac{P(A ~ \mbox{and}~ B)}{P(B)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> From that we can derive the general relation </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>A</emphasis>) <emphasis>P</emphasis>(<emphasis>B</emphasis>|<emphasis>A</emphasis>) </member>
</simplelist> </para>

  
  <para>This might not be as easy to remember, but if you translate it into English it should make sense: “The chance of both things happening is the chance that the first one happens, and then the second one given the first.” </para>

  
  <para>There is nothing special about the order of events, so we could also write </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>B</emphasis>) <emphasis>P</emphasis>(<emphasis>A</emphasis>|<emphasis>B</emphasis>) </member>
</simplelist> </para>

  
  <para>These relationships hold whether <emphasis>A</emphasis> and <emphasis>B</emphasis> are independent or not. If they are independent, then <emphasis>P</emphasis>(<emphasis>A</emphasis>|<emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>A</emphasis>), which gets us back where we started. </para>

  
  <para>Because all probabilities are in the range 0 to 1, it is easy to show that </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) ≤ <emphasis>P</emphasis>(<emphasis>A</emphasis>) </member>
</simplelist> </para>

  
  <para>To picture this, imagine a club that only admits people who satisfy some requirement, <emphasis>A</emphasis>. Now suppose they add a new requirement for membership, <emphasis>B</emphasis>. It seems obvious that the club will get smaller, or stay the same if it happens that all the members satisfy <emphasis>B</emphasis>. But there are some scenarios where people are surprisingly bad at this kind of analysis. For examples and discussion of this phenomenon, see <ulink url="http://wikipedia.org/wiki/Conjunction_fallacy">http://wikipedia.org/wiki/Conjunction_fallacy</ulink>. <indexterm>
  <primary>conjunction fallacy</primary>

</indexterm> <indexterm>
  <primary>fallacy</primary>
<secondary>conjunction</secondary>
</indexterm> </para>

  
  <example id="a0000000113">
  <title></title>
  
  
  <para> If I roll two dice and the total is 8, what is the chance that one of the dice is a 6? <indexterm>
  <primary>dice</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000114">
  <title></title>
  
  
  <para> If I roll 100 dice, what is the chance of getting all sixes? What is the chance of getting no sixes? </para>

</example>

  
  <example id="a0000000115">
  <title></title>
  
  
  <para> The following questions are adapted from Mlodinow, <emphasis>The Drunkard’s Walk</emphasis>. <indexterm>
  <primary>Mlodinow, Leonard</primary>

</indexterm> <indexterm>
  <primary>Florida, girl named</primary>

</indexterm> <indexterm>
  <primary>girl named Florida</primary>

</indexterm> </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>If a family has two children, what is the chance that they have two girls? </para>
</listitem>
  
  <listitem>
  
  <para>If a family has two children and we know that at least one of them is a girl, what is the chance that they have two girls? </para>
</listitem>
  
  <listitem>
  
  <para>If a family has two children and we know that the older one is a girl, what is the chance that they have two girls? </para>
</listitem>
  
  <listitem>
  
  <para>If a family has two children and we know that at least one of them is a girl named Florida, what is the chance that they have two girls? </para>
</listitem>
  
</orderedlist></para>

  
  <para>You can assume that the probability that any child is a girl is 1/2, and that the children in a family are independent trials (in more ways than one). You can also assume that the percentage of girls named Florida is small. </para>

</example>

</sect1><sect1 id="a0000000116" remap="section">
  <title>Monty Hall</title>
    
  
  <para> <indexterm>
  <primary>Monty Hall problem</primary>

</indexterm> </para>

  
  <para>The Monty Hall problem might be the most contentious question in the history of probability. The scenario is simple, but the correct answer is so counter-intuitive that many people just can’t accept it, and many smart people have embarrassed themselves not just by getting it wrong but by arguing the wrong side, aggressively, in public. </para>

  
  <para>Monty Hall was the original host of the game show <emphasis>Let’s Make a Deal</emphasis>. The Monty Hall problem is based on one of the regular games on the show. If you are on the show, here’s what happens: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>Monty shows you three closed doors and tells you that there is a prize behind each door: one prize is a car, the other two are less valuable prizes like peanut butter and fake finger nails. The prizes are arranged at random. </para>
</listitem>
  
    <listitem>
  
  <para>The object of the game is to guess which door has the car. If you guess right, you get to keep the car. </para>
</listitem>
  
    <listitem>
  
  <para>So you pick a door, which we will call Door A. We’ll call the other doors B and C. </para>
</listitem>
  
    <listitem>
  
  <para>Before opening the door you chose, Monty likes to increase the suspense by opening either Door B or C, whichever does not have the car. (If the car is actually behind Door A, Monty can safely open B or C, so he chooses one at random). </para>
</listitem>
  
    <listitem>
  
  <para>Then Monty offers you the option to stick with your original choice or switch to the one remaining unopened door. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>The question is, should you “stick” or “switch” or does it make no difference? <indexterm>
  <primary>stick</primary>

</indexterm> <indexterm>
  <primary>switch</primary>

</indexterm> <indexterm>
  <primary>intuition</primary>

</indexterm> </para>

  
  <para>Most people have the strong intuition that it makes no difference. There are two doors left, they reason, so the chance that the car is behind Door A is 50%. </para>

  
  <para>But that is wrong. In fact, the chance of winning if you stick with Door A is only 1/3; if you switch, your chances are 2/3. I will explain why, but I don’t expect you to believe me. </para>

  
  <para>The key is to realize that there are three possible scenarios: the car is behind Door A, B or C. Since the prizes are arranged at random, the probability of each scenario is 1/3. </para>

  
  <para>If your strategy is to stick with Door A, then you will win only in Scenario A, which has probability 1/3. </para>

  
  <para>If your strategy is to switch, you will win in either Scenario B or Scenario C, so the total probability of winning is 2/3. </para>

  
  <para>If you are not completely convinced by this argument, you are in good company. When a friend presented this solution to Paul Erdȍs, he replied, “No, that is impossible. It should make no difference.<footnote><para>See Hoffman, <emphasis>The Man Who Loved Only Numbers</emphasis>, page 83.</para></footnote>” </para>

  
  <para>No amount of argument could convince him. In the end, it took a computer simulation to bring him around. </para>

  
  <example id="a0000000117">
  <title></title>
  
  
  <para> Write a program that simulates the Monty Hall problem and use it to estimate the probability of winning if you stick and if you switch. </para>

  
  <para>Then read the discussion of the problem at <ulink url="http://wikipedia.org/wiki/Monty_Hall_problem">http://wikipedia.org/wiki/Monty_Hall_problem</ulink>. </para>

  
  <para>Which do you find more convincing, the simulation or the arguments, and why? </para>

</example>

  
  <example id="a0000000118">
  <title></title>
  
  
  <para> To understand the Monty Hall problem, it is important to realize that by deciding which door to open, Monty is giving you information. To see why this matters, imagine the case where Monty doesn’t know where the prizes are, so he chooses Door B or C at random. <indexterm>
  <primary>Monty Hall</primary>
<secondary> confused</secondary>
</indexterm> <indexterm>
  <primary>confused Monty problem</primary>

</indexterm> </para>

  
  <para>If he opens the door with the car, the game is over, you lose, and you don’t get to choose whether to switch or stick. </para>

  
  <para>Otherwise, are you better off switching or sticking? </para>

</example>

  
  

</sect1><sect1 id="a0000000119" remap="section">
  <title>Poincaré</title>
    
  
  <para>Henri Poincaré was a French mathematician who taught at the Sorbonne around 1900. The following anecdote about him is probably fabricated, but it makes an interesting probability problem. <indexterm>
  <primary>bread police</primary>

</indexterm> </para>

  
  <para>Supposedly Poincaré suspected that his local bakery was selling loaves of bread that were lighter than the advertised weight of 1 kg, so every day for a year he bought a loaf of bread, brought it home and weighed it. At the end of the year, he plotted the distribution of his measurements and showed that it fit a normal distribution with mean 950 g and standard deviation 50 g. He brought this evidence to the bread police, who gave the baker a warning. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>For the next year, Poincaré continued the practice of weighing his bread every day. At the end of the year, he found that the average weight was 1000 g, just as it should be, but again he complained to the bread police, and this time they fined the baker. <indexterm>
  <primary>symmetric</primary>

</indexterm> </para>

  
  <para>Why? Because the shape of the distribution was asymmetric. Unlike the normal distribution, it was skewed to the right, which is consistent with the hypothesis that the baker was still making 950 g loaves, but deliberately giving Poincaré the heavier ones. </para>

  
  <example id="a0000000120">
  <title></title>
  
  
  <para> Write a program that simulates a baker who chooses <emphasis>n</emphasis> loaves from a distribution with mean 950 g and standard deviation 50 g, and gives the heaviest one to Poincaré. What value of <emphasis>n</emphasis> yields a distribution with mean 1000 g? What is the standard deviation? </para>

  
  <para>Compare this distribution to a normal distribution with the same mean and the same standard deviation. Is the difference in the shape of the distribution big enough to convince the bread police? </para>

</example>

  
  <example id="coef_var">
  <title></title>
  
  
  <para> <anchor id="a0000000121" /> If you go to a dance where partners are paired up randomly, what percentage of opposite sex couples will you see where the woman is taller than the man? <indexterm>
  <primary>dance</primary>

</indexterm> <indexterm>
  <primary>height</primary>

</indexterm> <indexterm>
  <primary>Behavioral Risk Factor Surveillance System</primary>

</indexterm> <indexterm>
  <primary>BRFSS</primary>

</indexterm> </para>

  
  <para>In the BRFSS (see <xref linkend="lognormal" />), the distribution of heights is roughly normal with parameters <emphasis>μ</emphasis> = 178 cm and <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis> = 59.4 cm for men, and <emphasis>μ</emphasis> = 163 cm and <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis> = 52.8 cm for women. <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> </para>

  
  <para>As an aside, you might notice that the standard deviation for men is higher and wonder whether men’s heights are more variable. To compare variability between groups, it is useful to compute the <emphasis role="bold">coefficient of variation</emphasis>, which is the standard deviation as a fraction of the mean, <emphasis>σ</emphasis>/<emphasis>μ</emphasis>. By this measure, women’s heights are slightly more variable. <indexterm>
  <primary>coefficient</primary>
<secondary>variation</secondary>
</indexterm> </para>

</example>

</sect1><sect1 id="a0000000122" remap="section">
  <title>Another rule of probability</title>
    
  
  <para> <indexterm>
  <primary>probability</primary>
<secondary>rules of</secondary>
</indexterm> <indexterm>
  <primary>mutually exclusive</primary>

</indexterm> </para>

  
  

  
  <para>If two events are <emphasis role="bold">mutually exclusive</emphasis>, that means that only one of them can happen, so the conditional probabilities are 0: </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis>|<emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>B</emphasis>|<emphasis>A</emphasis>) = 0 </member>
</simplelist> </para>

  
  <para>In this case it is easy to compute the probability of either event: </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">or</phrase> <emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>A</emphasis>) + <emphasis>P</emphasis>(<emphasis>B</emphasis>)  Warning: not always true.</member>
</simplelist> </para>

  
  <para>But remember that this only applies if the events are mutually exclusive. In general the probability of <emphasis>A</emphasis> or <emphasis>B</emphasis> or both is: </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">or</phrase> <emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>A</emphasis>) + <emphasis>P</emphasis>(<emphasis>B</emphasis>) − <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) </member>
</simplelist> </para>

  
  <para>The reason we have to subtract off <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) is that otherwise it gets counted twice. For example, if I flip two coins, the chance of getting at least one tails is 1/2 + 1/2 − 1/4. I have to subtract 1/4 because otherwise I am counting heads-heads twice. The problem becomes even clearer if I toss three coins. <indexterm>
  <primary>coin</primary>

</indexterm> </para>

  
  <example id="a0000000123">
  <title></title>
  
  
  <para> If I roll two dice, what is the chance of rolling at least one 6? <indexterm>
  <primary>dice</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000124">
  <title></title>
  
  
  <para> What is the general formula for the probability of <emphasis>A</emphasis> or <emphasis>B</emphasis> but not both? </para>

</example>

</sect1><sect1 id="a0000000125" remap="section">
  <title>Binomial distribution</title>
    
  
  <para> <indexterm>
  <primary>binomial distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>binomial</secondary>
</indexterm> </para>

  
  <para>If I roll 100 dice, the chance of getting all sixes is (1/6)<emphasis><superscript>100</superscript></emphasis>. And the chance of getting no sixes is (5/6)<emphasis><superscript>100</superscript></emphasis>. </para>

  
  <para>Those cases are easy, but more generally, we might like to know the chance of getting <emphasis>k</emphasis> sixes, for all values of <emphasis>k</emphasis> from 0 to 100. The answer is the <emphasis role="bold">binomial distribution</emphasis>, which has this PMF: </para>

  
  <para><informalequation id="a0000000126" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0015.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PMF}(k) = \binom {n}{k} p^ k (1-p)^{n-k} \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> where <emphasis>n</emphasis> is the number of trials, <emphasis>p</emphasis> is the probability of success, and <emphasis>k</emphasis> is the number of successes. <indexterm>
  <primary>binomial coefficient</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>binomial</secondary>
</indexterm> </para>

  
  <para>The <emphasis role="bold">binomial coefficient</emphasis> is pronounced “n choose k”, and it can be computed directly like this: </para>

  
  <para><informalequation id="a0000000127" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0016.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \binom {n}{k} = \frac{n!}{k!(n-k!)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Or recursively like this </para>

  
  <para><informalequation id="a0000000128" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0017.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \binom {n}{k} = \binom {n-1}{k} + \binom {n-1}{k-1}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> with two base cases: if <emphasis>n</emphasis> = 0 the result is 0; if <emphasis>k</emphasis> = 0 the result is 1. If you download <ulink url="http://thinkstats.com/thinkstats.py">http://thinkstats.com/thinkstats.py</ulink> you will see a function named <literal>Binom</literal> that computes the binomial coefficient with reasonable efficiency. <indexterm>
  <primary>thinkstats.py</primary>

</indexterm> </para>

  
  <example id="a0000000129">
  <title></title>
  
  
  <para> If you flip a coin 100 times, you expect about 50 heads, but what is the probability of getting exactly 50 heads? <indexterm>
  <primary>coin</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000130" remap="section">
  <title>Streaks and hot spots</title>
    
  
  <para> <indexterm>
  <primary>streak</primary>

</indexterm> <indexterm>
  <primary>hot spot</primary>

</indexterm> <indexterm>
  <primary>random</primary>

</indexterm> </para>

  
  <para>People do not have very good intuition for random processes. If you ask people to generate “random” numbers, they tend to generate sequences that are random-looking, but actually more ordered than real random sequences. Conversely, if you show them a real random sequence, they tend to see patterns where there are none. </para>

  
  <para>An example of the second phenomenon is that many people believe in “streaks” in sports: a player that has been successful recently is said to have a “hot hand;” a player that has been unsuccessful is “in a slump.” <indexterm>
  <primary>hot hand</primary>

</indexterm> <indexterm>
  <primary>slump</primary>

</indexterm> <indexterm>
  <primary>sport</primary>

</indexterm> </para>

  
  <para>Statisticians have tested these hypotheses in a number of sports, and the consistent result is that there is no such thing as a streak<footnote><para>For example, see Gilovich, Vallone and Tversky, “The hot hand in basketball: On the misperception of random sequences,” 1985.</para></footnote>. If you assume that each attempt is independent of previous attempts, you will see occasional long strings of successes or failures. These apparent streaks are not sufficient evidence that there is any relationship between successive attempts. <indexterm>
  <primary>clustering illusion</primary>

</indexterm> <indexterm>
  <primary>spatial pattern</primary>

</indexterm> </para>

  
  <para>A related phenomenon is the clustering illusion, which is the tendency to see clusters in spatial patterns that are actually random (see <ulink url="http://wikipedia.org/wiki/Clustering_illusion">http://wikipedia.org/wiki/Clustering_illusion</ulink>). <indexterm>
  <primary>simulation</primary>
<secondary>Monte Carlo</secondary>
</indexterm> <indexterm>
  <primary>Monte Carlo</primary>

</indexterm> </para>

  
  <para>To test whether an apparent cluster is likely to be meaningful, we can simulate the behavior of a random system to see whether it is likely to produce a similar cluster. This process is called <emphasis role="bold">Monte Carlo</emphasis> simulation because generating random numbers is reminiscent of casino games (and Monte Carlo is famous for its casino). </para>

  
  <example id="a0000000131">
  <title></title>
  
  
  <para> If there are 10 players in a basketball game and each one takes 15 shots during the course of the game, and each shot has a 50% probability of going in, what is the probability that you will see, in a given game, at least one player who hits 10 shots in a row? If you watch a season of 82 games, what are the chances you will see at least one streak of 10 hits or misses? <indexterm>
  <primary>basketball</primary>

</indexterm> <indexterm>
  <primary>simulation</primary>
<secondary>Monte Carlo</secondary>
</indexterm> <indexterm>
  <primary>Monte Carlo</primary>

</indexterm> </para>

  
  <para>This problem demonstrates some strengths and weaknesses of Monte Carlo simulation. A strength is that it is often easy and fast to write a simulation, and no great knowledge of probability is required. A weakness is that estimating the probability of rare events can take a long time! A little bit of analysis can save a lot of computing. </para>

</example>

  
  <example id="a0000000132">
  <title></title>
  
  
  <para> In 1941 Joe DiMaggio got at least one hit in 56 consecutive games<footnote><para>See <ulink url="http://wikipedia.org/wiki/Hitting_streak">http://wikipedia.org/wiki/Hitting_streak</ulink>.</para></footnote>. Many baseball fans consider this streak the greatest achievement in any sport in history, because it was so unlikely. <indexterm>
  <primary>baseball</primary>

</indexterm> <indexterm>
  <primary>DiMaggio, Joe</primary>

</indexterm> <indexterm>
  <primary>hitting streak</primary>

</indexterm> <indexterm>
  <primary>simulation</primary>
<secondary>Monte Carlo</secondary>
</indexterm> <indexterm>
  <primary>Monte Carlo</primary>

</indexterm> </para>

  
  <para>Use a Monte Carlo simulation to estimate the probability that any player in major league baseball will have a hitting streak of 57 or more games in the next century. </para>

</example>

  
  <example id="a0000000133">
  <title></title>
  
  
  <para> A cancer cluster is defined by the Centers for Disease Control (CDC) as “greater-than-expected number of cancer cases that occurs within a group of people in a geographic area over a period of time.<footnote><para>From <ulink url="http://cdc.gov/nceh/clusters/about.htm">http://cdc.gov/nceh/clusters/about.htm</ulink>.</para></footnote>” <indexterm>
  <primary>cluster</primary>

</indexterm> <indexterm>
  <primary>cancer cluster</primary>

</indexterm> <indexterm>
  <primary>Gawande, Atul</primary>

</indexterm> </para>

  
  <para>Many people interpret a cancer cluster as evidence of an environmental hazard, but many scientists and statisticians think that investigating cancer clusters is a waste of time<footnote><para>See Gawande, “The Cancer Cluster Myth,” <emphasis>New Yorker</emphasis>, Feb 8, 1997.</para></footnote>. Why? One reason (among several) is that identifying cancer clusters is a classic case of the Sharpshooter Fallacy (see <ulink url="http://wikipedia.org/wiki/Texas_sharpshooter_fallacy">http://wikipedia.org/wiki/Texas_sharpshooter_fallacy</ulink>). <indexterm>
  <primary>sharpshooter fallacy</primary>

</indexterm> <indexterm>
  <primary>fallacy</primary>
<secondary>sharpshooter</secondary>
</indexterm> </para>

  
  <para>Nevertheless, when someone reports a cancer cluster, the CDC is obligated to investigate. According to their web page: </para>

  
  <blockquote remap="quote">
  <para>“Investigators develop a ‘case’ definition, a time period of concern, and the population at risk. They then calculate the expected number of cases and compare them to the observed number. A cluster is confirmed when the observed/expected ratio is greater than 1.0, and the difference is statistically significant.” </para>
</blockquote>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Suppose that a particular cancer has an incidence of 1 case per thousand people per year. If you follow a particular cohort of 100 people for 10 years, you would expect to see about 1 case. If you saw two cases, that would not be very surprising, but more than than two would be rare.<indexterm>
  <primary>cohort</primary>

</indexterm> </para>

  
  <para>Write a program that simulates a large number of cohorts over a 10 year period and estimates the distribution of total cases. </para>
</listitem>
  
  <listitem>
  
  <para>An observation is considered statistically significant if its probability by chance alone, called a p-value, is less than 5%. In a cohort of 100 people over 10 years, how many cases would you have to see to meet this criterion? </para>
</listitem>
  
  <listitem>
  
  <para>Now imagine that you divide a population of 10000 people into 100 cohorts and follow them for 10 years. What is the chance that at least one of the cohorts will have a “statistically significant” cluster? What if we require a p-value of 1%.? </para>
</listitem>
  
  <listitem>
  
  <para>Now imagine that you arrange 10000 people in a 100 ×100 grid and follow them for 10 years. What is the chance that there will be at least one 10 ×10 block anywhere in the grid with a statistically significant cluster? </para>
</listitem>
  
  <listitem>
  
  <para>Finally, imagine that you follow a grid of 10000 people for 30 years. What is the chance that there will be a 10-year interval at some point with a 10 ×10 block anywhere in the grid with a statistically significant cluster? </para>
</listitem>
  
</orderedlist></para>

</example>

</sect1><sect1 id="a0000000134" remap="section">
  <title>Bayes’s theorem</title>
    
  
  <para> <indexterm>
  <primary>Bayes’s theorem</primary>

</indexterm> <indexterm>
  <primary>conditional probability</primary>

</indexterm> </para>

  
  <para>Bayes’s theorem is a relationship between the conditional probabilities of two events. A conditional probability, often written <emphasis>P</emphasis>(<emphasis>A</emphasis>|<emphasis>B</emphasis>) is the probability that Event <emphasis>A</emphasis>will occur given that we know that Event <emphasis>B</emphasis>has occurred. Bayes’s theorem states: </para>

  
  <para><informalequation id="a0000000135" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0018.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(A|B) = \frac{P(B|A)P(A)}{P(B)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> To see that this is true, it helps to write <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>), which is the probability that A and B occur </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>A</emphasis>) <emphasis>P</emphasis>(<emphasis>B</emphasis>|<emphasis>A</emphasis>) </member>
</simplelist> </para>

  
  <para>But it is also true that </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>A</emphasis> <phrase remap="mbox">and</phrase> <emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>B</emphasis>) <emphasis>P</emphasis>(<emphasis>A</emphasis>|<emphasis>B</emphasis>) </member>
</simplelist> </para>

  
  <para>So </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>B</emphasis>) <emphasis>P</emphasis>(<emphasis>A</emphasis>|<emphasis>B</emphasis>) = <emphasis>P</emphasis>(<emphasis>A</emphasis>) <emphasis>P</emphasis>(<emphasis>B</emphasis>|<emphasis>A</emphasis>) </member>
</simplelist> </para>

  
  <para>Dividing through by <emphasis>P</emphasis>(<emphasis>B</emphasis>) yields Bayes’s theorem<footnote><para>See <ulink url="http://wikipedia.org/wiki/Q.E.D.">http://wikipedia.org/wiki/Q.E.D.</ulink>!</para></footnote>. <indexterm>
  <primary>Q.E.D.</primary>

</indexterm> <indexterm>
  <primary>evidence</primary>

</indexterm> <indexterm>
  <primary>hypothesis</primary>

</indexterm> </para>

  
  <para>Bayes’s theorem is often interpreted as a statement about how a body of evidence, <emphasis>E</emphasis>, affects the probability of a hypothesis, <emphasis>H</emphasis>: </para>

  
  <para><informalequation id="a0000000136" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0019.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(H|E) = P(H) \frac{P(E|H)}{P(E)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> In words, this equation says that the probability of <emphasis>H</emphasis> after you have seen <emphasis>E</emphasis>is the product of <emphasis>P</emphasis>(<emphasis>H</emphasis>), which is the probability of <emphasis>H</emphasis> before you saw the evidence, and the ratio of <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis>), the probability of seeing the evidence assuming that <emphasis>H</emphasis> is true, and <emphasis>P</emphasis>(<emphasis>E</emphasis>), the probability of seeing the evidence under any circumstances (<emphasis>H</emphasis> true or not). <indexterm>
  <primary>diachronic</primary>

</indexterm> </para>

  
  <para>This way of reading Bayes’s theorem is called the “diachronic” interpretation because it describes how the probability of a hypothesis gets <emphasis role="bold">updated</emphasis> over time, usually in light of new evidence. In this context, <emphasis>P</emphasis>(<emphasis>H</emphasis>) is called the <emphasis role="bold">prior</emphasis> probability and <emphasis>P</emphasis>(<emphasis>H</emphasis>|<emphasis>E</emphasis>) is called the <emphasis role="bold">posterior</emphasis>. <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis>) is the <emphasis role="bold">likelihood</emphasis> of the evidence, and <emphasis>P</emphasis>(<emphasis>E</emphasis>) is the <emphasis role="bold">normalizing constant</emphasis>. <indexterm>
  <primary>update</primary>

</indexterm> <indexterm>
  <primary>prior</primary>

</indexterm> <indexterm>
  <primary>posterior</primary>

</indexterm> <indexterm>
  <primary>likelihood</primary>

</indexterm> <indexterm>
  <primary>normalizing constant</primary>

</indexterm> </para>

  
  <para>A classic use of Bayes’s theorem is the interpretation of clinical tests. For example, routine testing for illegal drug use is increasingly common in workplaces and schools (See <ulink url="http://aclu.org/drugpolicy/testing">http://aclu.org/drugpolicy/testing</ulink>.). The companies that perform these tests maintain that the tests are sensitive, which means that they are likely to produce a positive result if there are drugs (or metabolites) in a sample, and specific, which means that they are likely to yield a negative result if there are no drugs. <indexterm>
  <primary>drug testing</primary>

</indexterm> <indexterm>
  <primary>Journal of the American Medical Association</primary>

</indexterm> <indexterm>
  <primary>JAMA</primary>

</indexterm> </para>

  
  <para>Studies from the Journal of the American Medical Association<footnote><para>I got these numbers from Gleason and Barnum, “Predictive Probabilities In Employee Drug-Testing,” at <ulink url="http://piercelaw.edu/risk/vol2/winter/gleason.htm">http://piercelaw.edu/risk/vol2/winter/gleason.htm</ulink>.</para></footnote> estimate that the sensitivity of common drug tests is about 60% and the specificity is about 99%. </para>

  
  <para>Now suppose these tests are applied to a workforce where the actual rate of drug use is 5%. Of the employees who test positive, how many of them actually use drugs? </para>

  
  <para>In Bayesian terms, we want to compute the probability of drug use given a positive test, <emphasis>P</emphasis>(<emphasis>D</emphasis>|<emphasis>E</emphasis>). By Bayes’s theorem: </para>

  
  <para><informalequation id="a0000000137" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0020.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(D|E) = P(D) \frac{P(E|D)}{P(E)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The prior, <emphasis>P</emphasis>(<emphasis>D</emphasis>) is the probability of drug use before we see the outcome of the test, which is 5%. The likelihood, <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>D</emphasis>), is the probability of a positive test assuming drug use, which is the sensitivity. </para>

  
  <para>The normalizing constant, <emphasis>P</emphasis>(<emphasis>E</emphasis>) is a little harder to evaluate. We have to consider two possibilities, <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>D</emphasis>) and <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>N</emphasis>), where <emphasis>N</emphasis> is the hypothesis that the subject of the test does not use drugs: </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>E</emphasis>) = <emphasis>P</emphasis>(<emphasis>D</emphasis>) <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>D</emphasis>) + <emphasis>P</emphasis>(<emphasis>N</emphasis>) <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>N</emphasis>) </member>
</simplelist> </para>

  
  <para>The probability of a false positive, <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>N</emphasis>), is the complement of the specificity, or 1%. <indexterm>
  <primary>false positive</primary>

</indexterm> </para>

  
  <para>Putting it together, we have </para>

  
  <para><informalequation id="a0000000138" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0021.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(D|E) = \frac{P(D) P(E|D)}{P(D) P(E|D) + P(N) P(E|N)} \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Plugging in the given values yields <emphasis>P</emphasis>(<emphasis>D</emphasis>|<emphasis>E</emphasis>) = 0.76, which means that of the people who test positive, about 1 in 4 is innocent. </para>

  
  <example id="a0000000139">
  <title></title>
  
  
  <para> Write a program that takes the actual rate of drug use, and the sensitivity and specificity of the test, and uses Bayes’s theorem to compute <emphasis>P</emphasis>(<emphasis>D</emphasis>|<emphasis>E</emphasis>). </para>

  
  <para>Suppose the same test is applied to a population where the actual rate of drug use is 1%. What is the probability that someone who tests positive is actually a drug user? </para>

</example>

  
  <example id="a0000000140">
  <title></title>
  
  
  <para> This exercise is from <ulink url="http://wikipedia.org/wiki/Bayesian_inference">http://wikipedia.org/wiki/Bayesian_inference</ulink>. </para>

  
  <blockquote remap="quote">
  <para>“Suppose there are two full bowls of cookies. Bowl 1 has 10 chocolate chip and 30 plain cookies, while Bowl 2 has 20 of each. Our friend Fred picks a bowl at random, and then picks a cookie at random. The cookie turns out to be a plain one. How probable is it that Fred picked it out of Bowl 1?” </para>
</blockquote>

  
  <para> <indexterm>
  <primary>cookie</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000141">
  <title></title>
  
  
  <para>The blue M&amp;M was introduced in 1995. Before then, the color mix in a bag of plain M&amp;Ms was (30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan). Afterward it was (24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown). </para>

  
  <para>A friend of mine has two bags of M&amp;Ms, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&amp;M from each bag. One is yellow and one is green. What is the probability that the yellow M&amp;M came from the 1994 bag? </para>

</example>

  
  <example id="a0000000142">
  <title></title>
  
  
  <para> This exercise is adapted from MacKay, <emphasis>Information Theory, Inference, and Learning Algorithms</emphasis>: <indexterm>
  <primary>MacKay, David</primary>

</indexterm> <indexterm>
  <primary>Presley, Elvis</primary>

</indexterm> <indexterm>
  <primary>twin</primary>

</indexterm> </para>

  
  <para>Elvis Presley had a twin brother who died at birth. According to the Wikipedia article on twins: </para>

  
  <blockquote remap="quote">
  <para> “Twins are estimated to be approximately 1.9% of the world population, with monozygotic twins making up 0.2% of the total—and 8% of all twins.” </para>
</blockquote>

  
  <para>What is the probability that Elvis was an identical twin? </para>

</example>

</sect1><sect1 id="a0000000143" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>event:</term>
      <listitem>
  
  <para>Something that may or may not occur, with some probability. <indexterm>
  <primary>event</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>trial:</term>
      <listitem>
  
  <para>One in a series of occasions when an event might occur. <indexterm>
  <primary>trial</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>success:</term>
      <listitem>
  
  <para>A trial in which an event occurs. <indexterm>
  <primary>success</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>failure:</term>
      <listitem>
  
  <para>A trail in which no event occurs. <indexterm>
  <primary>failure</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>frequentism:</term>
      <listitem>
  
  <para>A strict interpretation of probability that only applies to a series of identical trials. <indexterm>
  <primary>frequentism</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Bayesianism:</term>
      <listitem>
  
  <para>A more general interpretation that uses probability to represent a subjective degree of belief. <indexterm>
  <primary>subjective belief</primary>

</indexterm> <indexterm>
  <primary>belief</primary>

</indexterm> <indexterm>
  <primary>Bayesianism</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>independent:</term>
      <listitem>
  
  <para>Two events are independent if the occurrence of one does has no effect on the probability of another. <indexterm>
  <primary>independent</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>coefficient of variation:</term>
      <listitem>
  
  <para>A statistic that measures spread, normalized by central tendency, for comparison between distributions with different means. <indexterm>
  <primary>coefficient</primary>
<secondary>variation</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Monte Carlo simulation:</term>
      <listitem>
  
  <para>A method of computing probabilities by simulating random processes (see <ulink url="http://wikipedia.org/wiki/Monte_Carlo_method">http://wikipedia.org/wiki/Monte_Carlo_method</ulink>). <indexterm>
  <primary>simulation</primary>
<secondary>Monte Carlo</secondary>
</indexterm> <indexterm>
  <primary>Monte Carlo</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>update:</term>
      <listitem>
  
  <para>The process of using data to revise a probability. <indexterm>
  <primary>update</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>prior:</term>
      <listitem>
  
  <para>A probability before a Bayesian update. <indexterm>
  <primary>prior</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>posterior:</term>
      <listitem>
  
  <para>A probability computed by a Bayesian update. <indexterm>
  <primary>posterior</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>likelihood of the evidence:</term>
      <listitem>
  
  <para>One of the terms in Bayes’s theorem, the probability of the evidence conditioned on a hypothesis. <indexterm>
  <primary>likelihood</primary>

</indexterm> <indexterm>
  <primary>evidence</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>normalizing constant:</term>
      <listitem>
  
  <para>The denominator of Bayes’s Theorem, used to normalize the result to be a probability. <indexterm>
  <primary>normalizing constant</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="operations">
  <title>Operations on distributions</title>
  
  
  <para> <anchor id="a0000000144" /> <indexterm>
  <primary>operations on distributions</primary>

</indexterm> <indexterm>
  <primary>distributions</primary>
<secondary>operations</secondary>
</indexterm> </para>
<sect1 id="a0000000145" remap="section">
  <title>Skewness</title>
    
  
  <para> <indexterm>
  <primary>skewness</primary>

</indexterm> </para>

  
  <para><emphasis role="bold">Skewness</emphasis> is a statistic that measures the asymmetry of a distribution. Given a sequence of values, <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>, the sample skewness is: </para>

  
  <para><informalequation id="a0000000146" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0022.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  g_1 = {m_3} / {m_2^{3/2}} \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para><informalequation id="a0000000147" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0023.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  m_2 = \frac{1}{n} \sum _ i (x_ i - \mu )^2  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para><informalequation id="a0000000148" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0024.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  m_3 = \frac{1}{n} \sum _ i (x_ i - \mu )^3  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> You might recognize <emphasis>m</emphasis><emphasis><subscript>2</subscript></emphasis> as the mean squared deviation (also known as variance); <emphasis>m</emphasis><emphasis><subscript>3</subscript></emphasis> is the mean cubed deviation. <indexterm>
  <primary>deviation</primary>

</indexterm> <indexterm>
  <primary>variance</primary>

</indexterm> </para>

  
  <para>Negative skewness indicates that a distribution “skews left;” that is, it extends farther to the left than the right. Positive skewness indicates that a distribution skews right. </para>

  
  <para>In practice, computing the skewness of a sample is usually not a good idea. If there are any outliers, they have a disproportionate effect on <emphasis>g</emphasis><emphasis><subscript>1</subscript></emphasis>. <indexterm>
  <primary>outlier</primary>

</indexterm> </para>

  
  <para>Another way to evaluate the asymmetry of a distribution is to look at the relationship between the mean and median. Extreme values have more effect on the mean than the median, so in a distribution that skews left, the mean is less than the median. <indexterm>
  <primary>symmetry</primary>

</indexterm> <indexterm>
  <primary>Pearson’s median skewness coefficient</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>skewness</secondary>
</indexterm> </para>

  
  <para><emphasis role="bold">Pearson’s median skewness coefficient</emphasis> is an alternative measure of skewness that explicitly captures the relationship between the mean, <emphasis>μ</emphasis>, and the median, <emphasis>μ</emphasis><emphasis><subscript>1/2</subscript></emphasis>: </para>

  
  <para><informalequation id="a0000000149" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0025.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  g_ p = 3 (\mu - \mu _{1/2}) / \sigma  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> This statistic is <emphasis role="bold">robust</emphasis>, which means that it is less vulnerable to the effect of outliers. <indexterm>
  <primary>robust</primary>

</indexterm> </para>

  
  <example id="a0000000150">
  <title></title>
  
  
  <para> Write a function named <literal>Skewness</literal> that computes <emphasis>g</emphasis><emphasis><subscript>1</subscript></emphasis> for a sample. </para>

  
  <para>Compute the skewness for the distributions of pregnancy length and birth weight. Are the results consistent with the shape of the distributions? <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>Write a function named <literal>PearsonSkewness</literal> that computes <emphasis>g</emphasis><emphasis><subscript>p</subscript></emphasis> for these distributions. How does <emphasis>g</emphasis><emphasis><subscript>p</subscript></emphasis> compare to <emphasis>g</emphasis><emphasis><subscript>1</subscript></emphasis>? </para>

</example>

  
  <example id="a0000000151">
  <title></title>
  
  
  <para> The “Lake Wobegon effect” is an amusing nickname<footnote><para>If you don’t get it, see <ulink url="http://wikipedia.org/wiki/Lake_Wobegon">http://wikipedia.org/wiki/Lake_Wobegon</ulink>.</para></footnote> for <emphasis role="bold">illusory superiority</emphasis>, which is the tendency for people to overestimate their abilities relative to others. For example, in some surveys, more than 80% of respondents believe that they are better than the average driver (see <ulink url="http://wikipedia.org/wiki/Illusory_superiority">http://wikipedia.org/wiki/Illusory_superiority</ulink>). <indexterm>
  <primary>average</primary>

</indexterm> <indexterm>
  <primary>Lake Wobegon effect</primary>

</indexterm> <indexterm>
  <primary>illusory superiority</primary>

</indexterm> <indexterm>
  <primary>fallacy</primary>
<secondary>illusory superiority</secondary>
</indexterm> </para>

  
  <para>If we interpret “average” to mean median, then this result is logically impossible, but if “average” is the mean, this result is possible, although unlikely. </para>

  
  <para>What percentage of the population has more than the average number of legs? </para>

</example>

  
  <example id="a0000000152">
  <title></title>
  
  
  <para> The Internal Revenue Service of the United States (IRS) provides data about income taxes, and other statistics, at <ulink url="http://irs.gov/taxstats">http://irs.gov/taxstats</ulink>. If you did <xref linkend="irs" />, you have already worked with this data; otherwise, follow the instructions there to extract the distribution of incomes from this dataset. <indexterm>
  <primary>Internal Revenue Service</primary>

</indexterm> <indexterm>
  <primary>IRS</primary>

</indexterm> <indexterm>
  <primary>income</primary>

</indexterm> <indexterm>
  <primary>taxes</primary>

</indexterm> </para>

  
  <para>What fraction of the population reports a taxable income below the mean? </para>

  
  <para>Compute the median, mean, skewness and Pearson’s skewness of the income data. Because the data has been binned, you will have to make some approximations. <indexterm>
  <primary>Gini coefficient</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>Gini</secondary>
</indexterm> </para>

  
  <para>The Gini coefficient is a measure of income inequality. Read about it at <ulink url="http://wikipedia.org/wiki/Gini_coefficient">http://wikipedia.org/wiki/Gini_coefficient</ulink> and write a function called <literal>Gini</literal> that computes it for the income distribution. <indexterm>
  <primary>relative mean difference</primary>

</indexterm> </para>

  
  <para>Hint: use the PMF to compute the relative mean difference (see <ulink url="http://wikipedia.org/wiki/Mean_difference">http://wikipedia.org/wiki/Mean_difference</ulink>). <indexterm>
  <primary>gini.py</primary>

</indexterm> </para>

  
  <para>You can download a solution to this exercise from <ulink url="http://thinkstats.com/gini.py">http://thinkstats.com/gini.py</ulink>. </para>

</example>

</sect1><sect1 id="a0000000153" remap="section">
  <title>Random Variables</title>
    
  
  <para> <indexterm>
  <primary>random variable</primary>

</indexterm> <indexterm>
  <primary>variable</primary>
<secondary>random</secondary>
</indexterm> </para>

  
  <para>A <emphasis role="bold">random variable</emphasis> represents a process that generates a random number. Random variables are usually written with a capital letter, like <emphasis>X</emphasis>. When you see a random variable, you should think “a value selected from a distribution.” <indexterm>
  <primary>cumulative distribution function</primary>

</indexterm> <indexterm>
  <primary>CDF</primary>

</indexterm> </para>

  
  <para>For example, the formal definition of the cumulative distribution function is: </para>

  
  <para><simplelist>
  <member> CDF<emphasis><subscript>X</subscript></emphasis>(<emphasis>x</emphasis>) = <emphasis>P</emphasis>(<emphasis>X</emphasis> ≤ <emphasis>x</emphasis>) </member>
</simplelist> </para>

  
  <para>I have avoided this notation until now because it is so awful, but here’s what it means: The CDF of the random variable <emphasis>X</emphasis>, evaluated for a particular value <emphasis>x</emphasis>, is defined as the probability that a value generated by the random process <emphasis>X</emphasis> is less than or equal to <emphasis>x</emphasis>. </para>

  
  <para>As a computer scientist, I find it helpful to think of a random variable as an object that provides a method, which I will call <literal>generate</literal>, that uses a random process to generate values. </para>

  
  <para>For example, here is a definition for a class that represents random variables: </para>

  
  <programlisting>class RandomVariable(object):
    """Parent class for all random variables."""</programlisting>

  
  <para>And here is a random variable with an exponential distribution: <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>

  
  <programlisting>class Exponential(RandomVariable):
    def __init__(self, lam):
        self.lam = lam

    def generate(self):
        return random.expovariate(self.lam)</programlisting>

  
  <para>The init method takes the parameter, <emphasis>λ</emphasis>, and stores it as an attribute. The <literal>generate</literal> method returns a random value from the exponential distribution with that parameter. <indexterm>
  <primary>init method</primary>

</indexterm> <indexterm>
  <primary>method</primary>
<secondary>init</secondary>
</indexterm> </para>

  
  <para>Each time you invoke <literal>generate</literal>, you get a different value. The value you get is called a <emphasis role="bold">random variate</emphasis>, which is why many function names in the <literal>random</literal> module include the word “variate.” <indexterm>
  <primary>random variate</primary>

</indexterm> <indexterm>
  <primary>variate</primary>
<secondary>random</secondary>
</indexterm> </para>

  
  <para>If I were just generating exponential variates, I would not bother to define a new class; I would use <literal>random.expovariate</literal>. But for other distributions it might be useful to use RandomVariable objects. For example, the Erlang distribution is a continuous distribution with parameters <emphasis>λ</emphasis> and <emphasis>k</emphasis> (see <ulink url="http://wikipedia.org/wiki/Erlang_distribution">http://wikipedia.org/wiki/Erlang_distribution</ulink>). <indexterm>
  <primary>Erlang distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Erlang</secondary>
</indexterm> </para>

  
  <para>One way to generate values from an Erlang distribution is to add <emphasis>k</emphasis> values from an exponential distribution with the same <emphasis>λ</emphasis>. Here’s an implementation: </para>

  
  <programlisting>class Erlang(RandomVariable):
    def __init__(self, lam, k):
        self.lam = lam
        self.k = k
        self.expo = Exponential(lam)

    def generate(self):
        total = 0
        for i in range(self.k):
            total += self.expo.generate()
        return total</programlisting>

  
  <para>The init method creates an Exponential object with the given parameter; then <literal>generate</literal> uses it. In general, the init method can take any set of parameters and the <literal>generate</literal> function can implement any random process. </para>

  
  <example id="a0000000154">
  <title></title>
  
  
  <para> Write a definition for a class that represents a random variable with a Gumbel distribution (see <ulink url="http://wikipedia.org/wiki/Gumbel_distribution">http://wikipedia.org/wiki/Gumbel_distribution</ulink>). <indexterm>
  <primary>Gumbel distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gumbel</secondary>
</indexterm> </para>

</example>

</sect1><sect1 id="density" remap="section">
  <title>PDFs</title>
    
  
  <para> <anchor id="a0000000155" /> <indexterm>
  <primary>PDF</primary>

</indexterm> <indexterm>
  <primary>probability density function</primary>

</indexterm> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> <indexterm>
  <primary>CDF</primary>

</indexterm> <indexterm>
  <primary>derivative</primary>

</indexterm> </para>

  
  <para>The derivative of a CDF is called a <emphasis role="bold">probability density function</emphasis>, or PDF. For example, the PDF of an exponential distribution is </para>

  
  <para><informalequation id="a0000000156" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0026.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PDF}_{expo}(x) = \lambda e^{-\lambda x}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The PDF of a normal distribution is </para>

  
  <para><informalequation id="a0000000157" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0027.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PDF}_{normal}(x) = \frac{1}{\sigma \sqrt {2 \pi }} \exp \left[ -\frac{1}{2} \left( \frac{x - \mu }{\sigma } \right)^2 \right]  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Evaluating a PDF for a particular value of <emphasis>x</emphasis> is usually not useful. The result is not a probability; it is a probability <emphasis>density</emphasis>. <indexterm>
  <primary>density</primary>

</indexterm> <indexterm>
  <primary>mass</primary>

</indexterm> </para>

  
  <para>In physics, density is mass per unit of volume; in order to get a mass, you have to multiply by volume or, if the density is not constant, you have to integrate over volume. <indexterm>
  <primary>inertia</primary>

</indexterm> <indexterm>
  <primary>moment of inertia</primary>

</indexterm> </para>

  
  <para>Similarly, probability density measures probability per unit of <emphasis>x</emphasis>. In order to get a probability mass<footnote><para>To take the analogy one step farther, the mean of a distribution is its center of mass, and the variance is its moment of inertia.</para></footnote>, you have to integrate over <emphasis>x</emphasis>. For example, if <emphasis>x</emphasis> is a random variable whose PDF is PDF<emphasis><subscript>X</subscript></emphasis>, we can compute the probability that a value from <emphasis>X</emphasis> falls between −0.5 and 0.5: </para>

  
  <para><informalequation id="a0000000158" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0028.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(-0.5 \le X &lt; 0.5) = \int _{-0.5}^{0.5} \mathrm{PDF}_{X}(x) dx  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Or, since the CDF is the integral of the PDF, we can write </para>

  
  <para><informalequation id="a0000000159" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0029.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(-0.5 \le X &lt; 0.5) = \mathrm{CDF}_{X}(0.5) - \mathrm{CDF}_{X}(-0.5)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> For some distributions we can evaluate the CDF explicitly, so we would use the second option. Otherwise we usually have to integrate the PDF numerically. </para>

  
  <example id="expo_pdf">
  <title></title>
  
  
  

  
  <para>What is the probability that a value chosen from an exponential distribution with parameter <emphasis>λ</emphasis> falls between 1 and 20? Express your answer as a function of <emphasis>λ</emphasis>. Keep this result handy; we will use it in <xref linkend="censored" />. <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>

</example>

  
  <example id="a0000000161">
  <title></title>
  
  
  <para> In the BRFSS (see <xref linkend="lognormal" />), the distribution of heights is roughly normal with parameters <emphasis>μ</emphasis> = 178 cm and <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis> = 59.4 cm for men, and <emphasis>μ</emphasis> = 163 cm and <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis> = 52.8 cm for women. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>height</primary>

</indexterm> <indexterm>
  <primary>Blue Man Group</primary>

</indexterm> <indexterm>
  <primary>Group, Blue Man</primary>

</indexterm> </para>

  
  <para>In order to join Blue Man Group, you have to be male between 5’10” and 6’1” (see <ulink url="http://bluemancasting.com">http://bluemancasting.com</ulink>). What percentage of the U.S. male population is in this range? Hint: see <xref linkend="normal" />. </para>

</example>

</sect1><sect1 id="a0000000162" remap="section">
  <title>Convolution</title>
    
  
  <para> <indexterm>
  <primary>convolution</primary>

</indexterm> <indexterm>
  <primary>PDF</primary>

</indexterm> </para>

  
  <para>Suppose we have two random variables, <emphasis>X</emphasis> and <emphasis>Y</emphasis>, with distributions CDF<emphasis><subscript>X</subscript></emphasis> and CDF<emphasis><subscript>Y</subscript></emphasis>. What is the distribution of the sum <emphasis>Z</emphasis> = <emphasis>X</emphasis> + <emphasis>Y</emphasis>? <indexterm>
  <primary>random variable</primary>

</indexterm> <indexterm>
  <primary>variable</primary>
<secondary>random</secondary>
</indexterm> <indexterm>
  <primary>sum</primary>

</indexterm> </para>

  
  <para>One option is to write a RandomVariable object that generates the sum: </para>

  
  <programlisting>class Sum(RandomVariable):
    def __init__(X, Y):
        self.X = X
        self.Y = Y

    def generate():
        return X.generate() + Y.generate()</programlisting>

  
  <para>Given any RandomVariables, <emphasis>X</emphasis> and <emphasis>Y</emphasis>, we can create a Sum object that represents <emphasis>Z</emphasis>. Then we can use a sample from <emphasis>Z</emphasis>to approximate CDF<emphasis><subscript>Z</subscript></emphasis>. </para>

  
  <para>This approach is simple and versatile, but not very efficient; we have to generate a large sample to estimate CDF<emphasis><subscript>Z</subscript></emphasis> accurately, and even then it is not exact. </para>

  
  <para>If CDF<emphasis><subscript>X</subscript></emphasis> and CDF<emphasis><subscript>Y</subscript></emphasis> are expressed as functions, sometimes we can find CDF<emphasis><subscript>Z</subscript></emphasis> exactly. Here’s how: </para>

  
  

  
  <para><orderedlist>
  
  <listitem>
  
  <para>To start, assume that the particular value of <emphasis>X</emphasis> is <emphasis>x</emphasis>. Then CDF<emphasis><subscript>Z</subscript></emphasis>(<emphasis>z</emphasis>) is </para>

  
  <para><informalequation id="a0000000163" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0030.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(Z \le z ~ |~ X = x) = P(Y \le z-x)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Let’s read that back. The left side is “the probability that the sum is less than <emphasis>z</emphasis>, given that the first term is <emphasis>x</emphasis>.” Well, if the first term is <emphasis>x</emphasis> and the sum has to be less than <emphasis>z</emphasis>, then the second term has to be less than <emphasis>z</emphasis> − <emphasis>x</emphasis>. </para>
</listitem>
  
  <listitem>
  
  <para>To get the probability that <emphasis>Y</emphasis> is less than <emphasis>z</emphasis> − <emphasis>x</emphasis>, we evaluate CDF<emphasis><subscript>Y</subscript></emphasis>. </para>

  
  <para><informalequation id="a0000000164" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0031.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(Y \le z-x) = \mathrm{CDF}_ Y(z-x)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> This follows from the definition of the CDF. </para>
</listitem>
  
  <listitem>
  
  <para>Good so far? Let’s go on. Since we don’t actually know the value of <emphasis>x</emphasis>, we have to consider all values it could have and integrate over them: </para>

  
  <para><informalequation id="a0000000165" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0032.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(Z \le z) = \int _{-\infty }^{\infty }P(Z \le z ~ |~ X = x) ~ \mathrm{PDF}_ X(x) ~ dx  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The integrand is “the probability that <emphasis>Z</emphasis> is less than or equal to <emphasis>z</emphasis>, given that <emphasis>X</emphasis> = <emphasis>x</emphasis>, times the probability that <emphasis>X</emphasis> = <emphasis>x</emphasis>.” </para>

  
  <para>Substituting from the previous steps we get </para>

  
  <para><informalequation id="a0000000166" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0033.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(Z \le z) = \int _{-\infty }^{\infty }\mathrm{CDF}_ Y(z-x) ~ \mathrm{PDF}_ X(x) ~ dx  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The left side is the definition of CDF<emphasis><subscript>Z</subscript></emphasis>, so we conclude: </para>

  
  <para><informalequation id="a0000000167" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0034.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{CDF}_ Z(z) = \int _{-\infty }^{\infty }\mathrm{CDF}_ Y(z-x) ~ \mathrm{PDF}_ X(x) ~ dx  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>
</listitem>
  
  <listitem>
  
  <para>To get PDF<emphasis><subscript>Z</subscript></emphasis>, take the derivative of both sides with respect to <emphasis>z</emphasis>. The result is </para>

  
  <para><informalequation id="a0000000168" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0035.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PDF}_ Z(z) = \int _{-\infty }^{\infty }\mathrm{PDF}_ Y(z-x) ~ \mathrm{PDF}_ X(x) ~ dx  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> If you have studied signals and systems, you might recognize that integral. It is the <emphasis role="bold">convolution</emphasis> of PDF<emphasis><subscript>Y</subscript></emphasis> and PDF<emphasis><subscript>X</subscript></emphasis>, denoted with the operator ∗. </para>

  
  <para><simplelist>
  <member> PDF<emphasis><subscript>Z</subscript></emphasis> = PDF<emphasis><subscript>Y</subscript></emphasis> ∗ PDF<emphasis><subscript>X</subscript></emphasis> </member>
</simplelist> </para>

  
  <para>So the distribution of the sum is the convolution of the distributions. See <ulink url="http://wiktionary.org/wiki/booyah">http://wiktionary.org/wiki/booyah</ulink>! <indexterm>
  <primary>booyah</primary>

</indexterm> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>
</listitem>
  
</orderedlist></para>

  
  <para> <indexterm>
  <primary>independent</primary>

</indexterm> </para>

  
  <para>As an example, suppose <emphasis>X</emphasis> and <emphasis>Y</emphasis> are random variables with an exponential distribution with parameter <emphasis>λ</emphasis>. The distribution of <emphasis>Z</emphasis> = <emphasis>X</emphasis> + <emphasis>Y</emphasis>is: </para>

  
  <para><informalequation id="a0000000169" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0036.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PDF}_ Z(z) = \int _{-\infty }^{\infty }\mathrm{PDF}_ X(x)~ \mathrm{PDF}_ Y(z-x) ~ dx = \int _{-\infty }^{\infty }\lambda e^{-\lambda x}~ \lambda e^{\lambda (z-x)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Now we have to remember that <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0037.png" depth="8.500px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$PDF_{expo}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> is 0 for all negative values, but we can handle that by adjusting the limits of integration: </para>

  
  <para><informalequation id="a0000000170" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0038.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PDF}_ Z(z) = \int _{0}^{z} \lambda e^{-\lambda x}~ \lambda e^{-\lambda (z-x)} ~ dx  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Now we can combine terms and move constants outside the integral: </para>

  
  <para><informalequation id="a0000000171" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0039.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PDF}_ Z(z) = \lambda ^2 e^{-\lambda z} \int _{0}^{z} dx = \lambda ^2 z ~ e^{-\lambda z}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> This, it turns out, is the PDF of an Erlang distribution with parameter <emphasis>k</emphasis> = 2 (see <ulink url="http://wikipedia.org/wiki/Erlang_distribution">http://wikipedia.org/wiki/Erlang_distribution</ulink>). So the convolution of two exponential distributions (with the same parameter) is an Erlang distribution. <indexterm>
  <primary>Erlang distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Erlang</secondary>
</indexterm> </para>

  
  <example id="a0000000172">
  <title></title>
  
  
  <para> If <emphasis>X</emphasis> has an exponential distribution with parameter <emphasis>λ</emphasis>, and <emphasis>Y</emphasis> has an Erlang distribution with parameters <emphasis>k</emphasis> and <emphasis>λ</emphasis>, what is the distribution of the sum <emphasis>Z</emphasis> = <emphasis>X</emphasis> + <emphasis>Y</emphasis>? <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>

</example>

  
  <example id="a0000000173">
  <title></title>
  
  
  <para> Suppose I draw two values from a distribution; what is the distribution of the larger value? Express your answer in terms of the PDF or CDF of the distribution. <indexterm>
  <primary>max</primary>

</indexterm> </para>

  
  <para>As the number of values increases, the distribution of the maximum converges on one of the extreme value distributions; see <ulink url="http://wikipedia.org/wiki/Gumbel_distribution">http://wikipedia.org/wiki/Gumbel_distribution</ulink>. <indexterm>
  <primary>Gumbel distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gumbel</secondary>
</indexterm> </para>

</example>

  
  <example id="a0000000174">
  <title></title>
  
  
  <para> If you are given Pmf objects, you can compute the distribution of the sum by enumerating all pairs of values: <indexterm>
  <primary>Pmf object</primary>

</indexterm> </para>

  
  <programlisting>for x in pmf_x.Values():
    for y in pmf_y.Values():
        z = x + y</programlisting>

  
  <para>Write a function that takes PMF<emphasis><subscript>X</subscript></emphasis> and PMF<emphasis><subscript>Y</subscript></emphasis> and returns a new Pmf that represents the distribution of the sum <emphasis>Z</emphasis> = <emphasis>X</emphasis> + <emphasis>Y</emphasis>. </para>

  
  <para>Write a similar function that computes the PMF of <emphasis>Z</emphasis> = max(<emphasis>X</emphasis>, <emphasis>Y</emphasis>). </para>

</example>

</sect1><sect1 id="why_normal" remap="section">
  <title>Why normal?</title>
    
  
  <para> <anchor id="a0000000175" /> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>I said earlier that normal distributions are amenable to analysis, but I didn’t say why. One reason is that they are closed under linear transformation and convolution. To explain what that means, it will help to introduce some notation. <indexterm>
  <primary>analysis</primary>

</indexterm> <indexterm>
  <primary>random variable</primary>

</indexterm> <indexterm>
  <primary>variable</primary>
<secondary>random</secondary>
</indexterm> </para>

  
  <para>If the distribution of a random variable, <emphasis>X</emphasis>, is normal with parameters <emphasis>μ</emphasis> and <emphasis>σ</emphasis>, you can write </para>

  
  <para><simplelist>
  <member> <emphasis>X</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis>, <emphasis>σ</emphasis>) </member>
</simplelist> </para>

  
  <para>where the symbol ∼ means “is distributed” and the script letter <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> stands for “normal.” </para>

  
  <para>A linear transformation of <emphasis>X</emphasis> is something like <emphasis>X</emphasis>’ = <emphasis>a</emphasis><emphasis>X</emphasis> + <emphasis>b</emphasis>, where <emphasis>a</emphasis>and <emphasis>b</emphasis> are real numbers.<indexterm>
  <primary>linear transformation</primary>

</indexterm> A family of distributions is closed under linear transformation if <emphasis>X</emphasis>’ is in the same family as <emphasis>X</emphasis>. The normal distribution has this property; if <emphasis>X</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis>, <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>), </para>

  
  <para><simplelist>
  <member> <emphasis>X</emphasis>’ ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>a</emphasis><emphasis>μ</emphasis> + <emphasis>b</emphasis>, <emphasis>a</emphasis><emphasis><superscript>2</superscript></emphasis> <emphasis>σ</emphasis>) </member>
</simplelist> </para>

  
  <para>Normal distributions are also closed under convolution. If <emphasis>Z</emphasis> = <emphasis>X</emphasis> + <emphasis>Y</emphasis>and <emphasis>X</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis><emphasis><subscript>X</subscript></emphasis>, <emphasis>σ</emphasis><emphasis><subscript>X</subscript></emphasis><emphasis><superscript>2</superscript></emphasis>) and <emphasis>Y</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis><emphasis><subscript>Y</subscript></emphasis>, <emphasis>σ</emphasis><emphasis><subscript>Y</subscript></emphasis><emphasis><superscript>2</superscript></emphasis>) then </para>

  
  <para><informalequation id="a0000000176" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0041.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  Z \sim \normal (\mu _ X + \mu _ Y, \sigma _ X^2 + \sigma _ Y^2)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The other distributions we have looked at do not have these properties. <indexterm>
  <primary>convolution</primary>

</indexterm> </para>

  
  <example id="a0000000177">
  <title></title>
  
  
  <para> If <emphasis>X</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis><emphasis><subscript>X</subscript></emphasis>, <emphasis>σ</emphasis><emphasis><subscript>X</subscript></emphasis><emphasis><superscript>2</superscript></emphasis>) and <emphasis>Y</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis><emphasis><subscript>Y</subscript></emphasis>, <emphasis>σ</emphasis><emphasis><subscript>Y</subscript></emphasis><emphasis><superscript>2</superscript></emphasis>), what is the distribution of <emphasis>Z</emphasis> = <emphasis>a</emphasis><emphasis>X</emphasis> + <emphasis>b</emphasis><emphasis>Y</emphasis>? </para>

</example>

  
  <example id="a0000000178">
  <title></title>
  
  
  <para> Let’s see what happens when we add values from other distributions. Choose a pair of distributions (any two of exponential, normal, lognormal, and Pareto) and choose parameters that make their mean and variance similar. <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> <indexterm>
  <primary>sum</primary>

</indexterm> </para>

  
  <para>Generate random numbers from these distributions and compute the distribution of their sums. Use the tests from <xref linkend="continuous" /> to see if the sum can be modeled by a continuous distribution. </para>

</example>

</sect1><sect1 id="CLT" remap="section">
  <title>Central limit theorem</title>
    
  
  <para> <anchor id="a0000000179" /> <indexterm>
  <primary>Central Limit Theorem</primary>

</indexterm> </para>

  
  <para>So far we have seen: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>If we add values drawn from normal distributions, the distribution of the sum is normal. <indexterm>
  <primary>sum</primary>

</indexterm> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>If we add values drawn from other distributions, the sum does not generally have one of the continuous distributions we have seen. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>But it turns out that if we add up a large number of values from almost any distribution, the distribution of the sum converges to normal. </para>

  
  <para>More specifically, if the distribution of the values has mean and standard deviation <emphasis>μ</emphasis> and <emphasis>σ</emphasis>, the distribution of the sum is approximately <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>n</emphasis><emphasis>μ</emphasis>, <emphasis>n</emphasis><emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>). </para>

  
  <para>This is called the <emphasis role="bold">Central Limit Theorem</emphasis>. It is one of the most useful tools for statistical analysis, but it comes with caveats: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>The values have to be drawn independently. <indexterm>
  <primary>independent</primary>

</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>The values have to come from the same distribution (although this requirement can be relaxed). <indexterm>
  <primary>identical</primary>

</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>The values have to be drawn from a distribution with finite mean and variance, so most Pareto distributions are out. <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>variance</primary>

</indexterm> <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>The number of values you need before you see convergence depends on the skewness of the distribution. Sums from an exponential distribution converge for small sample sizes. Sums from a lognormal distribution do not. <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> </para>
</listitem>
  
</itemizedlist></para>

  
  <para>The Central Limit Theorem explains, at least in part, the prevalence of normal distributions in the natural world. Most characteristics of animals and other life forms are affected by a large number of genetic and environmental factors whose effect is additive. The characteristics we measure are the sum of a large number of small effects, so their distribution tends to be normal. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <example id="a0000000180">
  <title></title>
  
  
  <para> If I draw a sample, <emphasis>x</emphasis><emphasis><subscript>1</subscript></emphasis> .. <emphasis>x</emphasis><emphasis><subscript>n</subscript></emphasis>, independently from a distribution with finite mean <emphasis>μ</emphasis> and variance <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>, what is the distribution of the sample mean: </para>

  
  <para><informalequation id="a0000000181" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0042.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \bar{x}= \frac{1}{n} \sum x_ i  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> As <emphasis>n</emphasis> increases, what happens to the variance of the sample mean? Hint: review <xref linkend="why_normal" />. </para>

</example>

  
  <example id="a0000000182">
  <title></title>
  
  
  <para> Choose a distribution (one of exponential, lognormal or Pareto) and choose values for the parameter(s). Generate samples with sizes 2, 4, 8, etc., and compute the distribution of their sums. Use a normal probability plot to see if the distribution is approximately normal. How many terms do you have to add to see convergence? <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>Pareto distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Pareto</secondary>
</indexterm> <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> <indexterm>
  <primary>convergence</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000183">
  <title></title>
  
  
  <para> Instead of the distribution of sums, compute the distribution of products; what happens as the number of terms increases? Hint: look at the distribution of the log of the products. <indexterm>
  <primary>logarithm</primary>

</indexterm> <indexterm>
  <primary>product</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000184" remap="section">
  <title>The distribution framework</title>
    
  
  <para> <indexterm>
  <primary>distribution framework</primary>

</indexterm> <indexterm>
  <primary>framework, distributions</primary>

</indexterm> </para>

  
  <figure id="dist_framework">
  
  <title>A framework that relates representations of distribution functions.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/distribution_functions.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>At this point we have seen PMFs, CDFs and PDFs; let’s take a minute to review. <xref linkend="dist_framework" /> shows how these functions relate to each other. <indexterm>
  <primary>PMF</primary>

</indexterm> </para>

  
  <para>We started with PMFs, which represent the probabilities for a discrete set of values. To get from a PMF to a CDF, we computed a cumulative sum. To be more consistent, a discrete CDF should be called a cumulative mass function (CMF), but as far as I can tell no one uses that term. <indexterm>
  <primary>CDF</primary>

</indexterm> </para>

  
  <para>To get from a CDF to a PMF, you can compute differences in cumulative probabilities. <indexterm>
  <primary>PDF</primary>

</indexterm> </para>

  
  <para>Similarly, a PDF is the derivative of a continuous CDF; or, equivalently, a CDF is the integral of a PDF. But remember that a PDF maps from values to probability densities; to get a probability, you have to integrate. <indexterm>
  <primary>discrete</primary>

</indexterm> <indexterm>
  <primary>continuous</primary>

</indexterm> </para>

  
  <para>To get from a discrete to a continuous distribution, you can perform various kinds of smoothing. One form of smoothing is to assume that the data come from an analytic continuous distribution (like exponential or normal) and to estimate the parameters of that distribution. And that’s what <xref linkend="estimation" /> is about. <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> <indexterm>
  <primary>bin</primary>

</indexterm> </para>

  
  <para>If you divide a PDF into a set of bins, you can generate a PMF that is at least an approximation of the PDF. We use this technique in <xref linkend="estimation" /> to do Bayesian estimation. <indexterm>
  <primary>Bayesian estimation</primary>

</indexterm> <indexterm>
  <primary>estimation</primary>
<secondary>Bayesian</secondary>
</indexterm> </para>

  
  <example id="a0000000186">
  <title></title>
  
  
  <para> Write a function called <literal>MakePmfFromCdf</literal> that takes a Cdf object and returns the corresponding Pmf object. </para>

  
  <para>You can find a solution to this exercise in <literal>thinkstats.com/Pmf.py</literal>. <indexterm>
  <primary sortas="pmf.py">Pmf.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000187" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>skewness:</term>
      <listitem>
  
  <para>A characteristic of a distribution; intuitively, it is a measure of how asymmetric the distribution is. <indexterm>
  <primary>skewness</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>robust:</term>
      <listitem>
  
  <para>A statistic is robust if it is relatively immune to the effect of outliers. <indexterm>
  <primary>robust</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>illusory superiority:</term>
      <listitem>
  
  <para>The tendency of people to imagine that they are better than average. <indexterm>
  <primary>illusory superiority</primary>

</indexterm> <indexterm>
  <primary>fallacy</primary>
<secondary>illusory superiority</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>random variable:</term>
      <listitem>
  
  <para>An object that represents a random process. <indexterm>
  <primary>random variable</primary>

</indexterm> <indexterm>
  <primary>variable</primary>
<secondary>random</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>random variate:</term>
      <listitem>
  
  <para>A value generated by a random process. <indexterm>
  <primary>random variate</primary>

</indexterm> <indexterm>
  <primary>variate</primary>
<secondary>random</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>PDF:</term>
      <listitem>
  
  <para>Probability density function, the derivative of a continuous CDF. <indexterm>
  <primary>PDF</primary>

</indexterm> <indexterm>
  <primary>probability density function</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>convolution:</term>
      <listitem>
  
  <para>An operation that computes the distribution of the sum of values from two distributions. <indexterm>
  <primary>convolution</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Central Limit Theorem:</term>
      <listitem>
  
  <para>“The supreme law of Unreason,” according to Sir Francis Galton, an early statistician. <indexterm>
  <primary>Central Limit Theorem</primary>

</indexterm> <indexterm>
  <primary>Galton, Francis</primary>

</indexterm> <indexterm>
  <primary>unreason, supreme law</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="testing">
  <title>Hypothesis testing</title>
  
  
  <para> <anchor id="a0000000188" /> <indexterm>
  <primary>hypothesis testing</primary>

</indexterm> <indexterm>
  <primary>apparent effect</primary>

</indexterm> </para>

  
  <para>Exploring the data from the NSFG, we saw several “apparent effects,” including a number of differences between first babies and others. So far we have taken these effects at face value; in this chapter, finally, we put them to the test. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>The fundamental question we want to address is whether these effects are real. For example, if we see a difference in the mean pregnancy length for first babies and others, we want to know whether that difference is real, or whether it occurred by chance. <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>That question turns out to be hard to address directly, so we will proceed in two steps. First we will test whether the effect is <emphasis role="bold">significant</emphasis>, then we will try to interpret the result as an answer to the original question. <indexterm>
  <primary>significant</primary>

</indexterm> </para>

  
  <para>In the context of statistics, “significant” has a technical definition that is different from its use in common language. As defined earlier, an apparent effect is statistically significant if it is unlikely to have occurred by chance. <indexterm>
  <primary>chance</primary>

</indexterm> </para>

  
  <para>To make this more precise, we have to answer three questions: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>What do we mean by “chance”? </para>
</listitem>
  
  <listitem>
  
  <para>What do we mean by “unlikely”? </para>
</listitem>
  
  <listitem>
  
  <para>What do we mean by “effect”? </para>
</listitem>
  
</orderedlist></para>

  
  <para>All three of these questions are harder than they look. Nevertheless, there is a general structure that people use to test statistical significance: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>Null hypothesis:</term>
      <listitem>
  
  <para>The <emphasis role="bold">null hypothesis</emphasis> is a model of the system based on the assumption that the apparent effect was actually due to chance. <indexterm>
  <primary>null hypothesis</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>p-value:</term>
      <listitem>
  
  <para>The <emphasis role="bold">p-value</emphasis> is the probability of the apparent effect under the null hypothesis. <indexterm>
  <primary>p-value</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Interpretation:</term>
      <listitem>
  
  <para>Based on the p-value, we conclude that the effect is either statistically significant, or not. </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>This process is called <emphasis role="bold">hypothesis testing</emphasis>. The underlying logic is similar to a proof by contradiction. To prove a mathematical statement, A, you assume temporarily that A is false. If that assumption leads to a contradiction, you conclude that A must actually be true. <indexterm>
  <primary>contradiction, proof by</primary>

</indexterm> </para>

  
  <para>Similarly, to test a hypothesis like, “This effect is real,” we assume, temporarily, that is is not. That’s the null hypothesis. Based on that assumption, we compute the probability of the apparent effect. That’s the p-value. If the p-value is low enough, we conclude that the null hypothesis is unlikely to be true. </para>
<sect1 id="a0000000189" remap="section">
  <title>Testing a difference in means</title>
    
  
  <para> <indexterm>
  <primary>mean, difference in</primary>

</indexterm> </para>

  
  <para>One of the easiest hypotheses to test is an apparent difference in mean between two groups. In the NSFG data, we saw that the mean pregnancy length for first babies is slightly longer, and the mean weight at birth is slightly smaller. Now we will see if those effects are significant. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>For these examples, the null hypothesis is that the distributions for the two groups are the same, and that the apparent difference is due to chance. <indexterm>
  <primary>null hypothesis</primary>

</indexterm> </para>

  
  <para>To compute p-values, we find the pooled distribution for all live births (first babies and others), generate random samples that are the same size as the observed samples, and compute the difference in means under the null hypothesis. <indexterm>
  <primary>p-value</primary>

</indexterm> </para>

  
  <para>If we generate a large number of samples, we can count how often the difference in means (due to chance) is as big or bigger than the difference we actually observed. This fraction is the p-value. </para>

  
  <para>For pregnancy length, we observed <emphasis>n</emphasis> = 4413 first babies and <emphasis>m</emphasis> = 4735 others, and the difference in mean was <emphasis>δ</emphasis> = 0.078 weeks. To approximate the p-value of this effect, I pooled the distributions, generated samples with sizes <emphasis>n</emphasis> and <emphasis>m</emphasis> and computed the difference in mean. <indexterm>
  <primary>resampling</primary>

</indexterm> </para>

  
  <para>This is another example of resampling, because we are drawing a random sample from a dataset that is, itself, a sample of the general population. I computed differences for 1000 sample pairs; <xref linkend="length_deltas_cdf" /> shows their distribution. </para>

  
  <figure id="length_deltas_cdf">
  
  <title>CDF of difference in mean for resampled data.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/length_deltas_cdf.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>The mean difference is near 0, as you would expect with samples from the same distribution. The vertical lines show the cutoffs where <emphasis>x</emphasis> = −<emphasis>δ</emphasis> or <emphasis>x</emphasis> = <emphasis>δ</emphasis>. </para>

  
  <para>Of 1000 sample pairs, there were 166 where the difference in mean (positive or negative) was as big or bigger than <emphasis>δ</emphasis>, so the p-value is approximately 0.166. In other words, we expect to see an effect as big as <emphasis>δ</emphasis> about 17% of the time, even if the actual distribution for the two groups is the same. </para>

  
  <para>So the apparent effect is not very likely, but is it unlikely enough? I’ll address that in the next section. </para>

  
  <example id="a0000000191">
  <title></title>
  
  
  <para> In the NSFG dataset, the difference in mean weight for first births is 2.0 ounces. Compute the p-value of this difference. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>Hint: for this kind of resampling it is important to sample with replacement, so you should use <literal>random.choice</literal> rather than <literal>random.sample</literal> (see <xref linkend="random" />). <indexterm>
  <primary>resampling</primary>

</indexterm> <indexterm>
  <primary>random module</primary>

</indexterm> </para>

  
  <para>You can start with the code I used to generate the results in this section, which you can download from <ulink url="http://thinkstats.com/hypothesis.py">http://thinkstats.com/hypothesis.py</ulink>. <indexterm>
  <primary>hypothesis.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="threshold" remap="section">
  <title>Choosing a threshold</title>
    
  
  <para> <anchor id="a0000000192" /> <indexterm>
  <primary>threshold</primary>

</indexterm> </para>

  
  <para>In hypothesis testing we have to worry about two kinds of errors. <indexterm>
  <primary>false positive</primary>

</indexterm> <indexterm>
  <primary>false negative</primary>

</indexterm> <indexterm>
  <primary>error, Type I</primary>

</indexterm> <indexterm>
  <primary>error, Type II</primary>

</indexterm> </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>A Type I error, also called a <emphasis role="bold">false positive</emphasis>, is when we accept a hypothesis that is actually false; that is, we consider an effect significant when it was actually due to chance. </para>
</listitem>
  
    <listitem>
  
  <para>A Type II error, also called a <emphasis role="bold">false negative</emphasis>, is when we reject a hypothesis that is actually true; that is, we attribute an effect to chance when it was actually real. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>The most common approach to hypothesis testing is to choose a threshold<footnote><para>Also known as a “Significance criterion.”</para></footnote>, <emphasis>α</emphasis>, for the p-value and to accept as significant any effect with a p-value less than <emphasis>α</emphasis>. A common choice for <emphasis>α</emphasis> is 5%. By this criterion, the apparent difference in pregnancy length for first babies is not significant, but the difference in weight is. <indexterm>
  <primary>significance criterion</primary>

</indexterm> <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>For this kind of hypothesis testing, we can compute the probability of a false positive explicitly: it turns out to be <emphasis>α</emphasis>. </para>

  
  <para>To see why, think about the definition of false positive—the chance of accepting a hypothesis that is false—and the definition of a p-value—the chance of generating the measured effect if the hypothesis is false. </para>

  
  <para>Putting these together, we can ask: if the hypothesis is false, what is the chance of generating a measured effect that will be considered significant with threshold <emphasis>α</emphasis>? The answer is <emphasis>α</emphasis>. </para>

  
  <para>We can decrease the chance of a false positive by decreasing the threshold. For example, if the threshold is 1%, there is only a 1% chance of a false positive. </para>

  
  <para>But there is a price to pay: decreasing the threshold raises the standard of evidence, which increases the chance of rejecting a valid hypothesis. </para>

  
  <para>In general there is a tradeoff between Type I and Type II errors. The only way to decrease both at the same time is to increase the sample size (or, in some cases, decrease measurement error). <indexterm>
  <primary>sample size</primary>

</indexterm> </para>

  
  <example id="a0000000193">
  <title></title>
  
  
  <para> To investigate the effect of sample size on p-value, see what happens if you discard half of the data from the NSFG. Hint: use <literal>random.sample</literal>. What if you discard three-quarters of the data, and so on? <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>What is the smallest sample size where the difference in mean birth weight is still significant with <emphasis>α</emphasis> = 5%? How much larger does the sample size have to be with <emphasis>α</emphasis> = 1%? </para>

  
  <para>You can start with the code I used to generate the results in this section, which you can download from <ulink url="http://thinkstats.com/hypothesis.py">http://thinkstats.com/hypothesis.py</ulink>. <indexterm>
  <primary>hypothesis.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000194" remap="section">
  <title>Defining the effect</title>
    
  
  <para>When something unusual happens, people often say something like, “Wow! What were the chances of <emphasis>that</emphasis>?” This question makes sense because we have an intuitive sense that some things are more likely than others. But this intuition doesn’t always hold up to scrutiny. <indexterm>
  <primary>chance</primary>

</indexterm> <indexterm>
  <primary>coin</primary>

</indexterm> </para>

  
  <para>For example, suppose I toss a coin 10 times, and after each toss I write down H for heads and T for tails. If the result was a sequence like THHTHTTTHH, you wouldn’t be too surprised. But if the result was HHHHHHHHHH, you would say something like, “Wow! What were the chances of <emphasis>that</emphasis>?” </para>

  
  <para>But in this example, the probability of the two sequences is the same: one in 1024. And the same is true for any other sequence. So when we ask, “What were the chances of <emphasis>that</emphasis>,” we have to be careful about what we mean by “that.” </para>

  
  <para>For the NSFG data, I defined the effect as “a difference in mean (positive or negative) as big or bigger than <emphasis>δ</emphasis>.” By making this choice, I decided to evaluate the magnitude of the difference, ignoring the sign. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>A test like that is called <emphasis role="bold">two-sided</emphasis>, because we consider both sides (positive and negative) in the distribution from <xref linkend="length_deltas_cdf" />. By using a two-sided test we are testing the hypothesis that there is a significant difference between the distributions, without specifying the sign of the difference. <indexterm>
  <primary>one-sided test</primary>

</indexterm> <indexterm>
  <primary>two-sided test</primary>

</indexterm> <indexterm>
  <primary>test</primary>
<secondary>one-sided</secondary>
</indexterm> <indexterm>
  <primary>test</primary>
<secondary>two-sided</secondary>
</indexterm> </para>

  
  <para>The alternative is to use a <emphasis role="bold">one-sided</emphasis> test, which asks whether the mean for first babies is significantly <emphasis>higher</emphasis> than the mean for others. Because the hypothesis is more specific, the p-value is lower—in this case it is roughly half. </para>

</sect1><sect1 id="a0000000195" remap="section">
  <title>Interpreting the result</title>
    
  
  <para>At the beginning of this chapter I said that the question we want to address is whether an apparent effect is real. We started by defining the null hypothesis, denoted <emphasis>H</emphasis><emphasis><subscript>0</subscript></emphasis>, which is the hypothesis that the effect is not real. Then we defined the p-value, which is <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>0</subscript></emphasis>), where <emphasis>E</emphasis>is an effect as big as or bigger than the apparent effect. Then we computed p-values and compared them to a threshold, <emphasis>α</emphasis>. </para>

  
  <para>That’s a useful step, but it doesn’t answer the original question, which is whether the effect is real. There are several ways to interpret the result of a hypothesis test: </para>

  
  <para><variablelist>
  <varlistentry>
    <term>Classical:</term>
      <listitem>
  
  <para>In classical hypothesis testing, if a p-value is less than <emphasis>α</emphasis>, you can say that the effect is statistically significant, but you can’t conclude that it’s real. This formulation is careful to avoid leaping to conclusions, but it is deeply unsatisfying. </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Practical:</term>
      <listitem>
  
  <para>In practice, people are not so formal. In most science journals, researchers report p-values without apology, and readers interpret them as evidence that the apparent effect is real. The lower the p-value, the higher their confidence in this conclusion. <indexterm>
  <primary>Bayesian probability</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>Bayesian:</term>
      <listitem>
  
  <para>What we really want to know is <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>|<emphasis>E</emphasis>), where <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis> is the hypothesis that the effect is real. By Bayes’s theorem </para>

  
  <para><informalequation id="a0000000196" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0043.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(H_ A ~ |~ E) = \frac{P(E ~ |~ H_ A) ~ P(H_ A)}{P(E)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> where <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) is the prior probability of <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis> before we saw the effect, <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) is the probability of seeing <emphasis>E</emphasis>, assuming that the effect is real, and <emphasis>P</emphasis>(<emphasis>E</emphasis>) is the probability of seeing <emphasis>E</emphasis>under any hypothesis. Since the effect is either real or it’s not, </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>E</emphasis>) = <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) + <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>0</subscript></emphasis>) <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>0</subscript></emphasis>) </member>
</simplelist> </para>
</listitem>
  </varlistentry>
</variablelist></para>

  
  <para>As an example, I’ll compute <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>|E) for pregnancy lengths in the NSFG. We have already computed <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>0</subscript></emphasis>) = 0.166, so all we have to do is compute <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) and choose a value for the prior. <indexterm>
  <primary>prior probability</primary>

</indexterm> <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>To compute <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>), we assume that the effect is real—that is, that the difference in mean duration, <emphasis>δ</emphasis>, is actually what we observed, 0.078. (This way of formulating <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis> is a little bit bogus. I will explain and fix the problem in the next section.) </para>

  
  <para>By generating 1000 sample pairs, one from each distribution, I estimated <emphasis>P</emphasis>(E|<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) = 0.494. With the prior <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) = 0.5, the posterior probability of <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis> is 0.748. <indexterm>
  <primary>posterior probability</primary>

</indexterm> <indexterm>
  <primary>update</primary>

</indexterm> </para>

  
  <para>So if the prior probability of <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis> is 50%, the updated probability, taking into account the evidence from this dataset, is almost 75%. It makes sense that the posterior is higher, since the data provide some support for the hypothesis. But it might seem surprising that the difference is so large, especially since we found that the difference in means was not statistically significant. </para>

  
  <para>In fact, the method I used in this section is not quite right, and it tends to overstate the impact of the evidence. In the next section we will correct this tendency. </para>

  
  <example id="a0000000197">
  <title></title>
  
  
  <para> Using the data from the NSFG, what is the posterior probability that the distribution of birth weights is different for first babies and others? <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>You can start with the code I used to generate the results in this section, which you can download from <ulink url="http://thinkstats.com/hypothesis.py">http://thinkstats.com/hypothesis.py</ulink>. <indexterm>
  <primary>hypothesis.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000198" remap="section">
  <title>Cross-validation</title>
    
  
  <para> <indexterm>
  <primary>cross-validation</primary>

</indexterm> </para>

  
  <para>In the previous example, we used the dataset to formulate the hypothesis <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>, and then we used the same dataset to test it. That’s not a good idea; it is too easy to generate misleading results. </para>

  
  <para>The problem is that even when the null hypothesis is true, there is likely to be some difference, <emphasis>δ</emphasis>, between any two groups, just by chance. If we use the observed value of <emphasis>δ</emphasis> to formulate the hypothesis, <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>|<emphasis>E</emphasis>) is likely to be high even when <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis> is false. </para>

  
  <para>We can address this problem with <emphasis role="bold">cross-validation</emphasis>, which uses one dataset to compute <emphasis>δ</emphasis> and a <emphasis>different</emphasis> dataset to evaluate <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>. The first dataset is called the <emphasis role="bold">training set</emphasis>; the second is called the <emphasis role="bold">testing set</emphasis>. <indexterm>
  <primary>training set</primary>

</indexterm> <indexterm>
  <primary>test set</primary>

</indexterm> <indexterm>
  <primary>cohort</primary>

</indexterm> <indexterm>
  <primary>cycle</primary>

</indexterm> </para>

  
  <para>In a study like the NSFG, which studies a different cohort in each cycle, we can use one cycle for training and another for testing. Or we can partition the data into subsets (at random), then use one for training and one for testing. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>I implemented the second approach, dividing the Cycle 6 data roughly in half. I ran the test several times with different random partitions. The average posterior probability was <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>|<emphasis>E</emphasis>) = 0.621. As expected, the impact of the evidence is smaller, partly because of the smaller sample size in the test set, and also because we are no longer using the same data for training and testing. </para>

</sect1><sect1 id="a0000000199" remap="section">
  <title>Reporting Bayesian probabilities</title>
    
  
  <para> <indexterm>
  <primary>Bayesian probability</primary>

</indexterm> </para>

  
  <para>In the previous section we chose the prior probability <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>) = 0.5. If we have a set of hypotheses and no reason to think one is more likely than another, it is common to assign each the same probability. </para>

  
  <para>Some people object to Bayesian probabilities because they depend on prior probabilities, and people might not agree on the right priors. For people who expect scientific results to be objective and universal, this property is deeply unsettling. <indexterm>
  <primary>subjective belief</primary>

</indexterm> <indexterm>
  <primary>belief</primary>

</indexterm> <indexterm>
  <primary>swamp</primary>

</indexterm> <indexterm>
  <primary>convergence</primary>

</indexterm> </para>

  
  <para>One response to this objection is that, in practice, strong evidence tends to swamp the effect of the prior, so people who start with different priors will converge toward the same posterior probability. <indexterm>
  <primary>likelihood ratio</primary>

</indexterm> <indexterm>
  <primary>Bayes factor</primary>

</indexterm> </para>

  
  <para>Another option is to report just the <emphasis role="bold">likelihood ratio</emphasis>, <emphasis>P</emphasis>(E | <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>)  /  <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis><emphasis><subscript>0</subscript></emphasis>), rather than the posterior probability. That way readers can plug in whatever prior they like and compute their own posteriors (no pun intended). The likelihood ratio is sometimes called a Bayes factor (see <ulink url="http://wikipedia.org/wiki/Bayes_factor">http://wikipedia.org/wiki/Bayes_factor</ulink>). </para>

  
  <example id="a0000000200">
  <title></title>
  
  
  <para> If your prior probability for a hypothesis, <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>, is 0.3 and new evidence becomes available that yields a likelihood ratio of 3 relative to the null hypothesis, <emphasis>H</emphasis><emphasis><subscript>0</subscript></emphasis>, what is your posterior probability for <emphasis>H</emphasis><emphasis><subscript>A</subscript></emphasis>? <indexterm>
  <primary>null hypothesis</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000201">
  <title></title>
  
  
  <para> This exercise is adapted from MacKay, <emphasis>Information Theory, Inference, and Learning Algorithms</emphasis>: <indexterm>
  <primary>MacKay, David</primary>

</indexterm> </para>

  
  <blockquote remap="quote">
  <para>Two people have left traces of their own blood at the scene of a crime. A suspect, Oliver, is tested and found to have type O blood. The blood groups of the two traces are found to be of type O (a common type in the local population, having frequency 60%) and of type AB (a rare type, with frequency 1%). Do these data (the blood types found at the scene) give evidence in favor of the proposition that Oliver was one of the two people whose blood was found at the scene? </para>
</blockquote>

  
  <para>Hint: Compute the likelihood ratio for this evidence; if it is greater than 1, then the evidence is in favor of the proposition. For a solution and discussion, see page 55 of MacKay’s book. <indexterm>
  <primary>likelihood</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000202" remap="section">
  <title>Chi-square test</title>
    
  
  <para> <indexterm>
  <primary>Chi-square test</primary>

</indexterm> </para>

  
  <para>In <xref linkend="threshold" /> we concluded that the apparent difference in mean pregnancy length for first babies and others was not significant. But in <xref linkend="relative.risk" />, when we computed relative risk, we saw that first babies are more likely to be early, less likely to be on time, and more likely to be late. <indexterm>
  <primary>relative risk</primary>

</indexterm> <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>So maybe the distributions have the same mean and different variance. We could test the significance of the difference in variance, but variances are less robust than means, and hypothesis tests for variance often behave badly. <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>variance</primary>

</indexterm> <indexterm>
  <primary>hypothesis</primary>

</indexterm> </para>

  
  <para>An alternative is to test a hypothesis that more directly reflects the effect as it appears; that is, the hypothesis that first babies are more likely to be early, less likely to be on time, and more likely to be late. </para>

  
  <para>We proceed in five easy steps: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>We define a set of categories, called <emphasis role="bold">cells</emphasis>, that each baby might fall into. In this example, there are six cells because there are two groups (first babies and others) and three bins (early, on time or late). <indexterm>
  <primary>cell</primary>

</indexterm> </para>

  
  <para>I’ll use the definitions from <xref linkend="relative.risk" />: a baby is early if it is born during Week 37 or earlier, on time if it is born during Week 38, 39 or 40, and late if it is born during Week 41 or later. </para>
</listitem>
  
  <listitem>
  
  <para>We compute the number of babies we expect in each cell. Under the null hypothesis, we assume that the distributions are the same for the two groups, so we can compute the pooled probabilities: <emphasis>P</emphasis>(early), <emphasis>P</emphasis>(ontime) and <emphasis>P</emphasis>(late). </para>

  
  <para>For first babies, we have <emphasis>n</emphasis> = 4413 samples, so under the null hypothesis we expect <emphasis>n</emphasis> <emphasis>P</emphasis>(early) first babies to be early, <emphasis>n</emphasis> <emphasis>P</emphasis>(ontime) to be on time, etc. Likewise, we have <emphasis>m</emphasis> = 4735 other babies, so we expect <emphasis>m</emphasis> <emphasis>P</emphasis>(early) other babies to be early, etc. </para>
</listitem>
  
  <listitem>
  
  <para>For each cell we compute the deviation; that is, the difference between the observed value, <emphasis>O</emphasis><emphasis><subscript>i</subscript></emphasis>, and the expected value, <emphasis>E</emphasis><emphasis><subscript>i</subscript></emphasis>. <indexterm>
  <primary>test statistic</primary>

</indexterm> <indexterm>
  <primary>chi-square statistic</primary>

</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>We compute some measure of the total deviation; this quantity is called the <emphasis role="bold">test statistic</emphasis>. The most common choice is the chi-square statistic: </para>

  
  <para><informalequation id="a0000000203" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0044.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \chi ^2 = \sum _ i \frac{(O_ i - E_ i)^2}{E_ i}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>
</listitem>
  
  <listitem>
  
  <para>We can use a Monte Carlo simulation to compute the p-value, which is the probability of seeing a chi-square statistic as high as the observed value under the null hypothesis. <indexterm>
  <primary>simulation</primary>
<secondary>Monte Carlo</secondary>
</indexterm> <indexterm>
  <primary>Monte Carlo</primary>

</indexterm> </para>
</listitem>
  
</orderedlist></para>

  
  <para>When the chi-square statistic is used, this process is called a <emphasis role="bold">chi-square test</emphasis>. One feature of the chi-square test is that the distribution of the test statistic can be computed analytically. <indexterm>
  <primary>chi-square test</primary>

</indexterm> <indexterm>
  <primary>analysis</primary>

</indexterm> </para>

  
  <para>Using the data from the NSFG I computed <emphasis>χ</emphasis><emphasis><superscript>2</superscript></emphasis> = 91.64, which would occur by chance about one time in 10,000. I conclude that this result is statistically significant, with one caution: again we used the same dataset for exploration and testing. It would be a good idea to confirm this result with another dataset. <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>You can download the code I used in this section from <ulink url="http://thinkstats.com/chi.py">http://thinkstats.com/chi.py</ulink>. <indexterm>
  <primary>chi.py</primary>

</indexterm> </para>

  
  <example id="a0000000204">
  <title></title>
  
  
  <para> Suppose you run a casino and you suspect that a customer has replaced a die provided by the casino with a “crooked die;” that is, one that has been tampered with to make one of the faces more likely to come up than the others. You apprehend the alleged cheater and confiscate the die, but now you have to prove that it is crooked. <indexterm>
  <primary>casino</primary>

</indexterm> <indexterm>
  <primary>dice</primary>

</indexterm> <indexterm>
  <primary>crooked die</primary>

</indexterm> </para>

  
  <para>You roll the die 60 times and get the following results: </para>

  
  <para> 
   
   
     <informaltable remap="tabular">
     <tr>
     
       
       <td>
  
  <para> Value </para>
</td>
     
       
       <td>
  
  <para> 1 </para>
</td>
     
       
       <td>
  
  <para> 2 </para>
</td>
     
       
       <td>
  
  <para> 3 </para>
</td>
     
       
       <td>
  
  <para> 4 </para>
</td>
     
       
       <td>
  
  <para> 5 </para>
</td>
     
       
       <td>
  
  <para> 6 </para>
</td>
     
     </tr><tr>
     
       
       <td>
  
  <para>Frequency </para>
</td>
     
       
       <td>
  
  <para> 8 </para>
</td>
     
       
       <td>
  
  <para> 9 </para>
</td>
     
       
       <td>
  
  <para> 19 </para>
</td>
     
       
       <td>
  
  <para> 6 </para>
</td>
     
       
       <td>
  
  <para> 8 </para>
</td>
     
       
       <td>
  
  <para> 10 </para>
</td>
     
     </tr>
     </informaltable>
   
 </para>

  
  <para>What is the chi-squared statistic for these values? What is the probability of seeing a chi-squared value as large by chance? </para>

</example>

</sect1><sect1 id="a0000000205" remap="section">
  <title>Efficient resampling</title>
    
  
  <para> <indexterm>
  <primary>resampling</primary>

</indexterm> </para>

  
  <para>Anyone reading this book who has prior training in statistics probably laughed when they saw <xref linkend="length_deltas_cdf" />, because I used a lot of computer power to simulate something I could have figured out analytically. <indexterm>
  <primary>analysis</primary>

</indexterm> <indexterm>
  <primary>pregnancy length</primary>

</indexterm> <indexterm>
  <primary>length</primary>
<secondary>pregnancy</secondary>
</indexterm> </para>

  
  <para>Obviously mathematical analysis is not the focus of this book. I am willing to use computers to do things the “dumb” way, because I think it is easier for beginners to understand simulations, and easier to demonstrate that they are correct. So as long as the simulations don’t take too long to run, I don’t feel guilty for skipping the analysis. </para>

  
  <para>However, there are times when a little analysis can save a lot of computing, and <xref linkend="length_deltas_cdf" /> is one of those times. <indexterm>
  <primary>mean, difference in</primary>

</indexterm> </para>

  
  <para>Remember that we were testing the observed difference in the mean between pregnancy lengths for <emphasis>n</emphasis> = 4413 first babies and <emphasis>m</emphasis> = 4735 others. We formed the pooled distribution for all babies, drew samples with sizes <emphasis>n</emphasis> and <emphasis>m</emphasis>, and computed the difference in sample means. <indexterm>
  <primary>sample mean</primary>

</indexterm> </para>

  
  <para>Instead, we could directly compute the distribution of the difference in sample means. To get started, let’s think about what a sample mean is: we draw <emphasis>n</emphasis> samples from a distribution, add them up, and divide by <emphasis>n</emphasis>. If the distribution has mean <emphasis>μ</emphasis> and variance <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>, then by the Central Limit Theorem, we know that the sum of the samples is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>n</emphasis><emphasis>μ</emphasis>, <emphasis>n</emphasis><emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>). <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>To figure out the distribution of the sample means, we have to invoke one of the properties of the normal distribution: if <emphasis>X</emphasis> is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis>, <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>), </para>

  
  <para><simplelist>
  <member> <emphasis>a</emphasis><emphasis>X</emphasis> + <emphasis>b</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>a</emphasis><emphasis>μ</emphasis> + <emphasis>b</emphasis>, <emphasis>a</emphasis><emphasis><superscript>2</superscript></emphasis> <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>) </member>
</simplelist> </para>

  
  <para>When we divide by <emphasis>n</emphasis>, <emphasis>a</emphasis> = 1/<emphasis>n</emphasis>and <emphasis>b</emphasis> = 0, so </para>

  
  <para><emphasis>X</emphasis>/<emphasis>n</emphasis> ∼ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis>/<emphasis>n</emphasis>, <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>/ <emphasis>n</emphasis><emphasis><superscript>2</superscript></emphasis>) </para>

  
  <para>So the distribution of the sample mean is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis>, <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>/<emphasis>n</emphasis>). </para>

  
  <para>To get the distribution of the difference between two sample means, we invoke another property of the normal distribution: if <emphasis>X</emphasis><emphasis><subscript>1</subscript></emphasis> is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis><emphasis><subscript>1</subscript></emphasis>, <emphasis>σ</emphasis><emphasis><subscript>1</subscript></emphasis><emphasis><superscript>2</superscript></emphasis>) and <emphasis>X</emphasis><emphasis><subscript>2</subscript></emphasis> is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(<emphasis>μ</emphasis><emphasis><subscript>2</subscript></emphasis>, <emphasis>σ</emphasis><emphasis><subscript>2</subscript></emphasis><emphasis><superscript>2</superscript></emphasis>), </para>

  
  <para><informalequation id="a0000000206" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0045.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  aX_1 + bX_2 \sim \normal (a\mu _1 + b\mu _2, a^2\sigma _1^2 + b^2\sigma _2^2)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> So as a special case: </para>

  
  <para><informalequation id="a0000000207" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0046.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  X_1 - X_2 \sim \normal (\mu _1 - \mu _2, \sigma _1^2 + \sigma _2^2)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Putting it all together, we conclude that the sample in <xref linkend="length_deltas_cdf" /> is drawn from <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(0, <emphasis>f</emphasis><emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>), where <emphasis>f</emphasis> = 1/<emphasis>n</emphasis> + 1/<emphasis>m</emphasis>. Plugging in <emphasis>n</emphasis> = 4413 and <emphasis>m</emphasis> = 4735, we expect the difference of sample means to be <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0040.png" depth="10.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\mathcal{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>(0, 0.0032). <indexterm>
  <primary>p-value</primary>

</indexterm> </para>

  
  <para>We can use <literal>erf.NormalCdf</literal> to compute the p-value of the observed difference in the means: </para>

  
  <programlisting>delta = 0.078
sigma = math.sqrt(0.0032)
left = erf.NormalCdf(-delta, 0.0, sigma)
right = 1 - erf.NormalCdf(delta, 0.0, sigma)</programlisting>

  
  <para>The sum of the left and right tails is the p-value, 0.168, which is pretty close to what we estimated by resampling, 0.166. You can download the code I used in this section from <ulink url="http://thinkstats.com/hypothesis_analytic.py">http://thinkstats.com/hypothesis_analytic.py</ulink> <indexterm>
  <primary>hypothesis_analytic.py</primary>

</indexterm> </para>

</sect1><sect1 id="a0000000208" remap="section">
  <title>Power</title>
    
  
  <para> <indexterm>
  <primary>power</primary>

</indexterm> </para>

  
  <para>When the result of a hypothesis test is negative (that is, the effect is not statistically significant), can we conclude that the effect is not real? That depends on the power of the test. </para>

  
  <para>Statistical <emphasis role="bold">power</emphasis> is the probability that the test will be positive if the null hypothesis is false. In general, the power of a test depends on the sample size, the magnitude of the effect, and the threshold <emphasis>α</emphasis>. </para>

  
  <example id="a0000000209">
  <title></title>
  
  
  <para> What is the power of the test in <xref linkend="threshold" />, using <emphasis>α</emphasis> = 0.05 and assuming that the actual difference between the means is 0.078 weeks? </para>

  
  <para>You can estimate power by generating random samples from distributions with the given difference in the mean, testing the observed difference in the mean, and counting the number of positive tests. </para>

  
  <para>What is the power of the test with <emphasis>α</emphasis> = 0.10? </para>

</example>

  
  <para>One way to report the power of a test, along with a negative result, is to say something like, “If the apparent effect were as large as <emphasis>x</emphasis>, this test would reject the null hypothesis with probability <emphasis>p</emphasis>.” </para>

</sect1><sect1 id="a0000000210" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>significant:</term>
      <listitem>
  
  <para>An effect is statistically significant if it is unlikely to occur by chance. <indexterm>
  <primary>significant</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>null hypothesis:</term>
      <listitem>
  
  <para>A model of a system based on the assumption that an apparent effect is due to chance. <indexterm>
  <primary>null hypothesis</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>p-value:</term>
      <listitem>
  
  <para>The probability that an effect could occur by chance. <indexterm>
  <primary>p-value</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>hypothesis testing:</term>
      <listitem>
  
  <para>The process of determining whether an apparent effect is statistically significant. <indexterm>
  <primary>hypothesis testing</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>false positive:</term>
      <listitem>
  
  <para>The conclusion that an effect is real when it is not. <indexterm>
  <primary>false positive</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>false negative:</term>
      <listitem>
  
  <para>The conclusion that an effect is due to chance when it is not. <indexterm>
  <primary>false negative</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>two-sided test:</term>
      <listitem>
  
  <para>A test that asks, “What is the chance of an effect as big as the observed effect, positive or negative?” </para>
</listitem>
  </varlistentry><varlistentry>
    <term>one-sided test:</term>
      <listitem>
  
  <para>A test that asks, “What is the chance of an effect as big as the observed effect, and with the same sign?” <indexterm>
  <primary>one-sided test</primary>

</indexterm> <indexterm>
  <primary>two-sided test</primary>

</indexterm> <indexterm>
  <primary>test</primary>
<secondary>one-sided</secondary>
</indexterm> <indexterm>
  <primary>test</primary>
<secondary>two-sided</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>cross-validation:</term>
      <listitem>
  
  <para>A process of hypothesis testing that uses one dataset for exploratory data analysis and another dataset for testing. <indexterm>
  <primary>cross-validation</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>training set:</term>
      <listitem>
  
  <para>A dataset used to formulate a hypothesis for testing. <indexterm>
  <primary>training set</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>testing set:</term>
      <listitem>
  
  <para>A dataset used for testing. <indexterm>
  <primary>testing set</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>test statistic:</term>
      <listitem>
  
  <para>A statistic used to measure the deviation of an apparent effect from what is expected by chance. <indexterm>
  <primary>test statistic</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>chi-square test:</term>
      <listitem>
  
  <para>A test that uses the chi-square statistic as the test statistic. <indexterm>
  <primary>ch-square test</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>likelihood ratio:</term>
      <listitem>
  
  <para>The ratio of <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>A</emphasis>) to <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>B</emphasis>) for two hypotheses <emphasis>A</emphasis> and <emphasis>B</emphasis>, which is a way to report results from a Bayesian analysis without depending on priors. <indexterm>
  <primary>likelihood ratio</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>cell:</term>
      <listitem>
  
  <para>In a chi-square test, the categories the observations are divided into. <indexterm>
  <primary>cell</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>power:</term>
      <listitem>
  
  <para>The probability that a test will reject the null hypothesis if it is false. <indexterm>
  <primary>power</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="estimation">
  <title>Estimation</title>
  
  
  <para> <anchor id="a0000000211" /> <indexterm>
  <primary>estimation</primary>

</indexterm> </para>
<sect1 id="a0000000212" remap="section">
  <title>The estimation game</title>
    
  
  <para>Let’s play a game. I’ll think of a distribution, and you have to guess what it is. We’ll start out easy and work our way up. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para><emphasis>I’m thinking of a distribution.</emphasis> I’ll give you two hints; it’s a normal distribution, and here’s a random sample drawn from it: </para>

  
  <para>{−0.441, 1.774, −0.101, −1.138, 2.975, −2.138} </para>

  
  <para>What do you think is the mean parameter, <emphasis>μ</emphasis>, of this distribution? <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>parameter</primary>

</indexterm> </para>

  
  <para>One choice is to use the sample mean to estimate <emphasis>μ</emphasis>. Up until now we have used the symbol <emphasis>μ</emphasis> for both the sample mean and the mean parameter, but now to distinguish them I will use x̄ for the sample mean. In this example, x̄ is 0.155, so it would be reasonable to guess <emphasis>μ</emphasis> = 0.155. </para>

  
  <para>This process is called <emphasis role="bold">estimation</emphasis>, and the statistic we used (the sample mean) is called an <emphasis role="bold">estimator</emphasis>. <indexterm>
  <primary>estimator</primary>

</indexterm> </para>

  
  <para>Using the sample mean to estimate <emphasis>μ</emphasis> is so obvious that it is hard to imagine a reasonable alternative. But suppose we change the game by introducing outliers. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para><emphasis>I’m thinking of a distribution.</emphasis> It’s a normal distribution, and here’s a sample that was collected by an unreliable surveyor who occasionally puts the decimal point in the wrong place. </para>

  
  <para>{−0.441, 1.774, −0.101, −1.138, 2.975, −213.8} </para>

  
  <para>Now what’s your estimate of <emphasis>μ</emphasis>? If you use the sample mean your guess is −35.12. Is that the best choice? What are the alternatives? <indexterm>
  <primary>outlier</primary>

</indexterm> </para>

  
  <para>One option is to identify and discard outliers, then compute the sample mean of the rest. Another option is to use the median as an estimator. <indexterm>
  <primary>median</primary>

</indexterm> </para>

  
  <para>Which estimator is the best depends on the circumstances (for example, whether there are outliers) and on what the goal is. Are you trying to minimize errors, or maximize your chance of getting the right answer? <indexterm>
  <primary>error</primary>

</indexterm> <indexterm>
  <primary>MSE</primary>

</indexterm> <indexterm>
  <primary>mean squared error</primary>

</indexterm> </para>

  
  <para>If there are no outliers, the sample mean minimizes the <emphasis role="bold">mean squared error</emphasis> (MSE). If we play the game many times, and each time compute the error x̄ − <emphasis>μ</emphasis>, the sample mean minimizes </para>

  
  <para><informalequation id="a0000000213" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0047.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  MSE = \frac{1}{m} \sum (\bar{x}- \mu )^2  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Where <emphasis>m</emphasis> is the number of times you play the estimation game (not to be confused with <emphasis>n</emphasis>, which is the size of the sample used to compute x̄). </para>

  
  <para>Minimizing MSE is a nice property, but it’s not always the best strategy. For example, suppose we are estimating the distribution of wind speeds at a building site. If we guess too high, we might overbuild the structure, increasing its cost. But if we guess too low, the building might collapse. Because cost as a function of error is asymmetric, minimizing MSE is not the best strategy. <indexterm>
  <primary>prediction</primary>

</indexterm> </para>

  
  <para>As another example, suppose I roll three six-sided dice and ask you to predict the total. If you get it exactly right, you get a prize; otherwise you get nothing. In this case the value that minimizes MSE is 10.5, but that would be a terrible guess. For this game, you want an estimator that has the highest chance of being right, which is a <emphasis role="bold">maximum likelihood estimator</emphasis> (MLE). If you pick 10 or 11, your chance of winning is 1 in 8, and that’s the best you can do. <indexterm>
  <primary>MLE</primary>

</indexterm> <indexterm>
  <primary>maximum likelihood estimator</primary>

</indexterm> </para>

  
  <example id="a0000000214">
  <title></title>
  
  
  <para> Write a function that draws 6 values from a normal distribution with <emphasis>μ</emphasis> = 0 and <emphasis>σ</emphasis> = 1. Use the sample mean to estimate <emphasis>μ</emphasis> and compute the error x̄ − <emphasis>μ</emphasis>. Run the function 1000 times and compute MSE. <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>Now modify the program to use the median as an estimator. Compute MSE again and compare to the MSE for x̄. <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>median</primary>

</indexterm> <indexterm>
  <primary>MSE</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000215" remap="section">
  <title>Guess the variance</title>
    
  
  <para> <indexterm>
  <primary>variance</primary>

</indexterm> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para><emphasis>I’m thinking of a distribution.</emphasis> It’s a normal distribution, and here’s a (familiar) sample: </para>

  
  <para>{−0.441, 1.774, −0.101, −1.138, 2.975, −2.138} </para>

  
  <para>What do you think is the variance, <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>, of my distribution? Again, the obvious choice is to use the sample variance as an estimator. I will use <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0048.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>to denote the sample variance, to distinguish from the unknown parameter <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>. </para>

  
  <para><informalequation id="a0000000216" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0049.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  S^2 = \frac{1}{n} \sum (x_ i - \bar{x})^2  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> For large samples, <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0048.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>is an adequate estimator, but for small samples it tends to be too low. Because of this unfortunate property, it is called a <emphasis role="bold">biased</emphasis> estimator. <indexterm>
  <primary>sample variance</primary>

</indexterm> <indexterm>
  <primary>biased estimator</primary>

</indexterm> <indexterm>
  <primary>estimator</primary>
<secondary>biased</secondary>
</indexterm> <indexterm>
  <primary>unbiased estimator</primary>

</indexterm> <indexterm>
  <primary>estimator</primary>
<secondary>unbiased</secondary>
</indexterm> </para>

  
  <para>An estimator is <emphasis role="bold">unbiased</emphasis> if the expected total (or mean) error, after many iterations of the estimation game, is 0. Fortunately, there is another simple statistic that is an unbiased estimator of <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>: </para>

  
  <para><informalequation id="a0000000217" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0050.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  S_{n-1}^2 = \frac{1}{n-1} \sum (x_ i - \bar{x})^2  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The biggest problem with this estimator is that its name and symbol are used inconsistently. The name “sample variance” can refer to either <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0048.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>or <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0051.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S_{n-1}^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>, and the symbol <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0048.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>is used for either or both. </para>

  
  <para>For an explanation of why <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0048.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>is biased, and a proof that <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0051.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S_{n-1}^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>is unbiased, see <ulink url="http://wikipedia.org/wiki/Bias_of_an_estimator">http://wikipedia.org/wiki/Bias_of_an_estimator</ulink>. </para>

  
  <example id="a0000000218">
  <title></title>
  
  
  <para> Write a function that draws 6 values from a normal distribution with <emphasis>μ</emphasis>  =  0 and <emphasis>σ</emphasis>  =  1. Use the sample variance to estimate <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis> and compute the error <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0048.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> − <emphasis>σ</emphasis><emphasis><superscript>2</superscript></emphasis>. Run the function 1000 times and compute mean error (not squared). <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>Now modify the program to use the unbiased estimator <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0051.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$S_{n-1}^2$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>. Compute the mean error again and see if it converges to zero as you increase the number of games. </para>

</example>

</sect1><sect1 id="a0000000219" remap="section">
  <title>Understanding errors</title>
    
  
  <para> <indexterm>
  <primary>error</primary>

</indexterm> </para>

  
  <para>Before we go on, let’s clear up a common source of confusion. Properties like MSE and bias are long-term expectations based on many iterations of the estimation game. </para>

  
  <para>While you are playing the game, you don’t know the errors. That is, if I give you a sample and ask you to estimate a parameter, you can compute the value of the estimator, but you can’t compute the error. If you could, you wouldn’t need the estimator! </para>

  
  <para>The reason we talk about estimation error is to describe the behavior of different estimators in the long run. In this chapter we run experiments to examine those behaviors; these experiments are artificial in the sense that we know the actual values of the parameters, so we can compute errors. But when you work with real data, you don’t, so you can’t. </para>

  
  <para>Now let’s get back to the game. </para>

</sect1><sect1 id="a0000000220" remap="section">
  <title>Exponential distributions</title>
    
  
  <para> <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>

  
  <para><emphasis>I’m thinking of a distribution.</emphasis> It’s an exponential distribution, and here’s a sample: </para>

  
  <para>{5.384, 4.493, 19.198, 2.790, 6.122, 12.844} </para>

  
  <para>What do you think is the parameter, <emphasis>λ</emphasis>, of this distribution? <indexterm>
  <primary>parameter</primary>

</indexterm> <indexterm>
  <primary>mean</primary>

</indexterm> </para>

  
  

  
  <para>In general, the mean of an exponential distribution is 1/<emphasis>λ</emphasis>, so working backwards, we might choose </para>

  
  <para><simplelist>
  <member> <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0052.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\lambda }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = 1 / x̄</member>
</simplelist> </para>

  
  <para>It is common to use hat notation for estimators, so <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0052.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\lambda }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> is an estimator of <emphasis>λ</emphasis>. And not just any estimator; it is also the MLE estimator<footnote><para>See <ulink url="http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood">http://wikipedia.org/wiki/Exponential_distribution#Maximum_likelihood</ulink>.</para></footnote>. So if you want to maximize your chance of guessing <emphasis>λ</emphasis> exactly, <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0052.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\lambda }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> is the way to go. <indexterm>
  <primary>MLE</primary>

</indexterm> <indexterm>
  <primary>maximum likelihood estimator</primary>

</indexterm> </para>

  
  <para>But we know that x̄ is not robust in the presence of outliers, so we expect <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0052.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\lambda }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> to have the same problem. <indexterm>
  <primary>robust</primary>

</indexterm> <indexterm>
  <primary>outlier</primary>

</indexterm> </para>

  
  <para>Maybe we can find an alternative based on the sample median. Remember that the median of an exponential distribution is log(2) / <emphasis>λ</emphasis>, so working backwards again, we can define an estimator </para>

  
  <para><informalequation id="a0000000221" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0053.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \hat{\lambda }_{1/2}= \log (2) / \mu _{1/2}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> where <emphasis>μ</emphasis><emphasis><subscript>1/2</subscript></emphasis> is the sample median. <indexterm>
  <primary>median</primary>

</indexterm> </para>

  
  <example id="a0000000222">
  <title></title>
  
  
  <para> Run an experiment to see which of <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0052.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\lambda }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> and <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0054.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\lambda }_{1/2}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> yields lower MSE. Test whether either of them is biased. <indexterm>
  <primary>biased estimator</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000223" remap="section">
  <title>Confidence intervals</title>
    
  
  <para> <indexterm>
  <primary>confidence interval</primary>

</indexterm> <indexterm>
  <primary>interval</primary>
<secondary>confidence</secondary>
</indexterm> <indexterm>
  <primary>point estimate</primary>

</indexterm> </para>

  
  <para>So far we have looked at estimators that generate single values, known as <emphasis role="bold">point estimates</emphasis>. For many problems, we might prefer an interval that specifies an upper and lower bound on the unknown parameter. </para>

  
  <para>Or, more generally, we might want that whole distribution; that is, the range of values the parameter could have, and for each value in the range, a notion of how likely it is. </para>

  
  <para>Let’s start with <emphasis role="bold">confidence intervals</emphasis>. <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> </para>

  
  <para><emphasis>I’m thinking of a distribution.</emphasis> It’s an exponential distribution, and here’s a sample: </para>

  
  <para>{5.384, 4.493, 19.198, 2.790, 6.122, 12.844} </para>

  
  <para>I want you to give me a range of values that you think is likely to contain the unknown parameter <emphasis>λ</emphasis>. More specifically, I want a 90% confidence interval, which means that if we play this game over and over, your interval will contain <emphasis>λ</emphasis> 90% of the time. </para>

  
  <para>It turns out that this version of the game is hard, so I’m going to tell you the answer, and all you have to do is test it. </para>

  
  <para>Confidence intervals are usually described in terms of the miss rate, <emphasis>α</emphasis>, so a 90% confidence interval has miss rate <emphasis>α</emphasis> = 0.1. The confidence interval for the <emphasis>λ</emphasis> parameter of an exponential distribution is </para>

  
  <para><informalequation id="a0000000224" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0055.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \left( \hat{\lambda }\frac{\chi ^2(2n, 1-\alpha /2)}{2n}, \hat{\lambda }\frac{\chi ^2(2n, \alpha /2)}{2n} \right)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> where <emphasis>n</emphasis> is the sample size, <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0052.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\lambda }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> is the mean-based estimator from the previous section, and <emphasis>χ</emphasis><emphasis><superscript>2</superscript></emphasis>(<emphasis>k</emphasis>, <emphasis>x</emphasis>) is the CDF of a chi-squared distribution with <emphasis>k</emphasis> degrees of freedom, evaluated at <emphasis>x</emphasis>(see <ulink url="http://wikipedia.org/wiki/Chi-square_distribution">http://wikipedia.org/wiki/Chi-square_distribution</ulink>). <indexterm>
  <primary>chi-square distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>chi-square</secondary>
</indexterm> </para>

  
  <para>In general, confidence intervals are hard to compute analytically, but relatively easy to estimate using simulation. But first we need to talk about Bayesian estimation. <indexterm>
  <primary>Bayesian estimation</primary>

</indexterm> <indexterm>
  <primary>estimation</primary>
<secondary>Bayesian</secondary>
</indexterm> </para>

</sect1><sect1 id="a0000000225" remap="section">
  <title>Bayesian estimation</title>
    
  
  <para>If you collect a sample and compute a 90% confidence interval, it is tempting to say that the true value of the parameter has a 90% chance of falling in the interval. But from a frequentist point of view, that is not correct because the parameter is an unknown but fixed value. It is either in the interval you computed or not, so the frequentist definition of probability doesn’t apply. <indexterm>
  <primary>parameter</primary>

</indexterm> <indexterm>
  <primary>frequentism</primary>

</indexterm> <indexterm>
  <primary>Bayesianism</primary>

</indexterm> </para>

  
  <para>So let’s try a different version of the game. <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> </para>

  
  <para><emphasis>I’m thinking of a distribution.</emphasis> It’s an exponential distribution, and I chose <emphasis>λ</emphasis> from a uniform distribution between 0.5 and 1.5. Here’s a sample, which I’ll call <emphasis>X</emphasis>: </para>

  
  <para>{2.675, 0.198, 1.152, 0.787, 2.717, 4.269} </para>

  
  <para>Based on this sample, what value of <emphasis>λ</emphasis> do you think I chose? </para>

  
  <para>In this version of the game, <emphasis>λ</emphasis> <emphasis>is</emphasis> a random quantity, so we can reasonably talk about its distribution, and we can compute it easily using Bayes’s theorem. </para>

  
  <para>Here are the steps: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Divide the range (0.5, 1.5) into a set of equal-sized bins. For each bin, we define <emphasis>H</emphasis><emphasis><subscript>i</subscript></emphasis>, which is the hypothesis that the actual value of <emphasis>λ</emphasis> falls in the <emphasis>i</emphasis> th bin. Since <emphasis>λ</emphasis> was drawn from a uniform distribution, the prior probability, <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>i</subscript></emphasis>), is the same for all <emphasis>i</emphasis>. </para>
</listitem>
  
  <listitem>
  
  <para>For each hypothesis, we compute the likelihood, <emphasis>P</emphasis>(<emphasis>X</emphasis>|<emphasis>H</emphasis><emphasis><subscript>i</subscript></emphasis>), which is the chance of drawing the sample <emphasis>X</emphasis>given <emphasis>H</emphasis><emphasis><subscript>i</subscript></emphasis>. </para>

  
  <para><informalequation id="a0000000226" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0056.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(X ~ |~ H_ i) = \prod _ j \mathrm{expo}(\lambda _ i, x_ j)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> where expo(<emphasis>λ</emphasis>, <emphasis>x</emphasis>) is a function that computes the PDF of the exponential distribution with parameter <emphasis>λ</emphasis>, evaluated at <emphasis>x</emphasis>. </para>

  
  <para><informalequation id="a0000000227" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0057.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  PDF_{expo}(\lambda , x) = \lambda e^{-\lambda x} \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The symbol <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0058.png" depth="8.750px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\prod $</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> represents the product of a sequence (see <ulink url="http://wikipedia.org/wiki/Multiplication#Capital_Pi_notation">http://wikipedia.org/wiki/Multiplication#Capital_Pi_notation</ulink>). </para>
</listitem>
  
  <listitem>
  
  <para>Then by Bayes’s theorem the posterior distribution is </para>

  
  <para><simplelist>
  <member> <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>i</subscript></emphasis>|<emphasis>X</emphasis>) = <emphasis>P</emphasis>(<emphasis>H</emphasis><emphasis><subscript>i</subscript></emphasis>) <emphasis>P</emphasis>(<emphasis>X</emphasis>|<emphasis>H</emphasis><emphasis><subscript>i</subscript></emphasis>) /<emphasis>f</emphasis></member>
</simplelist> </para>

  
  <para>where <emphasis>f</emphasis>is the normalization factor </para>

  
  <para><informalequation id="a0000000228" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0059.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  f = \sum _ i P(H_ i) P(X ~ |~ H_ i)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>
</listitem>
  
</orderedlist></para>

  
  <para>Given a posterior distribution, it is easy to compute a confidence interval. For example, to compute a 90% CI, you can use the 5th and 95th percentiles of the posterior. <indexterm>
  <primary>credible interval</primary>

</indexterm> <indexterm>
  <primary>interval</primary>
<secondary>credible</secondary>
</indexterm> </para>

  
  <para>Bayesian confidence intervals are sometimes called <emphasis role="bold">credible intervals</emphasis>; for a discussion of the differences, see <ulink url="http://wikipedia.org/wiki/Credible_interval">http://wikipedia.org/wiki/Credible_interval</ulink>. </para>

</sect1><sect1 id="a0000000229" remap="section">
  <title>Implementing Bayesian estimation</title>
    
  
  <para>To represent the prior distribution, we could use a Pmf, Cdf, or any other representation of a distribution, but since we want to map from a hypothesis to a probability, a Pmf is a natural choice. <indexterm>
  <primary>Pmf object</primary>

</indexterm> <indexterm>
  <primary>prior</primary>

</indexterm> <indexterm>
  <primary>posterior</primary>

</indexterm> </para>

  
  <para>Each value in the Pmf represents a hypothesis; for example, the value 0.5 represents the hypothesis that <emphasis>λ</emphasis> is 0.5. In the prior distribution, all hypotheses have the same probability. So we can construct the prior like this: </para>

  
  <programlisting>def MakeUniformSuite(low, high, steps):
    hypos = [low + (high-low) * i / (steps-1.0) for i in range(steps)]
    pmf = Pmf.MakePmfFromList(hypos)
    return pmf</programlisting>

  
  <para>This function makes and returns a Pmf that represents a collection of related hypotheses, called a <emphasis role="bold">suite</emphasis>. Each hypothesis has the same probability, so the distribution is <emphasis role="bold">uniform</emphasis>. <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> <indexterm>
  <primary>suite</primary>

</indexterm> </para>

  
  <para>The arguments <literal>low</literal> and <literal>high</literal> specify the range of values; <literal>steps</literal> is the number of hypotheses. <indexterm>
  <primary>update</primary>

</indexterm> </para>

  
  <para>To perform the update, we take a suite of hypotheses and a body of evidence: </para>

  
  <programlisting>def Update(suite, evidence):
    for hypo in suite.Values():
        likelihood = Likelihood(evidence, hypo)
        suite.Mult(hypo, likelihood)
    suite.Normalize()</programlisting>

  
  <para>For each hypothesis in the suite, we multiply the prior probability by the likelihood of the evidence. Then we normalize the suite. </para>

  
  <para>In this function, <literal>suite</literal> has to be a Pmf, but <literal>evidence</literal> can be any type, as long as <literal>Likelihood</literal> knows how to interpret it. <indexterm>
  <primary>likelihood</primary>

</indexterm> </para>

  
  <para>Here’s the likelihood function: </para>

  
  <programlisting>def Likelihood(evidence, hypo):
    param = hypo
    likelihood = 1
    for x in evidence:
        likelihood *= ExpoPdf(x, param)

    return likelihood</programlisting>

  
  <para>In <literal>Likelihood</literal> we assume that <literal>evidence</literal> is a sample from an exponential distribution and compute the product in the previous section. <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>PDF</primary>

</indexterm> </para>

  
  <para><literal>ExpoPdf</literal> evaluates the PDF of the exponential distribution at <literal>x</literal>: </para>

  
  <programlisting>def ExpoPdf(x, param):
    p = param * math.exp(-param * x)
    return p</programlisting>

  
  <para>Putting it all together, here’s the code that creates the prior and computes the posterior: </para>

  
  <programlisting>evidence = [2.675, 0.198, 1.152, 0.787, 2.717, 4.269]
prior = MakeUniformSuite(0.5, 1.5, 100)
posterior = prior.Copy()
Update(posterior, evidence)</programlisting>

  
  <para>You can download the code in this section from <ulink url="http://thinkstats.com/estimate.py">http://thinkstats.com/estimate.py</ulink>. <indexterm>
  <primary>estimate.py</primary>

</indexterm> </para>

  
  <para>When I think of Bayesian estimation, I imagine a room full of people, where each person has a different guess about whatever you are trying to estimate. So in this example they each have a guess about the correct value of <emphasis>λ</emphasis>. </para>

  
  <para>Initially, each person has a degree of confidence about their own hypothesis. After seeing the evidence, each person updates their confidence based on <emphasis>P</emphasis>(<emphasis>E</emphasis>|<emphasis>H</emphasis>), the likelihood of the evidence, given their hypothesis. </para>

  
  <para>Most often the likelihood function computes a probability, which is at most 1, so initially everyone’s confidence goes down (or stays the same). But then we normalize, which increases everyone’s confidence. </para>

  
  <para>So the net effect is that some people get more confident, and some less, depending on the relative likelihood of their hypothesis. </para>

</sect1><sect1 id="censored" remap="section">
  <title>Censored data</title>
    
  
  <para> <anchor id="a0000000230" /> <indexterm>
  <primary>censored data</primary>

</indexterm> <indexterm>
  <primary>MacKay, David</primary>

</indexterm> </para>

  
  <para>The following problem appears in Chapter 3 of David MacKay’s <emphasis>Information Theory, Inference and Learning Algorithms</emphasis>, which you can download from <ulink url="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/">http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/</ulink>. </para>

  
  <blockquote remap="quote">
  <para> Unstable particles are emitted from a source and decay at a distance <emphasis>x</emphasis>, a real number that has an exponential probability distribution with [parameter] <emphasis>λ</emphasis>. Decay events can only be observed if they occur in a window extending from <emphasis>x</emphasis> = 1 cm to <emphasis>x</emphasis> = 20 cm. <emphasis>n</emphasis> decays are observed at locations { <emphasis>x</emphasis><emphasis><subscript>1</subscript></emphasis>, ... , <emphasis>x</emphasis><emphasis><subscript>N</subscript></emphasis> }. What is <emphasis>λ</emphasis>? <indexterm>
  <primary>exponential distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>exponential</secondary>
</indexterm> <indexterm>
  <primary>particles</primary>

</indexterm> <indexterm>
  <primary>unstable particles</primary>

</indexterm> <indexterm>
  <primary>decay</primary>

</indexterm> </para>
</blockquote>

  
  <para>This is an example of an estimation problem with <emphasis role="bold">censored data</emphasis>; that is, we know that some data are systematically excluded. </para>

  
  <para>One of the strengths of Bayesian estimation is that it can deal with censored data with relative ease. We can use the method from the previous section with only one change: we have to replace PDF<emphasis><subscript>expo</subscript></emphasis> with the conditional distribution: </para>

  
  <para><informalequation id="a0000000231" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0060.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \mathrm{PDF}_{cond}(\lambda , x) = \lambda e^{-\lambda x} / Z(\lambda )  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> for 1 &lt; <emphasis>x</emphasis> &lt; 20, and 0 otherwise, with </para>

  
  <para><informalequation id="a0000000232" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0061.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  Z(\lambda ) = \int _1^{20} \lambda e^{-\lambda x} ~ dx = e^{-\lambda } - e^{-20 \lambda }  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> You might remember Z(<emphasis>λ</emphasis>) from <xref linkend="expo_pdf" />. I told you to keep it handy. </para>

  
  <example id="a0000000233">
  <title></title>
  
  
  <para> Download <ulink url="http://thinkstats.com/estimate.py">http://thinkstats.com/estimate.py</ulink>, which contains the code from the previous section, and make a copy named <literal>decay.py</literal>. <indexterm>
  <primary>estimate.py</primary>

</indexterm> </para>

  
  <para>Modify <literal>decay.py</literal> to compute the posterior distribution of <emphasis>λ</emphasis> for the sample <emphasis>X</emphasis> = {1.5, 2, 3, 4, 5, 12}. For the prior you can use a uniform distribution between 0 and 1.5 (not including 0). <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> </para>

  
  <para>You can download a solution to this problem from <ulink url="http://thinkstats.com/decay.py">http://thinkstats.com/decay.py</ulink>. <indexterm>
  <primary>decay.py</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000234">
  <title></title>
  
  
  <para> In the 2008 Minnesota Senate race the final vote count was 1,212,629 votes for Al Franken and 1,212,317 votes for Norm Coleman. Franken was declared the winner, but as Charles Seife points out in <emphasis>Proofiness</emphasis>, the margin of victory was much smaller than the margin of error, so the result should have been considered a tie. <indexterm>
  <primary>Coleman, Norm</primary>

</indexterm> <indexterm>
  <primary>Franken, Al</primary>

</indexterm> <indexterm>
  <primary>Minnesota Senate Race</primary>

</indexterm> <indexterm>
  <primary>Seife, Charles</primary>

</indexterm> <indexterm>
  <primary sortas="Proofiness">Proofiness</primary>

</indexterm> <indexterm>
  <primary>margin of victory</primary>

</indexterm> <indexterm>
  <primary>margin of error</primary>

</indexterm> </para>

  
  <para>Assuming that there is a chance that any vote might be lost and a chance that any vote might be double-counted, what is the probability that Coleman actually received more votes? </para>

  
  <para>Hint: you will have to fill in some details to model the error process. </para>

</example>

</sect1><sect1 id="a0000000235" remap="section">
  <title>The locomotive problem</title>
    
  
  <para> <indexterm>
  <primary>locomotive problem</primary>

</indexterm> <indexterm>
  <primary>Mosteller, Frederick</primary>

</indexterm> <indexterm>
  <primary>German tank problem</primary>

</indexterm> </para>

  
  <para>The locomotive problem is a classic estimation problem also known as the “German tank problem.” Here is the version that appears in Mosteller, <emphasis>Fifty Challenging Problems in Probability</emphasis>: </para>

  
  <blockquote remap="quote">
  <para> “A railroad numbers its locomotives in order 1..N. One day you see a locomotive with the number 60. Estimate how many locomotives the railroad has.” </para>
</blockquote>

  
  <para>Before you read the rest of this section, try to answer these questions: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>For a given estimate, <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>, what is the likelihood of the evidence, <emphasis>P</emphasis>(<emphasis>E</emphasis>|<inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>)? What is the maximum likelihood estimator? <indexterm>
  <primary>MLE</primary>

</indexterm> <indexterm>
  <primary>maximum likelihood estimator</primary>

</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>If we see train <emphasis>i</emphasis> it seems reasonable that we would guess some multiple of <emphasis>i</emphasis> so let’s assume <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = <emphasis>a</emphasis><emphasis>i</emphasis>. What value of <emphasis>a</emphasis> minimizes mean squared error? <indexterm>
  <primary>MSE</primary>

</indexterm> <indexterm>
  <primary>mean squared error</primary>

</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>Still assuming that <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = <emphasis>a</emphasis><emphasis>i</emphasis> can you find a value of  <emphasis>a</emphasis> that makes <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> an unbiased estimator? <indexterm>
  <primary>unbiased estimator</primary>

</indexterm> <indexterm>
  <primary>estimator</primary>
<secondary>unbiased</secondary>
</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>For what value of N is 60 the average value? </para>
</listitem>
  
  <listitem>
  
  <para>What is the Bayesian posterior distribution assuming a prior distribution that is uniform from 1 to 200? <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> <indexterm>
  <primary>posterior distribution</primary>

</indexterm> </para>
</listitem>
  
</orderedlist></para>

  
  <para>For best results, you should take some time to work on these questions before you continue. </para>

  
  <para>For a given estimate, <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>, the likelihood of seeing train <emphasis>i</emphasis> is 1/<inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> if <emphasis>i</emphasis> ≤ <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>, and 0 otherwise. So the MLE is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = <emphasis>i</emphasis>. In other words, if you see train 60 and you want to maximize your chance of getting the answer exactly right, you should guess that there are 60 trains. </para>

  
  <para>But this estimator doesn’t do very well in terms of MSE. We can do better by choosing <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = <emphasis>a</emphasis><emphasis>i</emphasis>; all we have to do is find a good value for <emphasis>a</emphasis>. </para>

  
  <para>Suppose that there are, in fact, <emphasis>N</emphasis> trains. Each time we play the estimation game, we see train <emphasis>i</emphasis> and guess <emphasis>a</emphasis><emphasis>i</emphasis>, so the squared error is (<emphasis>a</emphasis><emphasis>i</emphasis> − <emphasis>N</emphasis>)<emphasis><superscript>2</superscript></emphasis>. </para>

  
  <para>If we play the game <emphasis>N</emphasis> times and see each train once, the mean squared error is </para>

  
  <para><informalequation id="a0000000236" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0063.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  MSE = \frac{1}{N} \sum _{i=1}^ N (ai - N)^2  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> To minimize MSE, we take the derivative with respect to <emphasis>a</emphasis>: </para>

  
  <para><informalequation id="a0000000237" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0064.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \frac{d MSE}{da} = \frac{1}{N} \sum _{i=1}^ N 2i (ai - N) = 0  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> And solve for <emphasis>a</emphasis>. </para>

  
  <para><informalequation id="a0000000238" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0065.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  a = \frac{3N}{2N+1}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> At first glance, that doesn’t seem very useful, because <emphasis>N</emphasis> appears on the right-hand side, which suggests that we need to know <emphasis>N</emphasis> to choose <emphasis>a</emphasis>, but if we knew <emphasis>N</emphasis>, we wouldn’t need an estimator in the first place. </para>

  
  <para>However, for large values of <emphasis>N</emphasis>, the optimal value for <emphasis>a</emphasis> converges to 3/2, so we could choose <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = 3<emphasis>i</emphasis>/ 2. </para>

  
  <para>To find an unbiased estimator, we can compute the mean error (ME): </para>

  
  <para><informalequation id="a0000000239" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0066.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  ME = \frac{1}{N} \sum _{i=1}^ N (ai - N)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> And find the value of <emphasis>a</emphasis> that yields ME = 0, which turns out to be </para>

  
  <para><informalequation id="a0000000240" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0067.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  a = \frac{2N}{N-1} \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> For large values of <emphasis>N</emphasis>, <emphasis>a</emphasis> converges to 2, so we could choose <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = 2<emphasis>i</emphasis>. </para>

  
  <para>So far we have generated three estimators, <emphasis>i</emphasis>, 3<emphasis>i</emphasis>/2, and 2<emphasis>i</emphasis>, that have the properties of maximizing likelihood, minimizing squared error, and being unbiased. </para>

  
  <para>Yet another way to generate an estimator is to choose the value that makes the population mean equal the sample mean. If we see train <emphasis>i</emphasis>, the sample mean is just <emphasis>i</emphasis>; the train population that has the same mean is <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0062.png" depth="11px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{N}$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> = 2<emphasis>i</emphasis> − 1. </para>

  
  <figure id="locomotive">
  
  <title>Posterior distribution of the number of trains.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/locomotive.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para>Finally, to compute the Bayesian posterior distribution, we compute </para>

  
  <para><informalequation id="a0000000242" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0068.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  P(H_ n ~ |~ i) = \frac{P(i ~ |~ H_ n) P(H_ n)}{P(i)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Where <emphasis>H</emphasis><emphasis><subscript>n</subscript></emphasis> is the hypothesis that there are <emphasis>n</emphasis> trains, and <emphasis>i</emphasis> is the evidence: we saw train <emphasis>i</emphasis>. Again, <emphasis>P</emphasis>(<emphasis>i</emphasis>|<emphasis>H</emphasis><emphasis><subscript>n</subscript></emphasis>) is 1/<emphasis>n</emphasis> if <emphasis>i</emphasis> &lt; <emphasis>n</emphasis>, and 0 otherwise. The normalizing constant, <emphasis>P</emphasis>(<emphasis>i</emphasis>), is just the sum of the numerators for each hypothesis. <indexterm>
  <primary>Bayesian estimation</primary>

</indexterm> <indexterm>
  <primary>prior distribution</primary>

</indexterm> </para>

  
  <para>If the prior distribution is uniform from 1 to 200, we start with 200 hypotheses and compute the likelihood for each. You can download an implementation from <ulink url="http://thinkstats.com/locomotive.py">http://thinkstats.com/locomotive.py</ulink>. <xref linkend="locomotive" /> shows what the result looks like. <indexterm>
  <primary>locomotive.py</primary>

</indexterm> <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> </para>

  
  <para>The 90% credible interval for this posterior is [63, 189], which is still quite wide. Seeing one train doesn’t provide strong evidence for any of the hypotheses (although it does rule out the hypotheses with <emphasis>n</emphasis> &lt; <emphasis>i</emphasis>). <indexterm>
  <primary>credible interval</primary>

</indexterm> </para>

  
  <para>If we start with a different prior, the posterior is significantly different, which helps to explain why the other estimators are so diverse. </para>

  
  <para>One way to think of different estimators is that they are implicitly based on different priors. If there is enough evidence to swamp the priors, then all estimators tend to converge; otherwise, as in this case, there is no single estimator that has all of the properties we might want. </para>

  
  <example id="a0000000243">
  <title></title>
  
  
  <para> Generalize <literal remap="verb">locomotive.py</literal> to handle the case where you see more than one train. You should only have to change a few lines of code. <indexterm>
  <primary>locomotive.py</primary>

</indexterm> </para>

  
  <para>See if you can answer the other questions for the case where you see more than one train. You can find a discussion of the problem and several solutions at <ulink url="http://wikipedia.org/wiki/German_tank_problem">http://wikipedia.org/wiki/German_tank_problem</ulink>. </para>

</example>

</sect1><sect1 id="a0000000244" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>estimation:</term>
      <listitem>
  
  <para>The process of inferring the parameters of a distribution from a sample. <indexterm>
  <primary>estimation</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>estimator:</term>
      <listitem>
  
  <para>A statistic used to estimate a parameter. <indexterm>
  <primary>estimation</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>mean squared error:</term>
      <listitem>
  
  <para>A measure of estimation error. <indexterm>
  <primary>mean squared error</primary>

</indexterm> <indexterm>
  <primary>MSE</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>maximum likelihood estimator:</term>
      <listitem>
  
  <para>An estimator that computes the point estimate with the highest likelihood. <indexterm>
  <primary>MLE</primary>

</indexterm> <indexterm>
  <primary>maximum likelihood estimator</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>bias:</term>
      <listitem>
  
  <para>The tendency of an estimator to be above or below the actual value of the parameter, when averaged over repeated samples. <indexterm>
  <primary>biased estimator</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>point estimate:</term>
      <listitem>
  
  <para>An estimate expressed as a single value. <indexterm>
  <primary>point estimation</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>confidence interval:</term>
      <listitem>
  
  <para>An estimate expressed as an interval with a given probability of containing the true value of the parameter. <indexterm>
  <primary>confidence interval</primary>

</indexterm> <indexterm>
  <primary>interval</primary>
<secondary>confidence</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>credible interval:</term>
      <listitem>
  
  <para>Another name for a Bayesian confidence interval. <indexterm>
  <primary>credible interval</primary>

</indexterm> <indexterm>
  <primary>interval</primary>
<secondary>credible</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>censored data:</term>
      <listitem>
  
  <para>A dataset sampled in a way that systematically excludes some data. <indexterm>
  <primary>censored data</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter><chapter id="a0000000245">
  <title>Correlation</title>
  <sect1 id="a0000000246" remap="section">
  <title>Standard scores</title>
    
  
  <para>In this chapter we look at relationships between variables. For example, we have a sense that height is related to weight; people who are taller tend to be heavier. <emphasis role="bold">Correlation</emphasis> is a description of this kind of relationship. <indexterm>
  <primary>correlation</primary>

</indexterm> </para>

  
  <para>A challenge in measuring correlation is that the variables we want to compare might not be expressed in the same units. For example, height might be in centimeters and weight in kilograms. And even if they are in the same units, they come from different distributions. <indexterm>
  <primary>units</primary>

</indexterm> </para>

  
  <para>There are two common solutions to these problems: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Transform all values to <emphasis role="bold">standard scores</emphasis>. This leads to the Pearson coefficient of correlation. <indexterm>
  <primary>standard scores</primary>

</indexterm> <indexterm>
  <primary>Pearson coefficient of correlation</primary>

</indexterm> <indexterm>
  <primary>Spearman coefficient of correlation</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>correlation</secondary>
</indexterm> </para>
</listitem>
  
  <listitem>
  
  <para>Transform all values to their percentile ranks. This leads to the Spearman coefficient. <indexterm>
  <primary>rank</primary>

</indexterm> <indexterm>
  <primary>percentile rank</primary>

</indexterm> </para>
</listitem>
  
</orderedlist></para>

  
  <para>If <emphasis>X</emphasis> is a series of values, <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>, we can convert to standard scores by subtracting the mean and dividing by the standard deviation: z<emphasis><subscript>i</subscript></emphasis> = (x<emphasis><subscript>i</subscript></emphasis> − <emphasis>μ</emphasis>) / <emphasis>σ</emphasis>. <indexterm>
  <primary>mean</primary>

</indexterm> <indexterm>
  <primary>standard deviation</primary>

</indexterm> </para>

  
  <para>The numerator is a deviation: the distance from the mean. Dividing by <emphasis>σ</emphasis> <emphasis role="bold">normalizes</emphasis> the deviation, so the values of <emphasis>Z</emphasis> are dimensionless (no units) and their distribution has mean 0 and variance 1. <indexterm>
  <primary>normalize</primary>

</indexterm> <indexterm>
  <primary>deviation</primary>

</indexterm> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>If <emphasis>X</emphasis> is normally-distributed, so is <emphasis>Z</emphasis>; but if <emphasis>X</emphasis> is skewed or has outliers, so does <emphasis>Z</emphasis>. In those cases it is more robust to use percentile ranks. If <emphasis>R</emphasis>contains the percentile ranks of the values in <emphasis>X</emphasis>, the distribution of <emphasis>R</emphasis>is uniform between 0 and 100, regardless of the distribution of <emphasis>X</emphasis>. <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> <indexterm>
  <primary>robust</primary>

</indexterm> </para>

</sect1><sect1 id="a0000000247" remap="section">
  <title>Covariance</title>
    
  
  <para> <indexterm>
  <primary>covariance</primary>

</indexterm> <indexterm>
  <primary>deviation</primary>

</indexterm> </para>

  
  <para><emphasis role="bold">Covariance</emphasis> is a measure of the tendency of two variables to vary together. If we have two series, <emphasis>X</emphasis> and <emphasis>Y</emphasis>, their deviations from the mean are </para>

  
  <para><simplelist>
  <member> <emphasis>d</emphasis><emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis> = <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis> − <emphasis>μ</emphasis><emphasis><subscript>X</subscript></emphasis> </member>
</simplelist> </para>

  
  <para><simplelist>
  <member> <emphasis>d</emphasis><emphasis>y</emphasis><emphasis><subscript>i</subscript></emphasis> = <emphasis>y</emphasis><emphasis><subscript>i</subscript></emphasis> − <emphasis>μ</emphasis><emphasis><subscript>Y</subscript></emphasis> </member>
</simplelist> </para>

  
  <para>where <emphasis>μ</emphasis><emphasis><subscript>X</subscript></emphasis> is the mean of <emphasis>X</emphasis> and <emphasis>μ</emphasis><emphasis><subscript>Y</subscript></emphasis> is the mean of <emphasis>Y</emphasis>. If <emphasis>X</emphasis> and <emphasis>Y</emphasis> vary together, their deviations tend to have the same sign. </para>

  
  <para>If we multiply them together, the product is positive when the deviations have the same sign and negative when they have the opposite sign. So adding up the products gives a measure of the tendency to vary together. </para>

  
  <para>Covariance is the mean of these products: </para>

  
  <para><informalequation id="a0000000248" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0069.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  Cov(X,Y) = \frac{1}{n} \sum dx_ i dy_ i  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> where <emphasis>n</emphasis> is the length of the two series (they have to be the same length). </para>

  
  <para>Covariance is useful in some computations, but it is seldom reported as a summary statistic because it is hard to interpret. Among other problems, its units are the product of the units of <emphasis>X</emphasis> and <emphasis>Y</emphasis>. So the covariance of weight and height might be in units of kilogram-meters, which doesn’t mean much. </para>

  
  <example id="a0000000249">
  <title></title>
  
  
  <para> Write a function called <literal>Cov</literal> that takes two lists and computes their covariance. To test your function, compute the covariance of a list with itself and confirm that Cov(<emphasis>X</emphasis>, <emphasis>X</emphasis>) = Var(<emphasis>X</emphasis>). </para>

  
  <para>You can download a solution from <ulink url="http://thinkstats.com/correlation.py">http://thinkstats.com/correlation.py</ulink>. <indexterm>
  <primary>correlation.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000250" remap="section">
  <title>Correlation</title>
    
  
  <para> <indexterm>
  <primary>correlation</primary>

</indexterm> <indexterm>
  <primary>standard score</primary>

</indexterm> </para>

  
  <para>One solution to this problem is to divide the deviations by <emphasis>σ</emphasis>, which yields standard scores, and compute the product of standard scores: </para>

  
  <para><informalequation id="a0000000251" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0070.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  p_ i = \frac{(x_ i - \mu _ X)}{\sigma _ X} \frac{(y_ i - \mu _ Y)}{\sigma _ Y}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> The mean of these products is </para>

  
  <para><informalequation id="a0000000252" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0071.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \rho = \frac{1}{n} \sum p_ i  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> This value is called <emphasis role="bold">Pearson’s correlation</emphasis> after Karl Pearson, an influential early statistician. It is easy to compute and easy to interpret. Because standard scores are dimensionless, so is <emphasis>ρ</emphasis>. <indexterm>
  <primary>Pearson, Karl</primary>

</indexterm> <indexterm>
  <primary>Pearson coefficient of correlation</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>correlation</secondary>
</indexterm> </para>

  
  <para>Also, the result is necessarily between −1 and +1. To see why, we can rewrite <emphasis>ρ</emphasis> by factoring out <emphasis>σ</emphasis><emphasis><subscript>X</subscript></emphasis> and <emphasis>σ</emphasis><emphasis><subscript>Y</subscript></emphasis>: </para>

  
  <para><informalequation id="a0000000253" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0072.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \rho = \frac{Cov(X,Y)}{\sigma _ X \sigma _ Y}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Expressed in terms of deviations, we have </para>

  
  <para><informalequation id="a0000000254" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0073.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \rho = \frac{\sum dx_ i dy_ x}{\sum dx_ i \sum dy_ i}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Then by the ever-useful Cauchy-Schwarz inequality<footnote><para>See <ulink url="http://wikipedia.org/wiki/Cauchy-Schwarz_inequality">http://wikipedia.org/wiki/Cauchy-Schwarz_inequality</ulink>.</para></footnote>, we can show that <emphasis>ρ</emphasis><emphasis><superscript>2</superscript></emphasis> ≤ 1, so −1 ≤ <emphasis>ρ</emphasis> ≤ 1. <indexterm>
  <primary>Cauchy-Schwarz inequality</primary>

</indexterm> </para>

  
  <para>The magnitude of <emphasis>ρ</emphasis> indicates the strength of the correlation. If <emphasis>ρ</emphasis> = 1 the variables are perfectly correlated, which means that if you know one, you can make a perfect prediction about the other. The same is also true if <emphasis>ρ</emphasis> = −1. It means that the variables are negatively correlated, but for purposes of prediction, a negative correlation is just as good as a positive one. <indexterm>
  <primary>prediction</primary>

</indexterm> </para>

  
  <para>Most correlation in the real world is not perfect, but it is still useful. For example, if you know someone’s height, you might be able to guess their weight. You might not get it exactly right, but your guess will be better than if you didn’t know the height. Pearson’s correlation is a measure of how much better. </para>

  
  <para>So if <emphasis>ρ</emphasis> = 0, does that mean there is no relationship between the variables? Unfortunately, no. Pearson’s correlation only measures <emphasis>linear</emphasis> relationships. If there’s a nonlinear relationship, <emphasis>ρ</emphasis> understates the strength of the dependence. <indexterm>
  <primary>linear relationship</primary>

</indexterm> </para>

  
  <figure id="corr_examples">
  
  <title>Examples of datasets with a range of correlations.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/Correlation_examples.png" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para><xref linkend="corr_examples" /> is from <ulink url="http://wikipedia.org/wiki/Correlation_and_dependence">http://wikipedia.org/wiki/Correlation_and_dependence</ulink>. It shows scatterplots and correlation coefficients for several carefully-constructed datasets. <indexterm>
  <primary>coefficient</primary>
<secondary>correlation</secondary>
</indexterm> <indexterm>
  <primary>scatter plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>scatter</secondary>
</indexterm> </para>

  
  <para>The top row shows linear relationships with a range of correlations; you can use this row to get a sense of what different values of <emphasis>ρ</emphasis> look like. The second row shows perfect correlations with a range of slopes, which demonstrates that correlation is unrelated to slope (we’ll talk about estimating slope soon). The third row shows variables that are clearly related, but because the relationship is non-linear, the correlation coefficient is 0. </para>

  
  <para>The moral of this story is that you should always look at a scatterplot of your data before blindly computing a correlation coefficient. <indexterm>
  <primary>correlation coefficient</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>correlation</secondary>
</indexterm> </para>

  
  <example id="a0000000256">
  <title></title>
  
  
  <para> Write a function called <literal>Corr</literal> that takes two lists and computes their correlation. Hint: use <literal>thinkstats.Var</literal> and the <literal>Cov</literal> function you wrote in the previous exercise. <indexterm>
  <primary>variance</primary>

</indexterm> <indexterm>
  <primary>covariance</primary>

</indexterm> </para>

  
  <para>To test your function, compute the covariance of a list with itself and confirm that Corr(<emphasis>X</emphasis>, <emphasis>X</emphasis>) is 1. You can download a solution from <ulink url="http://thinkstats.com/correlation.py">http://thinkstats.com/correlation.py</ulink>. <indexterm>
  <primary>correlation.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000257" remap="section">
  <title>Making scatterplots in pyplot</title>
    
  
  <para> <indexterm>
  <primary>scatter plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>scatter</secondary>
</indexterm> <indexterm>
  <primary>pyplot</primary>

</indexterm> </para>

  
  <figure id="scatterplot1">
  
  <title>Simple scatterplot of weight versus height for the respondents in the BRFSS.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/scatter1.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <figure id="scatterplot2">
  
  <title>Scatterplot with jittered data.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/scatter2.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <figure id="scatterplot3">
  
  <title>Scatterplot with jittering and transparency.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/scatter3.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <figure id="scatterplot4">
  
  <title>Scatterplot with binned data using <literal>pyplot.hexbin</literal>.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/scatter4.pdf" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  

  
  
</figure>

  
  <para> <indexterm>
  <primary>Behavioral Risk Factor Surveillance System</primary>

</indexterm> <indexterm>
  <primary>BRFSS</primary>

</indexterm> </para>

  
  <para>The simplest way to check for a relationship between two variables is a scatterplot, but making a good scatterplot is not always easy. As an example, I’ll plot weight versus height for the respondents in the BRFSS (see <xref linkend="lognormal" />). <literal>pyplot</literal> provides a function named <literal>scatter</literal> that makes scatterplots: </para>

  
  <programlisting>import matplotlib.pyplot as pyplot
pyplot.scatter(heights, weights)</programlisting>

  
  <para><xref linkend="scatterplot1" /> shows the result. Not surprisingly, it looks like there is a positive correlation: taller people tend to be heavier. But this is not the best representation of the data, because the data are packed into columns. The problem is that the heights were rounded to the nearest inch, converted to centimeters, and then rounded again. Some information is lost in translation. <indexterm>
  <primary>height</primary>

</indexterm> <indexterm>
  <primary>weight</primary>

</indexterm> <indexterm>
  <primary>jitter</primary>

</indexterm> </para>

  
  <para>We can’t get that information back, but we can minimize the effect on the scatterplot by <emphasis role="bold">jittering</emphasis> the data, which means adding random noise to reverse the effect of rounding off. Since these measurements were rounded to the nearest inch, they can be off by up to 0.5 inches or 1.3 cm. So I added uniform noise in the range −1.3 to 1.3: <indexterm>
  <primary>uniform distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>uniform</secondary>
</indexterm> <indexterm>
  <primary>noise</primary>

</indexterm> </para>

  
  <programlisting>jitter = 1.3
heights = [h + random.uniform(-jitter, jitter) for h in heights]</programlisting>

  
  <para><xref linkend="scatterplot2" /> shows the result. Jittering the data makes the shape of the relationship clearer. In general you should only jitter data for purposes of visualization and avoid using jittered data for analysis. </para>

  
  <para>Even with jittering, this is not the best way to represent the data. There are many overlapping points, which hides data in the dense parts of the figure and gives disproportionate emphasis to outliers. <indexterm>
  <primary>outlier</primary>

</indexterm> </para>

  
  <para>We can solve that with the <literal>alpha</literal> parameter, which makes the points partly transparent: </para>

  
  <programlisting>pyplot.scatter(heights, weights, alpha=0.2)</programlisting>

  
  <para> <xref linkend="scatterplot3" /> shows the result. Overlapping data points look darker, so darkness is proportional to density. In this version of the plot we can see an apparent artifact: a horizontal line near 90 kg or 200 pounds. Since this data is based on self-reports in pounds, the most likely explanation is some responses were rounded off (possibly down). </para>

  
  <para>Using transparency works well for moderate-sized datasets, but this figure only shows the first 1000 records in the BRFSS, out of a total of 414509. <indexterm>
  <primary>hexbin plot</primary>

</indexterm> <indexterm>
  <primary>plot</primary>
<secondary>hexbin</secondary>
</indexterm> </para>

  
  <para>To handle larger datasets, one option is a hexbin plot, which divides the graph into hexagonal bins and colors each bin according to how many data points fall in it. <literal>pyplot</literal> provides a function called <literal>hexbin</literal>: </para>

  
  <programlisting>pyplot.scatter(heights, weights, cmap=matplotlib.cm.Blues)</programlisting>

  
  <para> <xref linkend="scatterplot4" /> shows the result with a blue colormap. An advantage of a hexbin is that it shows the shape of the relationship well, and it is efficient for large datasets. A drawback is that it makes the outliers invisible. <indexterm>
  <primary>colormap</primary>

</indexterm> <indexterm>
  <primary>grayscale</primary>

</indexterm> </para>

  
  <para>The moral of this story is that it is not easy to make a scatterplot that is not potentially misleading. You can download the code for these figures from <ulink url="http://thinkstats.com/brfss_scatter.py">http://thinkstats.com/brfss_scatter.py</ulink>. <indexterm>
  <primary>brfss_scatter.py</primary>

</indexterm> </para>

</sect1><sect1 id="a0000000262" remap="section">
  <title>Spearman’s rank correlation</title>
    
  
  <para>Pearson’s correlation works well if the relationship between variables is linear and if the variables are roughly normal. But it is not robust in the presence of outliers. <indexterm>
  <primary>Pearson coefficient of correlation</primary>

</indexterm> <indexterm>
  <primary>Spearman coefficient of correlation</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>correlation</secondary>
</indexterm> <indexterm>
  <primary>normal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>normal</secondary>
</indexterm> <indexterm>
  <primary>Gaussian distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Gaussian</secondary>
</indexterm> </para>

  
  <para>Anscombe’s quartet demonstrates this effect; it contains four data sets with the same correlation. One is a linear relation with random noise, one is a non-linear relation, one is a perfect relation with an outlier, and one has no relation except an artifact caused by an outlier. You can read more about it at <ulink url="http://wikipedia.org/wiki/Anscombe’s_quartet">http://wikipedia.org/wiki/Anscombe’s_quartet</ulink>. <indexterm>
  <primary>Anscombe’s quartet</primary>

</indexterm> </para>

  
  <para>Spearman’s rank correlation is an alternative that mitigates the effect of outliers and skewed distributions. To compute Spearman’s correction, we have to compute the <emphasis role="bold">rank</emphasis> of each value, which is its index in the sorted sample. For example, in the sample {7, 1, 2, 5} the rank of the value 5 is 3, because it appears third if we sort the elements. Then we compute Pearson’s correlation for the ranks. </para>

  
  <para>An alternative to Spearman’s is to apply a transform that makes the data more nearly normal, the compute Pearson’s correlation for the transformed data. For example, if the data are approximately lognormal, you could take the log of each value and compute the correlation of the logs. <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> </para>

  
  <example id="a0000000263">
  <title></title>
  
  
  <para> Write a function that takes a sequence and returns a list that contains the rank for each element. For example, if the sequence is {7, 1, 2, 5}, the result should be { 4, 1, 2, 3}. </para>

  
  <para>If the same value appears more than once, the strictly correct solution is to assign each of them the average of their ranks. But if you ignore that and assign them ranks in arbitrary order, the error is usually small. </para>

  
  <para>Write a function that takes two sequences (with the same length) and computes their Spearman rank coefficient. You can download a solution from <ulink url="http://thinkstats.com/correlation.py">http://thinkstats.com/correlation.py</ulink>. <indexterm>
  <primary>correlation.py</primary>

</indexterm> <indexterm>
  <primary>Spearman coefficient of correlation</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>correlation</secondary>
</indexterm> </para>

</example>

  
  <example id="a0000000264">
  <title></title>
  
  
  <para> Download <ulink url="http://thinkstats.com/brfss.py">http://thinkstats.com/brfss.py</ulink> and <ulink url="http://thinkstats.com/brfss_scatter.py">http://thinkstats.com/brfss_scatter.py</ulink>. Run them and confirm that you can read the BFRSS data and generate scatterplots. <indexterm>
  <primary>Behavioral Risk Factor Surveillance System</primary>

</indexterm> <indexterm>
  <primary>BRFSS</primary>

</indexterm> <indexterm>
  <primary>brfss.py</primary>

</indexterm> <indexterm>
  <primary>brfss_scatter.py</primary>

</indexterm> </para>

  
  <para>Comparing the scatterplots to <xref linkend="corr_examples" />, what value do you expect for Pearson’s correlation? What value do you get? <indexterm>
  <primary>weight</primary>
<secondary>adult</secondary>
</indexterm> <indexterm>
  <primary>adult weight</primary>

</indexterm> <indexterm>
  <primary>lognormal distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>lognormal</secondary>
</indexterm> <indexterm>
  <primary>outlier</primary>

</indexterm> </para>

  
  <para>Because the distribution of adult weight is lognormal, there are outliers that affect the correlation. Try plotting log(weight) versus height, and compute Pearson’s correlation for the transformed variable. </para>

  
  <para>Finally, compute Spearman’s rank correlation for weight and height. Which coefficient do you think is the best measure of the strength of the relationship? You can download a solution from <ulink url="http://thinkstats.com/brfss_corr.py">http://thinkstats.com/brfss_corr.py</ulink>. <indexterm>
  <primary>brfss_corr.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000265" remap="section">
  <title>Least squares fit</title>
    
  
  <para>Correlation coefficients measure the strength and sign of a relationship, but not the slope. There are several ways to estimate the slope; the most common is a <emphasis role="bold">linear least squares fit</emphasis>. A “linear fit” is a line intended to model the relationship between variables. A “least squares” fit is one that minimizes the mean squared error (MSE) between the line and the data<footnote><para>See <ulink url="http://wikipedia.org/wiki/Simple_linear_regression">http://wikipedia.org/wiki/Simple_linear_regression</ulink>.</para></footnote>. <indexterm>
  <primary>least squares fit</primary>

</indexterm> <indexterm>
  <primary>linear least squares</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>correlation</secondary>
</indexterm> <indexterm>
  <primary>linear regression</primary>

</indexterm> </para>

  
  <para>Suppose we have a sequence of points, <emphasis>Y</emphasis>, that we want to express as a function of another sequence <emphasis>X</emphasis>. If there is a linear relationship between <emphasis>X</emphasis> and <emphasis>Y</emphasis> with intercept <emphasis>α</emphasis> and slope <emphasis>β</emphasis>, we expect each <emphasis>y</emphasis><emphasis><subscript>i</subscript></emphasis> to be roughly <emphasis>α</emphasis> + <emphasis>β</emphasis> <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>. <indexterm>
  <primary>residual</primary>

</indexterm> </para>

  
  <para>But unless the correlation is perfect, this prediction is only approximate. The deviation, or <emphasis role="bold">residual</emphasis>, is </para>

  
  <para><simplelist>
  <member> <emphasis>ε</emphasis><emphasis><subscript>i</subscript></emphasis> = (<emphasis>α</emphasis> + <emphasis>β</emphasis><emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>) − <emphasis>y</emphasis><emphasis><subscript>i</subscript></emphasis> </member>
</simplelist> </para>

  
  <para>The residual might be due to random factors like measurement error, or non-random factors that are unknown. For example, if we are trying to predict weight as a function of height, unknown factors might include diet, exercise, and body type. <indexterm>
  <primary>slope</primary>

</indexterm> <indexterm>
  <primary>intercept</primary>

</indexterm> </para>

  
  <para>If we get the parameters <emphasis>α</emphasis> and <emphasis>β</emphasis> wrong, the residuals get bigger, so it makes intuitive sense that the parameters we want are the ones that minimize the residuals. </para>

  
  <para>As usual, we could minimize the absolute value of the residuals, or their squares, or their cubes, etc. The most common choice is to minimize the sum of squared residuals </para>

  
  <para><informalequation id="a0000000266" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0074.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \min _{\alpha , \beta } \sum \varepsilon _ i^2  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> Why? There are three good reasons and one bad one: </para>

  
  <para><itemizedlist>
  
    <listitem>
  
  <para>Squaring has the obvious feature of treating positive and negative residuals the same, which is usually what we want. </para>
</listitem>
  
    <listitem>
  
  <para>Squaring gives more weight to large residuals, but not so much weight that the largest residual always dominates. </para>
</listitem>
  
    <listitem>
  
  <para>If the residuals are independent of <emphasis>x</emphasis>, random, and normally distributed with <emphasis>μ</emphasis> = 0 and constant (but unknown) <emphasis>σ</emphasis>, then the least squares fit is also the maximum likelihood estimator of <emphasis>α</emphasis> and <emphasis>β</emphasis>.<footnote><para>See Press et al., <emphasis>Numerical Recipes in C</emphasis>, Chapter 15 at <ulink url="http://www.nrbook.com/a/bookcpdf/c15-1.pdf">http://www.nrbook.com/a/bookcpdf/c15-1.pdf</ulink>.</para></footnote> <indexterm>
  <primary>MLE</primary>

</indexterm> <indexterm>
  <primary>maximum likelihood estimator</primary>

</indexterm> </para>
</listitem>
  
    <listitem>
  
  <para>The values of <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0075.png" depth="8.500px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\alpha }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> and <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0076.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\beta }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> that minimize the squared residuals can be computed efficiently. </para>
</listitem>
  
</itemizedlist></para>

  
  <para>That last reason made sense when computational efficiency was more important than choosing the method most appropriate to the problem at hand. That’s no longer the case, so it is worth considering whether squared residuals are the right thing to minimize. <indexterm>
  <primary>computation</primary>

</indexterm> </para>

  
  <para>For example, if you are using values of <emphasis>X</emphasis> to predict values of <emphasis>Y</emphasis>, guessing too high might be better (or worse) than guessing too low. In that case you might want to compute some cost function, cost(<emphasis>ε</emphasis><emphasis><subscript>i</subscript></emphasis>), and minimize total cost. <indexterm>
  <primary>cost function</primary>

</indexterm> </para>

  
  <para>However, computing a least squares fit is quick, easy and often good enough, so here’s how: </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Compute the sample means, x̄ and ȳ, the variance of <emphasis>X</emphasis>, and the covariance of <emphasis>X</emphasis> and <emphasis>Y</emphasis>. </para>
</listitem>
  
  <listitem>
  
  <para>The estimated slope is </para>

  
  <para><informalequation id="a0000000267" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0077.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \hat{\beta } = \frac{Cov(X,Y)}{Var(X)}  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>
</listitem>
  
  <listitem>
  
  <para>And the intercept is </para>

  
  <para><informalequation id="a0000000268" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0078.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  \hat{\alpha } = \bar{y}- \hat{\beta } \bar{x} \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>
</listitem>
  
</orderedlist></para>

  
  <para>To see how this is derived, you can read <ulink url="http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares">http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares</ulink>. </para>

  
  <example id="a0000000269">
  <title></title>
  
  
  <para> Write a function named <literal>LeastSquares</literal> that takes <emphasis>X</emphasis> and <emphasis>Y</emphasis> and computes <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0075.png" depth="8.500px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\alpha }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> and <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0076.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\beta }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>. You can download a solution from <ulink url="http://thinkstats.com/correlation.py">http://thinkstats.com/correlation.py</ulink>. <indexterm>
  <primary>correlation.py</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000270">
  <title></title>
  
  
  <para> Using the data from the BRFSS again, compute the linear least squares fit for log(weight) versus height. You can download a solution from <ulink url="http://thinkstats.com/brfss_corr.py">http://thinkstats.com/brfss_corr.py</ulink>. <indexterm>
  <primary>Behavioral Risk Factor Surveillance System</primary>

</indexterm> <indexterm>
  <primary>BRFSS</primary>

</indexterm> <indexterm>
  <primary>brfss_corr.py</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000271">
  <title></title>
  
  
  <para> The distribution of wind speeds in a given location determines the wind power density, which is an upper bound on the average power that a wind turbine at that location can generate. According to some sources, empirical distributions of wind speed are well modeled by a Weibull distribution (see <ulink url="http://wikipedia.org/wiki/Wind_power#Distribution_of_wind_speed">http://wikipedia.org/wiki/Wind_power#Distribution_of_wind_speed</ulink>). <indexterm>
  <primary>wind speed</primary>

</indexterm> <indexterm>
  <primary>wind power density</primary>

</indexterm> <indexterm>
  <primary>turbine</primary>

</indexterm> <indexterm>
  <primary>Weibull distribution</primary>

</indexterm> <indexterm>
  <primary>distribution</primary>
<secondary>Weibull</secondary>
</indexterm> </para>

  
  <para>To evaluate whether a location is a viable site for a wind turbine, you can set up an anemometer to measure wind speed for a period of time. But it is hard to measure the tail of the wind speed distribution accurately because, by definition, events in the tail don’t happen very often. <indexterm>
  <primary>tail</primary>

</indexterm> </para>

  
  <para>One way to address this problem is to use measurements to estimate the parameters of a Weibull distribution, then integrate over the continuous PDF to compute wind power density. <indexterm>
  <primary>PDF</primary>

</indexterm> </para>

  
  <para>To estimate the parameters of a Weibull distribution, we can use the transformation from <xref linkend="weibull" /> and then use a linear fit to find the slope and intercept of the transformed data. </para>

  
  <para>Write a function that takes a sample from a Weibull distribution and estimates its parameters. </para>

  
  <para>Now write a function that takes the parameters of a Weibull distribution of wind speed and computes average wind power density (you might have to do some research for this part). </para>

</example>

</sect1><sect1 id="a0000000272" remap="section">
  <title>Goodness of fit</title>
    
  
  <para> <indexterm>
  <primary>goodness of fit</primary>

</indexterm> <indexterm>
  <primary>fit, goodness</primary>

</indexterm> </para>

  
  <para>Having fit a linear model to the data, we might want to know how good it is. Well, that depends on what it’s for. One way to evaluate a model is its predictive power. </para>

  
  <para>In the context of prediction, the quantity we are trying to guess is called a <emphasis role="bold">dependent variable</emphasis> and the quantity we are using to make the guess is called an <emphasis role="bold">explanatory</emphasis> or <emphasis role="bold">independent variable</emphasis>. <indexterm>
  <primary>dependent variable</primary>

</indexterm> <indexterm>
  <primary>explanatory variable</primary>

</indexterm> <indexterm>
  <primary>independent variable</primary>

</indexterm> <indexterm>
  <primary>coefficient</primary>
<secondary>determination</secondary>
</indexterm> </para>

  
  <para>To measure the predictive power of a model, we can compute the <emphasis role="bold">coefficient of determination</emphasis>, more commonly known as “R-squared”: </para>

  
  <para><informalequation id="a0000000273" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0079.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  R^2 = 1 - \frac{Var(\varepsilon )}{Var(Y)} \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> To understand what <emphasis>R</emphasis><emphasis><superscript>2</superscript></emphasis> means, suppose (again) that you are trying to guess someone’s weight. If you didn’t know anything about them, your best strategy would be to guess ȳ; in that case the MSE of your guesses would be Var(<emphasis>Y</emphasis>): </para>

  
  <para><informalequation id="a0000000274" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0080.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  MSE = \frac{1}{n} \sum (\bar{y}- y_ i)^2 = Var(Y)  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> But if I told you their height, you would guess <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0081.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\alpha } + \hat{\beta }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> <emphasis>x</emphasis><emphasis><subscript>i</subscript></emphasis>; in that case your MSE would be Var(<emphasis>ε</emphasis>). </para>

  
  <para><informalequation id="a0000000275" remap="equation">
  
  <mediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0082.png" depth="0px" role="math" scale="25" valign="middle"></imagedata>
    </imageobject>
    <textobject role="tex">
      <phrase>\[  MSE = \frac{1}{n} \sum (\hat{\alpha } + \hat{\beta } x_ i - y_ i)^2 = Var(\varepsilon )  \]</phrase>
    </textobject>
  </mediaobject>
</informalequation></para>

  
  <para> So the term Var(<emphasis>ε</emphasis>)/Var(<emphasis>Y</emphasis>) is the ratio of mean squared error with and without the explanatory variable, which is the fraction of variability left unexplained by the model. The complement, <emphasis>R</emphasis><emphasis><superscript>2</superscript></emphasis>, is the fraction of variability explained by the model. <indexterm>
  <primary>variability</primary>

</indexterm> </para>

  
  <para>If a model yields <emphasis>R</emphasis><emphasis><superscript>2</superscript></emphasis> = 0.64, you could say that the model explains 64% of the variability, or it might be more precise to say that it reduces the MSE of your predictions by 64%. </para>

  
  <para>In the context of a linear least squares model, it turns out that there is a simple relationship between the coefficient of determination and Pearson’s correlation coefficient, <emphasis>ρ</emphasis>: </para>

  
  <para><simplelist>
  <member> <emphasis>R</emphasis><emphasis><superscript>2</superscript></emphasis> = <emphasis>ρ</emphasis><emphasis><superscript>2</superscript></emphasis> </member>
</simplelist> </para>

  
  <para>See <ulink url="http://wikipedia.org/wiki/Howzzat">http://wikipedia.org/wiki/Howzzat</ulink>! <indexterm>
  <primary>howzzat</primary>

</indexterm> </para>

  
  <example id="a0000000276">
  <title></title>
  
  
  <para> The Wechsler Adult Intelligence Scale (WAIS) is meant to be a measure of intelligence; scores are calibrated so that the mean and standard deviation in the general population are 100 and 15. <indexterm>
  <primary>Adult Intelligence Scale</primary>

</indexterm> <indexterm>
  <primary>WAIS</primary>

</indexterm> <indexterm>
  <primary>IQ</primary>

</indexterm> <indexterm>
  <primary>intelligence</primary>

</indexterm> </para>

  
  <para>Suppose that you wanted to predict someone’s WAIS score based on their SAT scores. According to one study, there is a Pearson correlation of 0.72 between total SAT scores and WAIS scores. </para>

  
  <para>If you applied your predictor to a large sample, what would you expect to be the mean squared error (MSE) of your predictions? </para>

  
  <para>Hint: What is the MSE if you always guess 100? </para>

</example>

  
  <example id="a0000000277">
  <title></title>
  
  
  <para> Write a function named <literal>Residuals</literal> that takes <emphasis>X</emphasis>, <emphasis>Y</emphasis>, <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0075.png" depth="8.500px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\alpha }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> and <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0076.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\beta }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> and returns a list of <emphasis>ε</emphasis><emphasis><subscript>i</subscript></emphasis>. <indexterm>
  <primary>residual</primary>

</indexterm> </para>

  
  <para>Write a function named <literal>CoefDetermination</literal> that takes the <emphasis>ε</emphasis><emphasis><subscript>i</subscript></emphasis> and <emphasis>Y</emphasis> and returns <emphasis>R</emphasis><emphasis><superscript>2</superscript></emphasis>. To test your functions, confirm that <emphasis>R</emphasis><emphasis><superscript>2</superscript></emphasis> = <emphasis>ρ</emphasis><emphasis><superscript>2</superscript></emphasis>. You can download a solution from <ulink url="http://thinkstats.com/correlation.py">http://thinkstats.com/correlation.py</ulink>. <indexterm>
  <primary>correlation.py</primary>

</indexterm> </para>

</example>

  
  <example id="a0000000278">
  <title></title>
  
  
  <para> Using the height and weight data from the BRFSS (one more time), compute <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0075.png" depth="8.500px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\alpha }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>, <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0076.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\beta }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> and <emphasis>R</emphasis><emphasis><superscript>2</superscript></emphasis>. If you were trying to guess someone’s weight, how much would it help to know their height? You can download a solution from <ulink url="http://thinkstats.com/brfss_corr.py">http://thinkstats.com/brfss_corr.py</ulink>. <indexterm>
  <primary>Behavioral Risk Factor Surveillance System</primary>

</indexterm> <indexterm>
  <primary>BRFSS</primary>

</indexterm> <indexterm>
  <primary>brfss_corr.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000279" remap="section">
  <title>Correlation and Causation</title>
    
  
  <para> <indexterm>
  <primary>correlation</primary>

</indexterm> <indexterm>
  <primary>causation</primary>

</indexterm> <indexterm>
  <primary>xkcd</primary>

</indexterm> <indexterm>
  <primary>comic</primary>

</indexterm> <indexterm>
  <primary>Munroe, Randall</primary>

</indexterm> </para>

  
  <para>The web comic <literal>xkcd</literal> demonstrates the difficulty of inferring causation: </para>

  
  <figure id="a0000000280">
  
  <title>From <literal>xkcd.com</literal> by Randall Munroe.</title>
<mediaobject>
  <imageobject remap="includegraphics">
    <imagedata fileref="figs/correlation.png" scale="40"></imagedata>
  </imageobject>
</mediaobject>

  
  
</figure>

  
  <para>In general, a relationship between two variables does not tell you whether one causes the other, or the other way around, or both, or whether they might both be caused by something else altogether. </para>

  
  <para>This rule can be summarized with the phrase “Correlation does not imply causation,” which is so pithy it has its own Wikipedia page: <ulink url="http://wikipedia.org/wiki/Correlation_does_not_imply_causation">http://wikipedia.org/wiki/Correlation_does_not_imply_causation</ulink>. </para>

  
  <para>So what can you do to provide evidence of causation? </para>

  
  <para><orderedlist>
  
  <listitem>
  
  <para>Use time. If A comes before B, then A can cause B but not the other way around (at least according to our common understanding of causation). The order of events can help us infer the direction of causation, but it does not preclude the possibility that something else causes both A and B. </para>
</listitem>
  
  <listitem>
  
  <para>Use randomness. If you divide a large population into two groups at random and compute the means of almost any variable, you expect the difference to be small. This is a consequence of the Central Limit Theorem (so it is subject to the same requirements). </para>

  
  <para>If the groups are nearly identical in all variable but one, you can eliminate spurious relationships. <indexterm>
  <primary>spurious relationship</primary>

</indexterm> </para>

  
  <para>This works even if you don’t know what the relevant variables are, but it works even better if you do, because you can check that the groups are identical. </para>
</listitem>
  
</orderedlist></para>

  
  <para>These ideas are the motivation for the <emphasis role="bold">randomized controlled trial</emphasis>, in which subjects are assigned randomly to two (or more) groups: a <emphasis role="bold">treatment</emphasis> group that receives some kind of intervention, like a new medicine, and a <emphasis role="bold">control group</emphasis> that receives no intervention, or another treatment whose effects are known. <indexterm>
  <primary>randomized controlled trial</primary>

</indexterm> <indexterm>
  <primary>controlled trial</primary>

</indexterm> <indexterm>
  <primary>treatment group</primary>

</indexterm> <indexterm>
  <primary>control group</primary>

</indexterm> <indexterm>
  <primary>medicine</primary>

</indexterm> </para>

  
  <para>A randomized controlled trial is the most reliable way to demonstrate a causal relationship, and the foundation of science-based medicine (see <ulink url="http://wikipedia.org/wiki/Randomized_controlled_trial">http://wikipedia.org/wiki/Randomized_controlled_trial</ulink>). </para>

  
  <para>Unfortunately, controlled trials are only possible in the laboratory sciences, medicine, and a few other disciplines. In the social sciences, controlled experiments are rare, usually because they are impossible or unethical. </para>

  
  <para>One alternative is to look for a <emphasis role="bold">natural experiment</emphasis>, where different “treatments” are applied to groups that are otherwise similar. One danger of natural experiments is that the groups might differ in ways that are not apparent. You can read more about this topic at <ulink url="http://wikipedia.org/wiki/Natural_experiment">http://wikipedia.org/wiki/Natural_experiment</ulink>. <indexterm>
  <primary>natural experiment</primary>

</indexterm> </para>

  
  <para>In some cases it is possible to infer causal relationships using <emphasis role="bold">regression analysis</emphasis>. A linear least squares fit is a simple form of regression that explains a dependent variable using one explanatory variable. There are similar techniques that work with arbitrary numbers of independent variables. <indexterm>
  <primary>regression analysis</primary>

</indexterm> </para>

  
  <para>I won’t cover those techniques here, but there are also simple ways to control for spurious relationships. For example, in the NSFG, we saw that first babies tend to be lighter than others (see <xref linkend="birth_weights" />). But birth weight is also correlated with the mother’s age, and mothers of first babies tend to be younger than mothers of other babies. <indexterm>
  <primary>birth weight</primary>

</indexterm> <indexterm>
  <primary>weight</primary>
<secondary>birth</secondary>
</indexterm> <indexterm>
  <primary>National Survey of Family Growth</primary>

</indexterm> <indexterm>
  <primary>NSFG</primary>

</indexterm> </para>

  
  <para>So it may be that first babies are lighter because their mothers are younger. To control for the effect of age, we could divide the mothers into age groups and compare birth weights for first babies and others in each age group. <indexterm>
  <primary>pooled data</primary>

</indexterm> </para>

  
  <para>If the difference between first babies and others is the same in each age group as it was in the pooled data, we conclude that the difference is not related to age. If there is no difference, we conclude that the effect is entirely due to age. Or, if the difference is smaller, we can quantify how much of the effect is due to age. </para>

  
  <example id="a0000000281">
  <title></title>
  
  
  <para> The NSFG data includes a variable named <literal>agepreg</literal> that records the age of the mother at the time of birth. Make a scatterplot of mother’s age and baby’s weight for each live birth. Can you see a relationship? </para>

  
  <para>Compute a linear least-squares fit for these variables. What are the units of the estimated parameters <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0075.png" depth="8.500px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\alpha }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation> and <inlineequation>
  
  <inlinemediaobject remap="math">
    <imageobject>
      <imagedata fileref="images/img-0076.png" depth="11.250px" scale="25" role="math" />
    </imageobject>
    <textobject role="tex">
      <phrase>$\hat{\beta }$</phrase>
    </textobject>
  </inlinemediaobject>
</inlineequation>? How would you summarize these results in a sentence or two? </para>

  
  <para>Compute the average age for mothers of first babies and the average age of other mothers. Based on the difference in ages between the groups, how much difference do you expect in the mean birth weights? What fraction of the actual difference in birth weights is explained by the difference in ages? </para>

  
  <para>You can download a solution to this problem from <ulink url="http://thinkstats.com/agemodel.py">http://thinkstats.com/agemodel.py</ulink>. If you are curious about multivariate regression, you can run <ulink url="http://thinkstats.com/age_lm.py">http://thinkstats.com/age_lm.py</ulink> which shows how to use the R statistical computing package from Python. But that’s a whole other book. <indexterm>
  <primary>agemodel.py</primary>

</indexterm> <indexterm>
  <primary>age_lm.py</primary>

</indexterm> </para>

</example>

</sect1><sect1 id="a0000000282" remap="section">
  <title>Glossary</title>
    
  
  <para><variablelist>
  <varlistentry>
    <term>correlation:</term>
      <listitem>
  
  <para>a description of the dependence between variables. <indexterm>
  <primary>correlation</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>normalize:</term>
      <listitem>
  
  <para>To transform a set of values so that their mean is 0 and their variance is 1. <indexterm>
  <primary>normalize</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>standard score:</term>
      <listitem>
  
  <para>A value that has been normalized. <indexterm>
  <primary>standard score</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>covariance:</term>
      <listitem>
  
  <para>a measure of the tendency of two variables to vary together. <indexterm>
  <primary>covariance</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>rank:</term>
      <listitem>
  
  <para>The index where an element appears in a sorted list. <indexterm>
  <primary>rank</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>least squares fit:</term>
      <listitem>
  
  <para>A model of a dataset that minimizes the sum of squares of the residuals. <indexterm>
  <primary>least squares fit</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>residual:</term>
      <listitem>
  
  <para>A measure of the deviation of an actual value from a model. <indexterm>
  <primary>residual</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>dependent variable:</term>
      <listitem>
  
  <para>A variable we are trying to predict or explain. <indexterm>
  <primary>dependent variable</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>independent variable:</term>
      <listitem>
  
  <para>A variable we are using to predict a dependent variable, also called an explanatory variable. <indexterm>
  <primary>independent variable</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>coefficient of determination:</term>
      <listitem>
  
  <para>A measure of the goodness of fit of a linear model. <indexterm>
  <primary>coefficient</primary>
<secondary>determination</secondary>
</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>randomized controlled trial:</term>
      <listitem>
  
  <para>An experimental design in which subject are divided into groups at random, and different groups are given different treatments. <indexterm>
  <primary>randomized controlled trial</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>treatment:</term>
      <listitem>
  
  <para>An change or intervention applied to one group in a controlled trial. <indexterm>
  <primary>treatment group</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>control group:</term>
      <listitem>
  
  <para>A group in a controlled trial that receives no treatment, or a treatment whose effect is known. <indexterm>
  <primary>control group</primary>

</indexterm> </para>
</listitem>
  </varlistentry><varlistentry>
    <term>natural experiment:</term>
      <listitem>
  
  <para>An experimental design that takes advantage of a natural division of subjects into groups in ways that are at least approximately random. <indexterm>
  <primary>natural experiment</primary>

</indexterm> </para>
</listitem>
  </varlistentry>
</variablelist></para>

</sect1>
</chapter>
<index></index>
</book>